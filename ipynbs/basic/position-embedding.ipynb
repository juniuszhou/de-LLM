{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1be025f4",
      "metadata": {},
      "source": [
        "# Position Embeddings: Sinusoidal/Cosine vs RoPE\n",
        "\n",
        "## Overview\n",
        "\n",
        "**Sinusoidal/Cosine Position Embedding (Original Transformer):**\n",
        "- Fixed, non-learnable embeddings\n",
        "- Uses sine and cosine functions with different frequencies\n",
        "- Added to token embeddings\n",
        "- Formula: `PE(pos, 2i) = sin(pos / 10000^(2i/d_model))` and `PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))`\n",
        "\n",
        "**RoPE (Rotary Position Embedding):**\n",
        "- Rotates query and key vectors by angle proportional to position\n",
        "- Applied during attention computation, not added to embeddings\n",
        "- Better extrapolation to longer sequences\n",
        "- Used in modern LLMs (LLaMA, GPT-NeoX, PaLM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "50e89f05",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from math import pi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "88580c81",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "Sinusoidal/Cosine Position Embedding\n",
            "======================================================================\n",
            "Shape: torch.Size([100, 64])\n",
            "\n",
            "First 5 positions, first 8 dimensions:\n",
            "tensor([[ 0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  1.0000],\n",
            "        [ 0.8415,  0.5403,  0.6816,  0.7318,  0.5332,  0.8460,  0.4093,  0.9124],\n",
            "        [ 0.9093, -0.4161,  0.9975,  0.0709,  0.9021,  0.4315,  0.7469,  0.6649],\n",
            "        [ 0.1411, -0.9900,  0.7783, -0.6279,  0.9933, -0.1160,  0.9536,  0.3010],\n",
            "        [-0.7568, -0.6536,  0.1415, -0.9899,  0.7785, -0.6277,  0.9933, -0.1157]])\n",
            "\n",
            "Last 5 positions, first 8 dimensions:\n",
            "tensor([[ 0.6833,  0.7302,  0.8504, -0.5262, -0.0154, -0.9999,  0.7029, -0.7112],\n",
            "        [ 0.9836, -0.1804,  0.2636, -0.9646, -0.5461, -0.8377,  0.3503, -0.9367],\n",
            "        [ 0.3796, -0.9251, -0.4645, -0.8856, -0.9086, -0.4176, -0.0638, -0.9980],\n",
            "        [-0.5734, -0.8193, -0.9435, -0.3314, -0.9914,  0.1312, -0.4667, -0.8844],\n",
            "        [-0.9992,  0.0398, -0.9163,  0.4005, -0.7687,  0.6396, -0.7878, -0.6159]])\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Sinusoidal/Cosine Position Embedding (Original Transformer)\n",
        "# ============================================================================\n",
        "\n",
        "def create_sinusoidal_position_embedding(max_len: int, d_model: int):\n",
        "    \"\"\"\n",
        "    Create sinusoidal position embeddings as in the original Transformer paper.\n",
        "    \n",
        "    Args:\n",
        "        max_len: Maximum sequence length\n",
        "        d_model: Model dimension\n",
        "        \n",
        "    Returns:\n",
        "        Position embedding matrix of shape (max_len, d_model)\n",
        "    \"\"\"\n",
        "    pe = torch.zeros(max_len, d_model)\n",
        "    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "    \n",
        "    # Create div_term: 10000^(2i/d_model) for i in [0, d_model//2]\n",
        "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
        "                        (-np.log(10000.0) / d_model))\n",
        "    \n",
        "    # Apply sin to even indices\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)\n",
        "    # Apply cos to odd indices\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)\n",
        "    \n",
        "    return pe\n",
        "\n",
        "\n",
        "# Example: Create sinusoidal position embeddings\n",
        "max_len = 100\n",
        "d_model = 64\n",
        "sinusoidal_pe = create_sinusoidal_position_embedding(max_len, d_model)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"Sinusoidal/Cosine Position Embedding\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Shape: {sinusoidal_pe.shape}\")\n",
        "print(f\"\\nFirst 5 positions, first 8 dimensions:\")\n",
        "print(sinusoidal_pe[:5, :8])\n",
        "print(f\"\\nLast 5 positions, first 8 dimensions:\")\n",
        "print(sinusoidal_pe[-5:, :8])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "bd5a237b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "RoPE (Rotary Position Embedding)\n",
            "======================================================================\n",
            "Frequencies shape: torch.Size([100, 32])\n",
            "\n",
            "First 5 positions, first 4 frequency pairs:\n",
            "tensor([[0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [1.0000, 0.7499, 0.5623, 0.4217],\n",
            "        [2.0000, 1.4998, 1.1247, 0.8434],\n",
            "        [3.0000, 2.2497, 1.6870, 1.2651],\n",
            "        [4.0000, 2.9996, 2.2494, 1.6868]])\n",
            "\n",
            "Last 5 positions, first 4 frequency pairs:\n",
            "tensor([[95.0000, 71.2400, 53.4224, 40.0612],\n",
            "        [96.0000, 71.9898, 53.9848, 40.4829],\n",
            "        [97.0000, 72.7397, 54.5471, 40.9046],\n",
            "        [98.0000, 73.4896, 55.1095, 41.3263],\n",
            "        [99.0000, 74.2395, 55.6718, 41.7480]])\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# RoPE (Rotary Position Embedding) Implementation\n",
        "# ============================================================================\n",
        "\n",
        "def apply_rope(x: torch.Tensor, freqs: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Apply rotary position embedding to input tensor.\n",
        "    \n",
        "    Args:\n",
        "        x: Input tensor of shape (..., seq_len, d_model)\n",
        "        freqs: Frequencies tensor of shape (seq_len, d_model // 2)\n",
        "        \n",
        "    Returns:\n",
        "        Rotated tensor of same shape as x\n",
        "    \"\"\"\n",
        "    # Reshape x to separate pairs: (..., seq_len, d_model//2, 2)\n",
        "    x_reshaped = x.reshape(*x.shape[:-1], x.shape[-1] // 2, 2)\n",
        "    \n",
        "    # Extract x1 and x2 (real and imaginary parts)\n",
        "    x1, x2 = x_reshaped[..., 0], x_reshaped[..., 1]\n",
        "    \n",
        "    # Expand frequencies to match x shape\n",
        "    # freqs shape: (seq_len, d_model//2) -> (..., seq_len, d_model//2)\n",
        "    freqs_expanded = freqs.unsqueeze(0).expand(*x.shape[:-2], -1, -1)\n",
        "    \n",
        "    # Compute cos and sin\n",
        "    cos_freqs = torch.cos(freqs_expanded)\n",
        "    sin_freqs = torch.sin(freqs_expanded)\n",
        "    \n",
        "    # Apply rotation: [x1', x2'] = [x1*cos - x2*sin, x1*sin + x2*cos]\n",
        "    x1_rotated = x1 * cos_freqs - x2 * sin_freqs\n",
        "    x2_rotated = x1 * sin_freqs + x2 * cos_freqs\n",
        "    \n",
        "    # Stack and reshape back\n",
        "    x_rotated = torch.stack([x1_rotated, x2_rotated], dim=-1)\n",
        "    x_rotated = x_rotated.reshape(*x.shape)\n",
        "    \n",
        "    return x_rotated\n",
        "\n",
        "\n",
        "def create_rope_frequencies(max_len: int, d_model: int, theta: float = 10000.0):\n",
        "    \"\"\"\n",
        "    Create frequencies for RoPE.\n",
        "    \n",
        "    Args:\n",
        "        max_len: Maximum sequence length\n",
        "        d_model: Model dimension (must be even)\n",
        "        theta: Base frequency parameter\n",
        "        \n",
        "    Returns:\n",
        "        Frequencies tensor of shape (max_len, d_model // 2)\n",
        "    \"\"\"\n",
        "    # Create position indices\n",
        "    positions = torch.arange(0, max_len, dtype=torch.float32)\n",
        "    \n",
        "    # Create dimension indices for pairs\n",
        "    dims = torch.arange(0, d_model, 2, dtype=torch.float32)\n",
        "    \n",
        "    # Compute frequencies: theta^(-2i/d_model) for each dimension pair\n",
        "    inv_freq = 1.0 / (theta ** (dims / d_model))\n",
        "    \n",
        "    # Outer product: position * inv_freq\n",
        "    freqs = torch.outer(positions, inv_freq)\n",
        "    \n",
        "    return freqs\n",
        "\n",
        "\n",
        "# Example: Create RoPE frequencies\n",
        "rope_freqs = create_rope_frequencies(max_len, d_model)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"RoPE (Rotary Position Embedding)\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Frequencies shape: {rope_freqs.shape}\")\n",
        "print(f\"\\nFirst 5 positions, first 4 frequency pairs:\")\n",
        "print(rope_freqs[:5, :4])\n",
        "print(f\"\\nLast 5 positions, first 4 frequency pairs:\")\n",
        "print(rope_freqs[-5:, :4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bb185bc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Visualization: Sinusoidal Position Embeddings\n",
        "# ============================================================================\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# 1. Heatmap of sinusoidal position embeddings\n",
        "sns.heatmap(sinusoidal_pe[:50, :].T, cmap='coolwarm', center=0, \n",
        "            ax=axes[0, 0], cbar_kws={'label': 'Embedding Value'})\n",
        "axes[0, 0].set_title('Sinusoidal Position Embeddings Heatmap\\n(First 50 positions, all dimensions)', \n",
        "                     fontsize=12, fontweight='bold')\n",
        "axes[0, 0].set_xlabel('Position')\n",
        "axes[0, 0].set_ylabel('Dimension')\n",
        "\n",
        "# 2. Plot of first few dimensions across positions\n",
        "for dim in range(0, min(8, d_model), 2):\n",
        "    axes[0, 1].plot(sinusoidal_pe[:50, dim].numpy(), \n",
        "                   label=f'Dim {dim} (sin)', linestyle='-', alpha=0.7)\n",
        "    if dim + 1 < d_model:\n",
        "        axes[0, 1].plot(sinusoidal_pe[:50, dim+1].numpy(), \n",
        "                       label=f'Dim {dim+1} (cos)', linestyle='--', alpha=0.7)\n",
        "axes[0, 1].set_title('Sinusoidal Embeddings Across Positions\\n(First 8 dimensions)', \n",
        "                    fontsize=12, fontweight='bold')\n",
        "axes[0, 1].set_xlabel('Position')\n",
        "axes[0, 1].set_ylabel('Embedding Value')\n",
        "axes[0, 1].legend(loc='upper right', fontsize=8)\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Frequency analysis: Show how different dimensions have different frequencies\n",
        "dim_indices = [0, 2, 4, 8, 16, 32]\n",
        "for dim in dim_indices:\n",
        "    if dim < d_model:\n",
        "        axes[1, 0].plot(sinusoidal_pe[:50, dim].numpy(), \n",
        "                       label=f'Dim {dim}', alpha=0.7)\n",
        "axes[1, 0].set_title('Different Frequencies for Different Dimensions', \n",
        "                     fontsize=12, fontweight='bold')\n",
        "axes[1, 0].set_xlabel('Position')\n",
        "axes[1, 0].set_ylabel('Embedding Value')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# 4. 2D visualization: Position vs Dimension\n",
        "im = axes[1, 1].imshow(sinusoidal_pe[:50, :].T, aspect='auto', cmap='coolwarm', \n",
        "                       interpolation='nearest', origin='lower')\n",
        "axes[1, 1].set_title('2D View: Position Embeddings', fontsize=12, fontweight='bold')\n",
        "axes[1, 1].set_xlabel('Position')\n",
        "axes[1, 1].set_ylabel('Dimension')\n",
        "plt.colorbar(im, ax=axes[1, 1], label='Embedding Value')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cbe45e7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Visualization: RoPE Frequencies\n",
        "# ============================================================================\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# 1. Heatmap of RoPE frequencies\n",
        "sns.heatmap(rope_freqs[:50, :].T, cmap='viridis', \n",
        "            ax=axes[0, 0], cbar_kws={'label': 'Frequency Value'})\n",
        "axes[0, 0].set_title('RoPE Frequencies Heatmap\\n(First 50 positions, all frequency pairs)', \n",
        "                     fontsize=12, fontweight='bold')\n",
        "axes[0, 0].set_xlabel('Position')\n",
        "axes[0, 0].set_ylabel('Frequency Pair Index')\n",
        "\n",
        "# 2. Plot frequencies for different dimension pairs\n",
        "for dim_pair in range(0, min(8, d_model // 2)):\n",
        "    axes[0, 1].plot(rope_freqs[:50, dim_pair].numpy(), \n",
        "                   label=f'Pair {dim_pair}', alpha=0.7)\n",
        "axes[0, 1].set_title('RoPE Frequencies Across Positions\\n(First 8 dimension pairs)', \n",
        "                     fontsize=12, fontweight='bold')\n",
        "axes[0, 1].set_xlabel('Position')\n",
        "axes[0, 1].set_ylabel('Frequency Value')\n",
        "axes[0, 1].legend(loc='upper right', fontsize=8)\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Show how RoPE rotates vectors\n",
        "# Create sample query and key vectors\n",
        "seq_len = 10\n",
        "sample_q = torch.randn(seq_len, d_model)\n",
        "sample_k = torch.randn(seq_len, d_model)\n",
        "\n",
        "# Apply RoPE\n",
        "rope_freqs_sample = create_rope_frequencies(seq_len, d_model)\n",
        "q_rotated = apply_rope(sample_q, rope_freqs_sample)\n",
        "k_rotated = apply_rope(sample_k, rope_freqs_sample)\n",
        "\n",
        "# Compute attention scores (simplified, just dot product for visualization)\n",
        "attention_scores = torch.matmul(q_rotated, k_rotated.transpose(-2, -1))\n",
        "\n",
        "sns.heatmap(attention_scores.detach().numpy(), cmap='viridis', \n",
        "            ax=axes[1, 0], cbar_kws={'label': 'Attention Score'})\n",
        "axes[1, 0].set_title('Attention Scores with RoPE\\n(Query @ Key^T)', \n",
        "                     fontsize=12, fontweight='bold')\n",
        "axes[1, 0].set_xlabel('Key Position')\n",
        "axes[1, 0].set_ylabel('Query Position')\n",
        "\n",
        "# 4. Compare frequencies for different positions\n",
        "positions_to_show = [0, 5, 10, 20, 30]\n",
        "for pos in positions_to_show:\n",
        "    if pos < max_len:\n",
        "        axes[1, 1].plot(rope_freqs[pos, :20].numpy(), \n",
        "                       label=f'Pos {pos}', marker='o', markersize=4, alpha=0.7)\n",
        "axes[1, 1].set_title('RoPE Frequencies for Different Positions\\n(First 20 pairs)', \n",
        "                     fontsize=12, fontweight='bold')\n",
        "axes[1, 1].set_xlabel('Frequency Pair Index')\n",
        "axes[1, 1].set_ylabel('Frequency Value')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f108e910",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Comparison: Sinusoidal vs RoPE\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"Comparison: Sinusoidal vs RoPE Position Embeddings\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Create sample embeddings\n",
        "seq_len = 20\n",
        "d_model = 32\n",
        "token_embeddings = torch.randn(seq_len, d_model)\n",
        "\n",
        "# Sinusoidal: Add position embeddings to token embeddings\n",
        "sinusoidal_pe_sample = create_sinusoidal_position_embedding(seq_len, d_model)\n",
        "embeddings_with_sinusoidal = token_embeddings + sinusoidal_pe_sample\n",
        "\n",
        "# RoPE: Apply rotation during attention (simulated here)\n",
        "rope_freqs_sample = create_rope_frequencies(seq_len, d_model)\n",
        "embeddings_with_rope = apply_rope(token_embeddings, rope_freqs_sample)\n",
        "\n",
        "print(f\"\\n1. SINUSOIDAL POSITION EMBEDDING:\")\n",
        "print(f\"   - Method: Addition (token_emb + pos_emb)\")\n",
        "print(f\"   - Shape: {embeddings_with_sinusoidal.shape}\")\n",
        "print(f\"   - Learnable: No (fixed)\")\n",
        "print(f\"   - First position embedding (first 8 dims): {sinusoidal_pe_sample[0, :8]}\")\n",
        "print(f\"   - Extrapolation: Limited (fixed max_len)\")\n",
        "\n",
        "print(f\"\\n2. ROPE (ROTARY POSITION EMBEDDING):\")\n",
        "print(f\"   - Method: Rotation (applied during attention)\")\n",
        "print(f\"   - Shape: {embeddings_with_rope.shape}\")\n",
        "print(f\"   - Learnable: No (fixed frequencies)\")\n",
        "print(f\"   - First position frequencies (first 4 pairs): {rope_freqs_sample[0, :4]}\")\n",
        "print(f\"   - Extrapolation: Better (relative positions)\")\n",
        "\n",
        "# Visual comparison\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# Original token embeddings\n",
        "im1 = axes[0].imshow(token_embeddings.T, aspect='auto', cmap='coolwarm', \n",
        "                    interpolation='nearest', origin='lower')\n",
        "axes[0].set_title('Original Token Embeddings\\n(No position info)', \n",
        "                 fontsize=12, fontweight='bold')\n",
        "axes[0].set_xlabel('Position')\n",
        "axes[0].set_ylabel('Dimension')\n",
        "plt.colorbar(im1, ax=axes[0], label='Value')\n",
        "\n",
        "# With sinusoidal PE\n",
        "im2 = axes[1].imshow(embeddings_with_sinusoidal.T, aspect='auto', cmap='coolwarm', \n",
        "                    interpolation='nearest', origin='lower')\n",
        "axes[1].set_title('Token + Sinusoidal Position Embedding\\n(Additive)', \n",
        "                 fontsize=12, fontweight='bold')\n",
        "axes[1].set_xlabel('Position')\n",
        "axes[1].set_ylabel('Dimension')\n",
        "plt.colorbar(im2, ax=axes[1], label='Value')\n",
        "\n",
        "# With RoPE (rotated)\n",
        "im3 = axes[2].imshow(embeddings_with_rope.T, aspect='auto', cmap='coolwarm', \n",
        "                    interpolation='nearest', origin='lower')\n",
        "axes[2].set_title('Token with RoPE Applied\\n(Rotational)', \n",
        "                 fontsize=12, fontweight='bold')\n",
        "axes[2].set_xlabel('Position')\n",
        "axes[2].set_ylabel('Dimension')\n",
        "plt.colorbar(im3, ax=axes[2], label='Value')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d66b1e5f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Example: How RoPE Works in Attention\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"RoPE in Attention Mechanism\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Simulate attention with RoPE\n",
        "seq_len = 8\n",
        "d_model = 16\n",
        "batch_size = 1\n",
        "\n",
        "# Create query, key, value\n",
        "q = torch.randn(batch_size, seq_len, d_model)\n",
        "k = torch.randn(batch_size, seq_len, d_model)\n",
        "v = torch.randn(batch_size, seq_len, d_model)\n",
        "\n",
        "# Create RoPE frequencies\n",
        "rope_freqs = create_rope_frequencies(seq_len, d_model)\n",
        "\n",
        "# Apply RoPE to q and k (not v)\n",
        "q_rope = apply_rope(q, rope_freqs)\n",
        "k_rope = apply_rope(k, rope_freqs)\n",
        "\n",
        "# Compute attention scores\n",
        "attention_scores = torch.matmul(q_rope, k_rope.transpose(-2, -1)) / np.sqrt(d_model)\n",
        "attention_weights = torch.softmax(attention_scores, dim=-1)\n",
        "\n",
        "print(f\"Query shape: {q.shape}\")\n",
        "print(f\"Key shape: {k.shape}\")\n",
        "print(f\"After RoPE - Query shape: {q_rope.shape}\")\n",
        "print(f\"After RoPE - Key shape: {k_rope.shape}\")\n",
        "print(f\"\\nAttention scores shape: {attention_scores.shape}\")\n",
        "print(f\"Attention weights shape: {attention_weights.shape}\")\n",
        "\n",
        "# Visualize attention pattern\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Attention scores\n",
        "sns.heatmap(attention_scores[0].detach().numpy(), cmap='viridis', \n",
        "            ax=axes[0], cbar_kws={'label': 'Score'})\n",
        "axes[0].set_title('Attention Scores (with RoPE)\\nQuery @ Key^T', \n",
        "                 fontsize=12, fontweight='bold')\n",
        "axes[0].set_xlabel('Key Position')\n",
        "axes[0].set_ylabel('Query Position')\n",
        "\n",
        "# Attention weights\n",
        "sns.heatmap(attention_weights[0].detach().numpy(), cmap='Blues', \n",
        "            ax=axes[1], cbar_kws={'label': 'Weight'})\n",
        "axes[1].set_title('Attention Weights (Softmax)\\nEach row sums to 1', \n",
        "                 fontsize=12, fontweight='bold')\n",
        "axes[1].set_xlabel('Key Position')\n",
        "axes[1].set_ylabel('Query Position')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Key Insight:\")\n",
        "print(\"=\" * 70)\n",
        "print(\"RoPE encodes relative position information in the attention mechanism.\")\n",
        "print(\"The rotation angle depends on both the position and the dimension,\")\n",
        "print(\"allowing the model to learn relative position patterns naturally.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "127a9232",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "| Feature | Sinusoidal/Cosine | RoPE |\n",
        "|---------|------------------|------|\n",
        "| **Method** | Addition to embeddings | Rotation during attention |\n",
        "| **Learnable** | No (fixed) | No (fixed frequencies) |\n",
        "| **Extrapolation** | Limited to max_len | Better (relative positions) |\n",
        "| **Computation** | Simple addition | Matrix rotation |\n",
        "| **Used in** | Original Transformer, BERT | LLaMA, GPT-NeoX, PaLM |\n",
        "| **Key Advantage** | Simple, interpretable | Better for long sequences |\n",
        "\n",
        "**Key Differences:**\n",
        "1. **Sinusoidal**: Adds fixed position embeddings directly to token embeddings\n",
        "2. **RoPE**: Rotates query/key vectors by angles proportional to position, preserving relative position information\n",
        "3. **RoPE** generally performs better on longer sequences due to its relative position encoding"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
