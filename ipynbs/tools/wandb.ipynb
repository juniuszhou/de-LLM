{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfe9620f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wandb version: 0.23.1\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Weights & Biases (wandb) Usage Guide\n",
    "# ============================================================================\n",
    "# \n",
    "# wandb is a tool for experiment tracking, visualization, and collaboration\n",
    "# \n",
    "# Installation: pip install wandb\n",
    "# Login: wandb login (or set WANDB_API_KEY environment variable)\n",
    "# \n",
    "# Key Features:\n",
    "# - Track metrics, hyperparameters, and system metrics\n",
    "# - Visualize training curves in real-time\n",
    "# - Log images, tables, and other artifacts\n",
    "# - Compare multiple runs\n",
    "# - Share results with team\n",
    "# ============================================================================\n",
    "\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"wandb version:\", wandb.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e75d72a",
   "metadata": {},
   "source": [
    "# 1. Basic Setup and Initialization\n",
    "\n",
    "## Steps to get started:\n",
    "1. **Install**: `pip install wandb`\n",
    "2. **Login**: Run `wandb login` in terminal (or set `WANDB_API_KEY` env var)\n",
    "3. **Initialize**: Call `wandb.init()` with your project name\n",
    "4. **Access UI**: Visit https://wandb.ai to view your runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f93de42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjun\u001b[0m to \u001b[32mhttp://localhost:8080\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user/github/junius/de-LLM/ipynbs/tools/wandb/run-20260110_212656-wxm42l4d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='http://localhost:8080/jun/de-LLM/runs/wxm42l4d' target=\"_blank\">basic-example</a></strong> to <a href='http://localhost:8080/jun/de-LLM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='http://localhost:8080/jun/de-LLM' target=\"_blank\">http://localhost:8080/jun/de-LLM</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='http://localhost:8080/jun/de-LLM/runs/wxm42l4d' target=\"_blank\">http://localhost:8080/jun/de-LLM/runs/wxm42l4d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID: wxm42l4d\n",
      "Run URL: http://localhost:8080/jun/de-LLM/runs/wxm42l4d\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁</td></tr><tr><td>loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.85</td></tr><tr><td>loss</td><td>0.5</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">basic-example</strong> at: <a href='http://localhost:8080/jun/de-LLM/runs/wxm42l4d' target=\"_blank\">http://localhost:8080/jun/de-LLM/runs/wxm42l4d</a><br> View project at: <a href='http://localhost:8080/jun/de-LLM' target=\"_blank\">http://localhost:8080/jun/de-LLM</a><br>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20260110_212656-wxm42l4d/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Example 1: Basic Initialization\n",
    "# ============================================================================\n",
    "\n",
    "# Initialize a run\n",
    "# This creates a new run in your project and returns a run object\n",
    "run = wandb.init(\n",
    "    project=\"de-LLM\",                    # Project name (creates/uses existing project)\n",
    "    name=\"basic-example\",                # Run name (optional, auto-generated if not provided)\n",
    "    notes=\"Learning wandb basics\",       # Notes about this run\n",
    "    tags=[\"tutorial\", \"basic\"],          # Tags for filtering runs\n",
    "    config={                              # Hyperparameters/config\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"batch_size\": 32,\n",
    "        \"epochs\": 10,\n",
    "        \"model\": \"simple-nn\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Run ID: {run.id}\")\n",
    "print(f\"Run URL: {run.url}\")  # Click this URL to view in browser!\n",
    "\n",
    "# Log a simple metric\n",
    "wandb.log({\"loss\": 0.5, \"accuracy\": 0.85})\n",
    "\n",
    "# Finish the run (saves all data)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9ab7414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user/github/junius/de-LLM/ipynbs/tools/wandb/run-20260110_212705-sy9bwjzo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='http://localhost:8080/jun/de-LLM/runs/sy9bwjzo' target=\"_blank\">training-example</a></strong> to <a href='http://localhost:8080/jun/de-LLM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='http://localhost:8080/jun/de-LLM' target=\"_blank\">http://localhost:8080/jun/de-LLM</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='http://localhost:8080/jun/de-LLM/runs/sy9bwjzo' target=\"_blank\">http://localhost:8080/jun/de-LLM/runs/sy9bwjzo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss=0.925, Val Acc=0.471\n",
      "Epoch 1: Train Loss=0.525, Val Acc=0.503\n",
      "Epoch 2: Train Loss=0.149, Val Acc=0.734\n",
      "Epoch 3: Train Loss=0.230, Val Acc=0.791\n",
      "Epoch 4: Train Loss=0.110, Val Acc=0.920\n",
      "Epoch 5: Train Loss=0.133, Val Acc=0.938\n",
      "Epoch 6: Train Loss=0.077, Val Acc=0.967\n",
      "Epoch 7: Train Loss=0.096, Val Acc=1.210\n",
      "Epoch 8: Train Loss=-0.052, Val Acc=1.212\n",
      "Epoch 9: Train Loss=-0.076, Val Acc=1.377\n",
      "Epoch 10: Train Loss=0.224, Val Acc=1.398\n",
      "Epoch 11: Train Loss=0.102, Val Acc=1.491\n",
      "Epoch 12: Train Loss=-0.024, Val Acc=1.677\n",
      "Epoch 13: Train Loss=0.029, Val Acc=1.723\n",
      "Epoch 14: Train Loss=0.095, Val Acc=1.841\n",
      "Epoch 15: Train Loss=-0.008, Val Acc=1.967\n",
      "Epoch 16: Train Loss=0.106, Val Acc=2.133\n",
      "Epoch 17: Train Loss=0.219, Val Acc=2.127\n",
      "Epoch 18: Train Loss=-0.009, Val Acc=2.238\n",
      "Epoch 19: Train Loss=0.107, Val Acc=2.354\n",
      "Epoch 20: Train Loss=0.259, Val Acc=2.449\n",
      "Epoch 21: Train Loss=-0.163, Val Acc=2.466\n",
      "Epoch 22: Train Loss=0.054, Val Acc=2.621\n",
      "Epoch 23: Train Loss=-0.010, Val Acc=2.812\n",
      "Epoch 24: Train Loss=0.201, Val Acc=2.829\n",
      "Epoch 25: Train Loss=0.081, Val Acc=2.871\n",
      "Epoch 26: Train Loss=0.031, Val Acc=3.029\n",
      "Epoch 27: Train Loss=0.025, Val Acc=3.140\n",
      "Epoch 28: Train Loss=-0.116, Val Acc=3.281\n",
      "Epoch 29: Train Loss=-0.220, Val Acc=3.381\n",
      "Epoch 30: Train Loss=-0.005, Val Acc=3.481\n",
      "Epoch 31: Train Loss=-0.198, Val Acc=3.519\n",
      "Epoch 32: Train Loss=0.096, Val Acc=3.719\n",
      "Epoch 33: Train Loss=-0.009, Val Acc=3.727\n",
      "Epoch 34: Train Loss=0.020, Val Acc=3.895\n",
      "Epoch 35: Train Loss=-0.117, Val Acc=3.993\n",
      "Epoch 36: Train Loss=-0.080, Val Acc=4.066\n",
      "Epoch 37: Train Loss=-0.110, Val Acc=4.054\n",
      "Epoch 38: Train Loss=-0.014, Val Acc=4.270\n",
      "Epoch 39: Train Loss=-0.117, Val Acc=4.303\n",
      "Epoch 40: Train Loss=0.125, Val Acc=4.409\n",
      "Epoch 41: Train Loss=-0.036, Val Acc=4.630\n",
      "Epoch 42: Train Loss=-0.042, Val Acc=4.647\n",
      "Epoch 43: Train Loss=-0.102, Val Acc=4.816\n",
      "Epoch 44: Train Loss=-0.050, Val Acc=4.869\n",
      "Epoch 45: Train Loss=-0.003, Val Acc=4.983\n",
      "Epoch 46: Train Loss=-0.084, Val Acc=5.093\n",
      "Epoch 47: Train Loss=-0.123, Val Acc=5.143\n",
      "Epoch 48: Train Loss=-0.081, Val Acc=5.251\n",
      "Epoch 49: Train Loss=0.087, Val Acc=5.319\n",
      "Epoch 50: Train Loss=-0.005, Val Acc=5.427\n",
      "Epoch 51: Train Loss=0.091, Val Acc=5.461\n",
      "Epoch 52: Train Loss=-0.039, Val Acc=5.596\n",
      "Epoch 53: Train Loss=0.102, Val Acc=5.705\n",
      "Epoch 54: Train Loss=0.086, Val Acc=5.829\n",
      "Epoch 55: Train Loss=0.062, Val Acc=5.893\n",
      "Epoch 56: Train Loss=0.021, Val Acc=5.992\n",
      "Epoch 57: Train Loss=0.075, Val Acc=6.229\n",
      "Epoch 58: Train Loss=0.187, Val Acc=6.219\n",
      "Epoch 59: Train Loss=0.013, Val Acc=6.359\n",
      "Epoch 60: Train Loss=-0.063, Val Acc=6.520\n",
      "Epoch 61: Train Loss=-0.009, Val Acc=6.523\n",
      "Epoch 62: Train Loss=0.043, Val Acc=6.639\n",
      "Epoch 63: Train Loss=0.050, Val Acc=6.797\n",
      "Epoch 64: Train Loss=0.149, Val Acc=6.895\n",
      "Epoch 65: Train Loss=0.129, Val Acc=6.958\n",
      "Epoch 66: Train Loss=0.165, Val Acc=7.048\n",
      "Epoch 67: Train Loss=0.134, Val Acc=7.198\n",
      "Epoch 68: Train Loss=0.012, Val Acc=7.276\n",
      "Epoch 69: Train Loss=-0.089, Val Acc=7.341\n",
      "Epoch 70: Train Loss=-0.029, Val Acc=7.441\n",
      "Epoch 71: Train Loss=-0.014, Val Acc=7.621\n",
      "Epoch 72: Train Loss=-0.069, Val Acc=7.636\n",
      "Epoch 73: Train Loss=-0.104, Val Acc=7.722\n",
      "Epoch 74: Train Loss=-0.053, Val Acc=7.817\n",
      "Epoch 75: Train Loss=0.189, Val Acc=7.952\n",
      "Epoch 76: Train Loss=0.065, Val Acc=8.045\n",
      "Epoch 77: Train Loss=0.022, Val Acc=8.157\n",
      "Epoch 78: Train Loss=-0.010, Val Acc=8.222\n",
      "Epoch 79: Train Loss=0.044, Val Acc=8.338\n",
      "Epoch 80: Train Loss=0.012, Val Acc=8.469\n",
      "Epoch 81: Train Loss=-0.022, Val Acc=8.554\n",
      "Epoch 82: Train Loss=0.136, Val Acc=8.720\n",
      "Epoch 83: Train Loss=-0.106, Val Acc=8.811\n",
      "Epoch 84: Train Loss=-0.055, Val Acc=8.910\n",
      "Epoch 85: Train Loss=0.244, Val Acc=8.870\n",
      "Epoch 86: Train Loss=0.060, Val Acc=8.959\n",
      "Epoch 87: Train Loss=0.097, Val Acc=9.167\n",
      "Epoch 88: Train Loss=0.178, Val Acc=9.304\n",
      "Epoch 89: Train Loss=0.075, Val Acc=9.314\n",
      "Epoch 90: Train Loss=0.029, Val Acc=9.440\n",
      "Epoch 91: Train Loss=0.070, Val Acc=9.550\n",
      "Epoch 92: Train Loss=0.037, Val Acc=9.734\n",
      "Epoch 93: Train Loss=-0.088, Val Acc=9.723\n",
      "Epoch 94: Train Loss=-0.173, Val Acc=9.863\n",
      "Epoch 95: Train Loss=0.028, Val Acc=9.914\n",
      "Epoch 96: Train Loss=0.057, Val Acc=10.050\n",
      "Epoch 97: Train Loss=-0.064, Val Acc=10.203\n",
      "Epoch 98: Train Loss=-0.072, Val Acc=10.267\n",
      "Epoch 99: Train Loss=0.101, Val Acc=10.421\n",
      "Epoch 100: Train Loss=0.042, Val Acc=10.391\n",
      "Epoch 101: Train Loss=0.053, Val Acc=10.559\n",
      "Epoch 102: Train Loss=0.126, Val Acc=10.628\n",
      "Epoch 103: Train Loss=0.133, Val Acc=10.823\n",
      "Epoch 104: Train Loss=0.070, Val Acc=10.876\n",
      "Epoch 105: Train Loss=-0.000, Val Acc=11.082\n",
      "Epoch 106: Train Loss=-0.018, Val Acc=11.136\n",
      "Epoch 107: Train Loss=0.032, Val Acc=11.111\n",
      "Epoch 108: Train Loss=0.011, Val Acc=11.284\n",
      "Epoch 109: Train Loss=0.034, Val Acc=11.345\n",
      "Epoch 110: Train Loss=-0.061, Val Acc=11.401\n",
      "Epoch 111: Train Loss=0.024, Val Acc=11.607\n",
      "Epoch 112: Train Loss=0.153, Val Acc=11.732\n",
      "Epoch 113: Train Loss=0.021, Val Acc=11.675\n",
      "Epoch 114: Train Loss=-0.013, Val Acc=11.794\n",
      "Epoch 115: Train Loss=-0.038, Val Acc=12.028\n",
      "Epoch 116: Train Loss=0.109, Val Acc=12.104\n",
      "Epoch 117: Train Loss=0.016, Val Acc=12.204\n",
      "Epoch 118: Train Loss=-0.036, Val Acc=12.311\n",
      "Epoch 119: Train Loss=0.155, Val Acc=12.391\n",
      "Epoch 120: Train Loss=-0.018, Val Acc=12.434\n",
      "Epoch 121: Train Loss=0.102, Val Acc=12.531\n",
      "Epoch 122: Train Loss=0.063, Val Acc=12.697\n",
      "Epoch 123: Train Loss=-0.000, Val Acc=12.719\n",
      "Epoch 124: Train Loss=-0.060, Val Acc=12.846\n",
      "Epoch 125: Train Loss=0.007, Val Acc=13.006\n",
      "Epoch 126: Train Loss=0.055, Val Acc=12.980\n",
      "Epoch 127: Train Loss=0.052, Val Acc=13.115\n",
      "Epoch 128: Train Loss=-0.047, Val Acc=13.230\n",
      "Epoch 129: Train Loss=-0.070, Val Acc=13.387\n",
      "Epoch 130: Train Loss=-0.038, Val Acc=13.379\n",
      "Epoch 131: Train Loss=-0.038, Val Acc=13.502\n",
      "Epoch 132: Train Loss=-0.048, Val Acc=13.685\n",
      "Epoch 133: Train Loss=0.072, Val Acc=13.712\n",
      "Epoch 134: Train Loss=0.056, Val Acc=13.921\n",
      "Epoch 135: Train Loss=-0.078, Val Acc=13.959\n",
      "Epoch 136: Train Loss=-0.212, Val Acc=14.077\n",
      "Epoch 137: Train Loss=-0.092, Val Acc=14.118\n",
      "Epoch 138: Train Loss=-0.157, Val Acc=14.316\n",
      "Epoch 139: Train Loss=-0.086, Val Acc=14.445\n",
      "Epoch 140: Train Loss=0.068, Val Acc=14.379\n",
      "Epoch 141: Train Loss=0.125, Val Acc=14.529\n",
      "Epoch 142: Train Loss=-0.007, Val Acc=14.647\n",
      "Epoch 143: Train Loss=-0.028, Val Acc=14.741\n",
      "Epoch 144: Train Loss=-0.020, Val Acc=14.970\n",
      "Epoch 145: Train Loss=0.015, Val Acc=14.880\n",
      "Epoch 146: Train Loss=-0.017, Val Acc=14.984\n",
      "Epoch 147: Train Loss=0.070, Val Acc=15.237\n",
      "Epoch 148: Train Loss=-0.090, Val Acc=15.241\n",
      "Epoch 149: Train Loss=0.069, Val Acc=15.414\n",
      "Epoch 150: Train Loss=0.167, Val Acc=15.450\n",
      "Epoch 151: Train Loss=-0.016, Val Acc=15.562\n",
      "Epoch 152: Train Loss=0.070, Val Acc=15.627\n",
      "Epoch 153: Train Loss=0.163, Val Acc=15.771\n",
      "Epoch 154: Train Loss=-0.110, Val Acc=15.738\n",
      "Epoch 155: Train Loss=-0.203, Val Acc=15.962\n",
      "Epoch 156: Train Loss=-0.015, Val Acc=16.080\n",
      "Epoch 157: Train Loss=0.154, Val Acc=16.141\n",
      "Epoch 158: Train Loss=-0.029, Val Acc=16.193\n",
      "Epoch 159: Train Loss=-0.072, Val Acc=16.307\n",
      "Epoch 160: Train Loss=0.044, Val Acc=16.420\n",
      "Epoch 161: Train Loss=0.066, Val Acc=16.591\n",
      "Epoch 162: Train Loss=-0.006, Val Acc=16.697\n",
      "Epoch 163: Train Loss=0.046, Val Acc=16.873\n",
      "Epoch 164: Train Loss=0.100, Val Acc=16.860\n",
      "Epoch 165: Train Loss=-0.031, Val Acc=16.963\n",
      "Epoch 166: Train Loss=-0.077, Val Acc=17.019\n",
      "Epoch 167: Train Loss=-0.037, Val Acc=17.192\n",
      "Epoch 168: Train Loss=0.004, Val Acc=17.286\n",
      "Epoch 169: Train Loss=0.145, Val Acc=17.337\n",
      "Epoch 170: Train Loss=-0.246, Val Acc=17.481\n",
      "Epoch 171: Train Loss=-0.100, Val Acc=17.534\n",
      "Epoch 172: Train Loss=0.085, Val Acc=17.725\n",
      "Epoch 173: Train Loss=0.056, Val Acc=17.786\n",
      "Epoch 174: Train Loss=0.097, Val Acc=17.793\n",
      "Epoch 175: Train Loss=0.024, Val Acc=17.926\n",
      "Epoch 176: Train Loss=0.095, Val Acc=18.067\n",
      "Epoch 177: Train Loss=-0.022, Val Acc=18.125\n",
      "Epoch 178: Train Loss=-0.128, Val Acc=18.264\n",
      "Epoch 179: Train Loss=-0.010, Val Acc=18.353\n",
      "Epoch 180: Train Loss=-0.074, Val Acc=18.440\n",
      "Epoch 181: Train Loss=-0.075, Val Acc=18.656\n",
      "Epoch 182: Train Loss=-0.043, Val Acc=18.684\n",
      "Epoch 183: Train Loss=0.054, Val Acc=18.850\n",
      "Epoch 184: Train Loss=0.011, Val Acc=18.873\n",
      "Epoch 185: Train Loss=0.017, Val Acc=18.958\n",
      "Epoch 186: Train Loss=0.172, Val Acc=18.990\n",
      "Epoch 187: Train Loss=-0.020, Val Acc=19.147\n",
      "Epoch 188: Train Loss=-0.046, Val Acc=19.219\n",
      "Epoch 189: Train Loss=-0.116, Val Acc=19.273\n",
      "Epoch 190: Train Loss=-0.057, Val Acc=19.507\n",
      "Epoch 191: Train Loss=-0.062, Val Acc=19.517\n",
      "Epoch 192: Train Loss=0.104, Val Acc=19.591\n",
      "Epoch 193: Train Loss=0.018, Val Acc=19.704\n",
      "Epoch 194: Train Loss=0.129, Val Acc=19.827\n",
      "Epoch 195: Train Loss=0.067, Val Acc=19.838\n",
      "Epoch 196: Train Loss=-0.099, Val Acc=20.012\n",
      "Epoch 197: Train Loss=0.232, Val Acc=20.154\n",
      "Epoch 198: Train Loss=-0.137, Val Acc=20.281\n",
      "Epoch 199: Train Loss=-0.211, Val Acc=20.310\n",
      "Epoch 200: Train Loss=0.026, Val Acc=20.410\n",
      "Epoch 201: Train Loss=-0.045, Val Acc=20.554\n",
      "Epoch 202: Train Loss=-0.101, Val Acc=20.657\n",
      "Epoch 203: Train Loss=0.088, Val Acc=20.795\n",
      "Epoch 204: Train Loss=-0.071, Val Acc=20.798\n",
      "Epoch 205: Train Loss=-0.049, Val Acc=20.969\n",
      "Epoch 206: Train Loss=0.048, Val Acc=21.011\n",
      "Epoch 207: Train Loss=0.150, Val Acc=21.117\n",
      "Epoch 208: Train Loss=-0.003, Val Acc=21.323\n",
      "Epoch 209: Train Loss=0.099, Val Acc=21.373\n",
      "Epoch 210: Train Loss=-0.099, Val Acc=21.464\n",
      "Epoch 211: Train Loss=-0.061, Val Acc=21.597\n",
      "Epoch 212: Train Loss=0.014, Val Acc=21.579\n",
      "Epoch 213: Train Loss=0.005, Val Acc=21.740\n",
      "Epoch 214: Train Loss=-0.054, Val Acc=21.871\n",
      "Epoch 215: Train Loss=0.087, Val Acc=21.879\n",
      "Epoch 216: Train Loss=-0.040, Val Acc=21.947\n",
      "Epoch 217: Train Loss=0.021, Val Acc=22.098\n",
      "Epoch 218: Train Loss=0.075, Val Acc=22.166\n",
      "Epoch 219: Train Loss=-0.063, Val Acc=22.341\n",
      "Epoch 220: Train Loss=-0.105, Val Acc=22.524\n",
      "Epoch 221: Train Loss=0.225, Val Acc=22.584\n",
      "Epoch 222: Train Loss=0.017, Val Acc=22.643\n",
      "Epoch 223: Train Loss=0.049, Val Acc=22.723\n",
      "Epoch 224: Train Loss=-0.104, Val Acc=22.942\n",
      "Epoch 225: Train Loss=0.194, Val Acc=22.939\n",
      "Epoch 226: Train Loss=-0.027, Val Acc=23.157\n",
      "Epoch 227: Train Loss=-0.024, Val Acc=23.121\n",
      "Epoch 228: Train Loss=0.065, Val Acc=23.294\n",
      "Epoch 229: Train Loss=-0.070, Val Acc=23.313\n",
      "Epoch 230: Train Loss=-0.126, Val Acc=23.558\n",
      "Epoch 231: Train Loss=-0.017, Val Acc=23.657\n",
      "Epoch 232: Train Loss=0.059, Val Acc=23.723\n",
      "Epoch 233: Train Loss=-0.030, Val Acc=23.710\n",
      "Epoch 234: Train Loss=-0.063, Val Acc=23.737\n",
      "Epoch 235: Train Loss=-0.051, Val Acc=23.981\n",
      "Epoch 236: Train Loss=-0.072, Val Acc=24.013\n",
      "Epoch 237: Train Loss=0.058, Val Acc=24.213\n",
      "Epoch 238: Train Loss=0.171, Val Acc=24.276\n",
      "Epoch 239: Train Loss=-0.128, Val Acc=24.350\n",
      "Epoch 240: Train Loss=0.041, Val Acc=24.473\n",
      "Epoch 241: Train Loss=-0.050, Val Acc=24.600\n",
      "Epoch 242: Train Loss=0.079, Val Acc=24.642\n",
      "Epoch 243: Train Loss=-0.087, Val Acc=24.759\n",
      "Epoch 244: Train Loss=0.002, Val Acc=24.835\n",
      "Epoch 245: Train Loss=-0.155, Val Acc=24.997\n",
      "Epoch 246: Train Loss=-0.040, Val Acc=25.036\n",
      "Epoch 247: Train Loss=0.082, Val Acc=25.224\n",
      "Epoch 248: Train Loss=-0.078, Val Acc=25.158\n",
      "Epoch 249: Train Loss=0.079, Val Acc=25.421\n",
      "Epoch 250: Train Loss=0.041, Val Acc=25.462\n",
      "Epoch 251: Train Loss=-0.187, Val Acc=25.513\n",
      "Epoch 252: Train Loss=0.116, Val Acc=25.635\n",
      "Epoch 253: Train Loss=-0.114, Val Acc=25.710\n",
      "Epoch 254: Train Loss=0.007, Val Acc=25.921\n",
      "Epoch 255: Train Loss=-0.170, Val Acc=25.974\n",
      "Epoch 256: Train Loss=-0.047, Val Acc=26.090\n",
      "Epoch 257: Train Loss=0.044, Val Acc=26.175\n",
      "Epoch 258: Train Loss=-0.064, Val Acc=26.178\n",
      "Epoch 259: Train Loss=0.081, Val Acc=26.339\n",
      "Epoch 260: Train Loss=-0.006, Val Acc=26.351\n",
      "Epoch 261: Train Loss=0.108, Val Acc=26.523\n",
      "Epoch 262: Train Loss=0.063, Val Acc=26.639\n",
      "Epoch 263: Train Loss=0.043, Val Acc=26.785\n",
      "Epoch 264: Train Loss=-0.248, Val Acc=26.893\n",
      "Epoch 265: Train Loss=-0.040, Val Acc=26.854\n",
      "Epoch 266: Train Loss=-0.089, Val Acc=27.020\n",
      "Epoch 267: Train Loss=-0.178, Val Acc=27.191\n",
      "Epoch 268: Train Loss=0.043, Val Acc=27.255\n",
      "Epoch 269: Train Loss=0.001, Val Acc=27.444\n",
      "Epoch 270: Train Loss=0.177, Val Acc=27.426\n",
      "Epoch 271: Train Loss=-0.039, Val Acc=27.451\n",
      "Epoch 272: Train Loss=0.149, Val Acc=27.618\n",
      "Epoch 273: Train Loss=0.043, Val Acc=27.687\n",
      "Epoch 274: Train Loss=0.017, Val Acc=27.833\n",
      "Epoch 275: Train Loss=-0.119, Val Acc=27.905\n",
      "Epoch 276: Train Loss=0.149, Val Acc=28.046\n",
      "Epoch 277: Train Loss=0.022, Val Acc=28.194\n",
      "Epoch 278: Train Loss=0.115, Val Acc=28.234\n",
      "Epoch 279: Train Loss=-0.089, Val Acc=28.320\n",
      "Epoch 280: Train Loss=-0.003, Val Acc=28.419\n",
      "Epoch 281: Train Loss=-0.023, Val Acc=28.513\n",
      "Epoch 282: Train Loss=0.031, Val Acc=28.604\n",
      "Epoch 283: Train Loss=0.096, Val Acc=28.756\n",
      "Epoch 284: Train Loss=-0.121, Val Acc=28.847\n",
      "Epoch 285: Train Loss=-0.011, Val Acc=28.941\n",
      "Epoch 286: Train Loss=-0.057, Val Acc=29.048\n",
      "Epoch 287: Train Loss=-0.010, Val Acc=29.168\n",
      "Epoch 288: Train Loss=-0.049, Val Acc=29.149\n",
      "Epoch 289: Train Loss=-0.072, Val Acc=29.305\n",
      "Epoch 290: Train Loss=-0.056, Val Acc=29.486\n",
      "Epoch 291: Train Loss=-0.036, Val Acc=29.525\n",
      "Epoch 292: Train Loss=0.072, Val Acc=29.616\n",
      "Epoch 293: Train Loss=-0.041, Val Acc=29.764\n",
      "Epoch 294: Train Loss=0.134, Val Acc=29.933\n",
      "Epoch 295: Train Loss=0.064, Val Acc=29.923\n",
      "Epoch 296: Train Loss=-0.094, Val Acc=30.096\n",
      "Epoch 297: Train Loss=-0.095, Val Acc=30.214\n",
      "Epoch 298: Train Loss=0.001, Val Acc=30.218\n",
      "Epoch 299: Train Loss=0.072, Val Acc=30.294\n",
      "Epoch 300: Train Loss=-0.159, Val Acc=30.438\n",
      "Epoch 301: Train Loss=-0.017, Val Acc=30.501\n",
      "Epoch 302: Train Loss=0.065, Val Acc=30.662\n",
      "Epoch 303: Train Loss=-0.133, Val Acc=30.799\n",
      "Epoch 304: Train Loss=0.141, Val Acc=30.909\n",
      "Epoch 305: Train Loss=-0.021, Val Acc=30.874\n",
      "Epoch 306: Train Loss=0.089, Val Acc=31.031\n",
      "Epoch 307: Train Loss=-0.147, Val Acc=31.197\n",
      "Epoch 308: Train Loss=0.075, Val Acc=31.303\n",
      "Epoch 309: Train Loss=0.096, Val Acc=31.333\n",
      "Epoch 310: Train Loss=-0.012, Val Acc=31.451\n",
      "Epoch 311: Train Loss=0.075, Val Acc=31.523\n",
      "Epoch 312: Train Loss=0.095, Val Acc=31.650\n",
      "Epoch 313: Train Loss=-0.247, Val Acc=31.728\n",
      "Epoch 314: Train Loss=-0.023, Val Acc=31.787\n",
      "Epoch 315: Train Loss=-0.063, Val Acc=31.920\n",
      "Epoch 316: Train Loss=-0.030, Val Acc=32.078\n",
      "Epoch 317: Train Loss=0.058, Val Acc=32.113\n",
      "Epoch 318: Train Loss=-0.136, Val Acc=32.240\n",
      "Epoch 319: Train Loss=-0.072, Val Acc=32.335\n",
      "Epoch 320: Train Loss=0.022, Val Acc=32.435\n",
      "Epoch 321: Train Loss=0.055, Val Acc=32.529\n",
      "Epoch 322: Train Loss=-0.036, Val Acc=32.576\n",
      "Epoch 323: Train Loss=0.079, Val Acc=32.746\n",
      "Epoch 324: Train Loss=-0.038, Val Acc=32.861\n",
      "Epoch 325: Train Loss=0.208, Val Acc=33.049\n",
      "Epoch 326: Train Loss=0.146, Val Acc=33.059\n",
      "Epoch 327: Train Loss=-0.015, Val Acc=33.156\n",
      "Epoch 328: Train Loss=0.059, Val Acc=33.260\n",
      "Epoch 329: Train Loss=0.031, Val Acc=33.457\n",
      "Epoch 330: Train Loss=-0.027, Val Acc=33.396\n",
      "Epoch 331: Train Loss=0.002, Val Acc=33.523\n",
      "Epoch 332: Train Loss=0.119, Val Acc=33.644\n",
      "Epoch 333: Train Loss=-0.088, Val Acc=33.723\n",
      "Epoch 334: Train Loss=-0.058, Val Acc=33.972\n",
      "Epoch 335: Train Loss=-0.108, Val Acc=33.935\n",
      "Epoch 336: Train Loss=-0.121, Val Acc=34.004\n",
      "Epoch 337: Train Loss=-0.064, Val Acc=34.084\n",
      "Epoch 338: Train Loss=-0.011, Val Acc=34.291\n",
      "Epoch 339: Train Loss=-0.088, Val Acc=34.351\n",
      "Epoch 340: Train Loss=-0.065, Val Acc=34.531\n",
      "Epoch 341: Train Loss=0.166, Val Acc=34.549\n",
      "Epoch 342: Train Loss=0.011, Val Acc=34.585\n",
      "Epoch 343: Train Loss=0.018, Val Acc=34.858\n",
      "Epoch 344: Train Loss=-0.008, Val Acc=34.812\n",
      "Epoch 345: Train Loss=0.086, Val Acc=34.910\n",
      "Epoch 346: Train Loss=0.017, Val Acc=35.063\n",
      "Epoch 347: Train Loss=-0.041, Val Acc=35.248\n",
      "Epoch 348: Train Loss=0.221, Val Acc=35.202\n",
      "Epoch 349: Train Loss=0.247, Val Acc=35.311\n",
      "Epoch 350: Train Loss=0.014, Val Acc=35.590\n",
      "Epoch 351: Train Loss=-0.023, Val Acc=35.510\n",
      "Epoch 352: Train Loss=0.054, Val Acc=35.573\n",
      "Epoch 353: Train Loss=0.110, Val Acc=35.659\n",
      "Epoch 354: Train Loss=0.027, Val Acc=35.783\n",
      "Epoch 355: Train Loss=-0.029, Val Acc=35.955\n",
      "Epoch 356: Train Loss=0.103, Val Acc=36.036\n",
      "Epoch 357: Train Loss=-0.106, Val Acc=36.139\n",
      "Epoch 358: Train Loss=0.015, Val Acc=36.225\n",
      "Epoch 359: Train Loss=0.116, Val Acc=36.386\n",
      "Epoch 360: Train Loss=0.054, Val Acc=36.338\n",
      "Epoch 361: Train Loss=0.206, Val Acc=36.547\n",
      "Epoch 362: Train Loss=-0.037, Val Acc=36.742\n",
      "Epoch 363: Train Loss=0.007, Val Acc=36.703\n",
      "Epoch 364: Train Loss=0.156, Val Acc=36.896\n",
      "Epoch 365: Train Loss=-0.136, Val Acc=36.865\n",
      "Epoch 366: Train Loss=0.030, Val Acc=37.047\n",
      "Epoch 367: Train Loss=-0.026, Val Acc=37.153\n",
      "Epoch 368: Train Loss=0.253, Val Acc=37.215\n",
      "Epoch 369: Train Loss=0.013, Val Acc=37.402\n",
      "Epoch 370: Train Loss=0.071, Val Acc=37.351\n",
      "Epoch 371: Train Loss=-0.113, Val Acc=37.544\n",
      "Epoch 372: Train Loss=-0.039, Val Acc=37.627\n",
      "Epoch 373: Train Loss=-0.244, Val Acc=37.795\n",
      "Epoch 374: Train Loss=0.100, Val Acc=37.825\n",
      "Epoch 375: Train Loss=0.092, Val Acc=37.940\n",
      "Epoch 376: Train Loss=0.118, Val Acc=38.039\n",
      "Epoch 377: Train Loss=0.090, Val Acc=38.136\n",
      "Epoch 378: Train Loss=-0.009, Val Acc=38.200\n",
      "Epoch 379: Train Loss=-0.026, Val Acc=38.389\n",
      "Epoch 380: Train Loss=-0.152, Val Acc=38.450\n",
      "Epoch 381: Train Loss=-0.030, Val Acc=38.500\n",
      "Epoch 382: Train Loss=0.092, Val Acc=38.710\n",
      "Epoch 383: Train Loss=0.023, Val Acc=38.796\n",
      "Epoch 384: Train Loss=-0.127, Val Acc=38.824\n",
      "Epoch 385: Train Loss=-0.093, Val Acc=38.990\n",
      "Epoch 386: Train Loss=0.120, Val Acc=39.116\n",
      "Epoch 387: Train Loss=0.158, Val Acc=39.143\n",
      "Epoch 388: Train Loss=-0.023, Val Acc=39.217\n",
      "Epoch 389: Train Loss=-0.008, Val Acc=39.264\n",
      "Epoch 390: Train Loss=0.093, Val Acc=39.437\n",
      "Epoch 391: Train Loss=-0.098, Val Acc=39.606\n",
      "Epoch 392: Train Loss=-0.142, Val Acc=39.643\n",
      "Epoch 393: Train Loss=-0.153, Val Acc=39.668\n",
      "Epoch 394: Train Loss=0.210, Val Acc=39.893\n",
      "Epoch 395: Train Loss=0.046, Val Acc=40.014\n",
      "Epoch 396: Train Loss=0.005, Val Acc=40.088\n",
      "Epoch 397: Train Loss=0.048, Val Acc=40.176\n",
      "Epoch 398: Train Loss=-0.051, Val Acc=40.179\n",
      "Epoch 399: Train Loss=-0.199, Val Acc=40.369\n",
      "Epoch 400: Train Loss=-0.121, Val Acc=40.471\n",
      "Epoch 401: Train Loss=0.064, Val Acc=40.496\n",
      "Epoch 402: Train Loss=-0.181, Val Acc=40.666\n",
      "Epoch 403: Train Loss=0.121, Val Acc=40.789\n",
      "Epoch 404: Train Loss=0.049, Val Acc=40.784\n",
      "Epoch 405: Train Loss=-0.109, Val Acc=40.904\n",
      "Epoch 406: Train Loss=0.088, Val Acc=41.102\n",
      "Epoch 407: Train Loss=0.052, Val Acc=41.164\n",
      "Epoch 408: Train Loss=0.236, Val Acc=41.170\n",
      "Epoch 409: Train Loss=0.005, Val Acc=41.382\n",
      "Epoch 410: Train Loss=-0.003, Val Acc=41.521\n",
      "Epoch 411: Train Loss=-0.018, Val Acc=41.517\n",
      "Epoch 412: Train Loss=0.027, Val Acc=41.657\n",
      "Epoch 413: Train Loss=0.109, Val Acc=41.746\n",
      "Epoch 414: Train Loss=0.031, Val Acc=41.846\n",
      "Epoch 415: Train Loss=-0.026, Val Acc=41.950\n",
      "Epoch 416: Train Loss=0.051, Val Acc=42.028\n",
      "Epoch 417: Train Loss=0.115, Val Acc=42.128\n",
      "Epoch 418: Train Loss=0.034, Val Acc=42.199\n",
      "Epoch 419: Train Loss=0.002, Val Acc=42.303\n",
      "Epoch 420: Train Loss=-0.053, Val Acc=42.465\n",
      "Epoch 421: Train Loss=-0.047, Val Acc=42.567\n",
      "Epoch 422: Train Loss=-0.165, Val Acc=42.576\n",
      "Epoch 423: Train Loss=-0.095, Val Acc=42.642\n",
      "Epoch 424: Train Loss=0.083, Val Acc=42.915\n",
      "Epoch 425: Train Loss=0.186, Val Acc=42.949\n",
      "Epoch 426: Train Loss=-0.048, Val Acc=43.020\n",
      "Epoch 427: Train Loss=0.128, Val Acc=43.161\n",
      "Epoch 428: Train Loss=0.235, Val Acc=43.276\n",
      "Epoch 429: Train Loss=-0.096, Val Acc=43.327\n",
      "Epoch 430: Train Loss=-0.137, Val Acc=43.381\n",
      "Epoch 431: Train Loss=-0.043, Val Acc=43.602\n",
      "Epoch 432: Train Loss=-0.073, Val Acc=43.663\n",
      "Epoch 433: Train Loss=0.061, Val Acc=43.793\n",
      "Epoch 434: Train Loss=0.016, Val Acc=43.835\n",
      "Epoch 435: Train Loss=0.019, Val Acc=43.922\n",
      "Epoch 436: Train Loss=0.041, Val Acc=44.130\n",
      "Epoch 437: Train Loss=0.076, Val Acc=44.157\n",
      "Epoch 438: Train Loss=0.035, Val Acc=44.226\n",
      "Epoch 439: Train Loss=0.107, Val Acc=44.345\n",
      "Epoch 440: Train Loss=0.098, Val Acc=44.475\n",
      "Epoch 441: Train Loss=0.133, Val Acc=44.575\n",
      "Epoch 442: Train Loss=-0.082, Val Acc=44.671\n",
      "Epoch 443: Train Loss=0.147, Val Acc=44.790\n",
      "Epoch 444: Train Loss=0.067, Val Acc=44.835\n",
      "Epoch 445: Train Loss=-0.168, Val Acc=44.968\n",
      "Epoch 446: Train Loss=-0.102, Val Acc=45.011\n",
      "Epoch 447: Train Loss=0.072, Val Acc=45.181\n",
      "Epoch 448: Train Loss=-0.084, Val Acc=45.244\n",
      "Epoch 449: Train Loss=0.041, Val Acc=45.431\n",
      "Epoch 450: Train Loss=0.023, Val Acc=45.369\n",
      "Epoch 451: Train Loss=0.020, Val Acc=45.529\n",
      "Epoch 452: Train Loss=-0.158, Val Acc=45.705\n",
      "Epoch 453: Train Loss=-0.165, Val Acc=45.748\n",
      "Epoch 454: Train Loss=0.140, Val Acc=45.861\n",
      "Epoch 455: Train Loss=0.012, Val Acc=45.931\n",
      "Epoch 456: Train Loss=-0.030, Val Acc=46.069\n",
      "Epoch 457: Train Loss=-0.032, Val Acc=46.135\n",
      "Epoch 458: Train Loss=0.032, Val Acc=46.192\n",
      "Epoch 459: Train Loss=0.007, Val Acc=46.385\n",
      "Epoch 460: Train Loss=-0.125, Val Acc=46.387\n",
      "Epoch 461: Train Loss=-0.036, Val Acc=46.535\n",
      "Epoch 462: Train Loss=0.020, Val Acc=46.656\n",
      "Epoch 463: Train Loss=0.013, Val Acc=46.751\n",
      "Epoch 464: Train Loss=0.166, Val Acc=46.935\n",
      "Epoch 465: Train Loss=-0.003, Val Acc=46.880\n",
      "Epoch 466: Train Loss=0.055, Val Acc=47.097\n",
      "Epoch 467: Train Loss=-0.055, Val Acc=47.119\n",
      "Epoch 468: Train Loss=-0.070, Val Acc=47.321\n",
      "Epoch 469: Train Loss=-0.017, Val Acc=47.325\n",
      "Epoch 470: Train Loss=-0.118, Val Acc=47.480\n",
      "Epoch 471: Train Loss=-0.139, Val Acc=47.571\n",
      "Epoch 472: Train Loss=0.104, Val Acc=47.653\n",
      "Epoch 473: Train Loss=-0.044, Val Acc=47.736\n",
      "Epoch 474: Train Loss=0.069, Val Acc=47.876\n",
      "Epoch 475: Train Loss=0.166, Val Acc=47.969\n",
      "Epoch 476: Train Loss=-0.013, Val Acc=48.079\n",
      "Epoch 477: Train Loss=-0.025, Val Acc=48.133\n",
      "Epoch 478: Train Loss=-0.089, Val Acc=48.215\n",
      "Epoch 479: Train Loss=-0.044, Val Acc=48.405\n",
      "Epoch 480: Train Loss=-0.100, Val Acc=48.490\n",
      "Epoch 481: Train Loss=-0.043, Val Acc=48.516\n",
      "Epoch 482: Train Loss=0.003, Val Acc=48.640\n",
      "Epoch 483: Train Loss=0.106, Val Acc=48.738\n",
      "Epoch 484: Train Loss=0.006, Val Acc=48.755\n",
      "Epoch 485: Train Loss=0.039, Val Acc=48.975\n",
      "Epoch 486: Train Loss=0.064, Val Acc=49.120\n",
      "Epoch 487: Train Loss=0.081, Val Acc=49.126\n",
      "Epoch 488: Train Loss=0.104, Val Acc=49.300\n",
      "Epoch 489: Train Loss=-0.013, Val Acc=49.284\n",
      "Epoch 490: Train Loss=-0.121, Val Acc=49.397\n",
      "Epoch 491: Train Loss=0.112, Val Acc=49.566\n",
      "Epoch 492: Train Loss=-0.047, Val Acc=49.707\n",
      "Epoch 493: Train Loss=0.132, Val Acc=49.758\n",
      "Epoch 494: Train Loss=-0.068, Val Acc=49.797\n",
      "Epoch 495: Train Loss=-0.058, Val Acc=49.970\n",
      "Epoch 496: Train Loss=-0.155, Val Acc=50.136\n",
      "Epoch 497: Train Loss=0.140, Val Acc=50.148\n",
      "Epoch 498: Train Loss=-0.062, Val Acc=50.242\n",
      "Epoch 499: Train Loss=-0.124, Val Acc=50.401\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇█</td></tr><tr><td>learning_rate</td><td>█▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/accuracy</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇█</td></tr><tr><td>train/loss</td><td>▇▃▃▄▂▅▇▄▃▅▄▅▅▅▄▅▇▁▇▃▂▅▄▃▅▄▇▆▄▄█▇▃▆▃▆▄▅▃▅</td></tr><tr><td>val/accuracy</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇█████</td></tr><tr><td>val/loss</td><td>█▅▃▄▃▃▄▃▃▃▃▄▁▂▂▃▂▃▃▄▄▃▄▂▂▂▅▄▃▃▃▃▄▃▁▃▁▃▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>499</td></tr><tr><td>learning_rate</td><td>0.0</td></tr><tr><td>train/accuracy</td><td>50.45092</td></tr><tr><td>train/loss</td><td>-0.12351</td></tr><tr><td>val/accuracy</td><td>50.40092</td></tr><tr><td>val/loss</td><td>-0.02351</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">training-example</strong> at: <a href='http://localhost:8080/jun/de-LLM/runs/sy9bwjzo' target=\"_blank\">http://localhost:8080/jun/de-LLM/runs/sy9bwjzo</a><br> View project at: <a href='http://localhost:8080/jun/de-LLM' target=\"_blank\">http://localhost:8080/jun/de-LLM</a><br>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20260110_212705-sy9bwjzo/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Example 2: Training Loop with Metrics Logging\n",
    "# ============================================================================\n",
    "\n",
    "# Simulate a training loop\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "wandb.init(\n",
    "    project=\"de-LLM\",\n",
    "    name=\"training-example\",\n",
    "    config={\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"batch_size\": 64,\n",
    "        \"epochs\": 5,\n",
    "        \"optimizer\": \"Adam\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Simulate training\n",
    "for epoch in range(500):\n",
    "    # Simulate training metrics\n",
    "    train_loss = 1.0 / (epoch + 1) + np.random.normal(0, 0.1)\n",
    "    train_acc = 0.5 + epoch * 0.1 + np.random.normal(0, 0.05)\n",
    "    \n",
    "    # Simulate validation metrics\n",
    "    val_loss = train_loss + 0.1\n",
    "    val_acc = train_acc - 0.05\n",
    "\n",
    "    sleep(0.1)\n",
    "    \n",
    "    # Log metrics (creates time series plots)\n",
    "    # group data according to the keys with \"/\"\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch,\n",
    "        \"train/loss\": train_loss,\n",
    "        \"train/accuracy\": train_acc,\n",
    "        \"val/loss\": val_loss,\n",
    "        \"val/accuracy\": val_acc,\n",
    "        \"learning_rate\": 0.001 * (0.9 ** epoch)  # Learning rate schedule\n",
    "    })\n",
    "    \n",
    "    print(f\"Epoch {epoch}: Train Loss={train_loss:.3f}, Val Acc={val_acc:.3f}\")\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145e3a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Example 3: Logging Images and Plots\n",
    "# ============================================================================\n",
    "\n",
    "wandb.init(\n",
    "    project=\"de-LLM\",\n",
    "    name=\"image-logging-example\",\n",
    "    config={\"plot_type\": \"matplotlib\"}\n",
    ")\n",
    "\n",
    "# Create a simple plot\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "x = np.linspace(0, 10, 100)\n",
    "y = np.sin(x)\n",
    "ax.plot(x, y, label=\"sin(x)\")\n",
    "ax.set_xlabel(\"X\")\n",
    "ax.set_ylabel(\"Y\")\n",
    "ax.set_title(\"Sample Plot\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "# Log the plot\n",
    "wandb.log({\"plot\": wandb.Image(fig)})\n",
    "\n",
    "# Log multiple plots\n",
    "for i in range(3):\n",
    "    fig, ax = plt.subplots()\n",
    "    x = np.linspace(0, 10, 100)\n",
    "    y = np.sin(x + i)\n",
    "    ax.plot(x, y, label=f\"sin(x + {i})\")\n",
    "    ax.legend()\n",
    "    wandb.log({f\"plot_{i}\": wandb.Image(fig)})\n",
    "    plt.close(fig)\n",
    "\n",
    "# Log numpy array as image\n",
    "random_image = np.random.rand(64, 64, 3)\n",
    "wandb.log({\"random_image\": wandb.Image(random_image)})\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd92553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Example 4: Logging Tables (for data analysis)\n",
    "# ============================================================================\n",
    "\n",
    "wandb.init(\n",
    "    project=\"de-LLM\",\n",
    "    name=\"table-logging-example\"\n",
    ")\n",
    "\n",
    "# Create a table with predictions\n",
    "columns = [\"image_id\", \"prediction\", \"ground_truth\", \"confidence\"]\n",
    "data = [\n",
    "    [f\"img_{i}\", f\"class_{np.random.randint(0, 10)}\", f\"class_{np.random.randint(0, 10)}\", np.random.rand()]\n",
    "    for i in range(10)\n",
    "]\n",
    "\n",
    "table = wandb.Table(columns=columns, data=data)\n",
    "wandb.log({\"predictions\": table})\n",
    "\n",
    "# Log confusion matrix as table\n",
    "confusion_matrix = np.random.randint(0, 100, (5, 5))\n",
    "cm_table = wandb.Table(\n",
    "    columns=[f\"class_{i}\" for i in range(5)],\n",
    "    data=confusion_matrix.tolist()\n",
    ")\n",
    "wandb.log({\"confusion_matrix\": cm_table})\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083927ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Example 5: Complete Training Example (PyTorch)\n",
    "# ============================================================================\n",
    "\n",
    "# Simple model\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize wandb with hyperparameters\n",
    "wandb.init(\n",
    "    project=\"de-LLM\",\n",
    "    name=\"pytorch-training-example\",\n",
    "    config={\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"batch_size\": 32,\n",
    "        \"epochs\": 3,\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"model\": \"SimpleMLP\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create model and log architecture\n",
    "model = SimpleModel()\n",
    "wandb.watch(model, log=\"all\", log_freq=10)  # Track gradients and parameters\n",
    "\n",
    "# Create dummy data\n",
    "dummy_input = torch.randn(32, 784)\n",
    "dummy_target = torch.randint(0, 10, (32,))\n",
    "\n",
    "# Training setup\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=wandb.config.learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(wandb.config.epochs):\n",
    "    model.train()\n",
    "    \n",
    "    # Simulate batch training\n",
    "    for batch_idx in range(10):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(dummy_input)\n",
    "        loss = criterion(output, dummy_target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Log metrics every batch\n",
    "        if batch_idx % 5 == 0:\n",
    "            accuracy = (output.argmax(dim=1) == dummy_target).float().mean()\n",
    "            wandb.log({\n",
    "                \"batch_loss\": loss.item(),\n",
    "                \"batch_accuracy\": accuracy.item(),\n",
    "                \"epoch\": epoch,\n",
    "                \"batch\": batch_idx\n",
    "            })\n",
    "    \n",
    "    # Log epoch-level metrics\n",
    "    wandb.log({\n",
    "        \"epoch_loss\": loss.item(),\n",
    "        \"epoch\": epoch\n",
    "    })\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1eee19",
   "metadata": {},
   "source": [
    "# 2. How to View Data in wandb UI\n",
    "\n",
    "## Accessing the UI:\n",
    "\n",
    "1. **Web Interface**: \n",
    "   - Visit https://wandb.ai\n",
    "   - Login with your account\n",
    "   - Select your project (e.g., \"de-LLM\")\n",
    "\n",
    "2. **From Code**:\n",
    "   - Use `run.url` to get direct link to your run\n",
    "   - Or use `wandb.run.url` after initialization\n",
    "\n",
    "## What You Can See in the UI:\n",
    "\n",
    "### Dashboard View:\n",
    "- **Runs Table**: List of all runs with metrics, configs, and status\n",
    "- **Parallel Coordinates**: Visualize hyperparameter relationships\n",
    "- **Scatter Plots**: Compare runs across different metrics\n",
    "\n",
    "### Run Details:\n",
    "- **Metrics Tab**: Time series plots of all logged metrics\n",
    "- **System Tab**: CPU, GPU, memory usage over time\n",
    "- **Logs Tab**: Console output from your training\n",
    "- **Files Tab**: Saved model checkpoints, configs, etc.\n",
    "- **Media Tab**: Images, videos, audio you logged\n",
    "- **Tables Tab**: Data tables you logged\n",
    "\n",
    "### Key Features:\n",
    "- **Compare Runs**: Select multiple runs to compare side-by-side\n",
    "- **Filter Runs**: By tags, config values, or metrics\n",
    "- **Group Runs**: Organize runs by hyperparameters\n",
    "- **Sweep**: Hyperparameter optimization (see Example 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68b340e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Example 6: Hyperparameter Sweep (Automated Search)\n",
    "# ============================================================================\n",
    "\n",
    "# Define sweep configuration\n",
    "sweep_config = {\n",
    "    \"method\": \"grid\",  # or \"random\", \"bayes\"\n",
    "    \"metric\": {\n",
    "        \"name\": \"val_accuracy\",\n",
    "        \"goal\": \"maximize\"\n",
    "    },\n",
    "    \"parameters\": {\n",
    "        \"learning_rate\": {\n",
    "            \"values\": [0.001, 0.01, 0.1]\n",
    "        },\n",
    "        \"batch_size\": {\n",
    "            \"values\": [32, 64, 128]\n",
    "        },\n",
    "        \"epochs\": {\n",
    "            \"value\": 5\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize sweep (uncomment to create sweep)\n",
    "# sweep_id = wandb.sweep(sweep_config, project=\"de-LLM\")\n",
    "\n",
    "# Define training function for sweep\n",
    "def train_sweep():\n",
    "    # Initialize wandb for this sweep run\n",
    "    wandb.init(project=\"de-LLM\")\n",
    "    \n",
    "    # Access hyperparameters from config\n",
    "    lr = wandb.config.learning_rate\n",
    "    batch_size = wandb.config.batch_size\n",
    "    \n",
    "    # Simulate training\n",
    "    for epoch in range(wandb.config.epochs):\n",
    "        train_loss = 1.0 / (epoch + 1) + np.random.normal(0, 0.1)\n",
    "        val_acc = 0.5 + epoch * 0.1 + np.random.normal(0, 0.05)\n",
    "        \n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"val_accuracy\": val_acc\n",
    "        })\n",
    "    \n",
    "    wandb.finish()\n",
    "\n",
    "# Run sweep (uncomment to execute)\n",
    "# wandb.agent(sweep_id, train_sweep, count=10)\n",
    "\n",
    "print(\"Sweep configuration created. Uncomment code to run sweep.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051cceaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Example 7: Logging Model Artifacts\n",
    "# ============================================================================\n",
    "\n",
    "wandb.init(\n",
    "    project=\"de-LLM\",\n",
    "    name=\"artifact-example\"\n",
    ")\n",
    "\n",
    "# Create a simple model\n",
    "model = SimpleModel()\n",
    "\n",
    "# Save model checkpoint\n",
    "torch.save(model.state_dict(), \"model_checkpoint.pth\")\n",
    "\n",
    "# Log as artifact\n",
    "artifact = wandb.Artifact(\"model\", type=\"model\")\n",
    "artifact.add_file(\"model_checkpoint.pth\")\n",
    "wandb.log_artifact(artifact)\n",
    "\n",
    "# Log dataset\n",
    "dataset_artifact = wandb.Artifact(\"dataset\", type=\"dataset\")\n",
    "# Add files to artifact\n",
    "# dataset_artifact.add_dir(\"data/\")\n",
    "wandb.log_artifact(dataset_artifact)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e131dcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Example 8: Best Practices and Tips\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\"\"\n",
    "BEST PRACTICES:\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "1. ✅ Use descriptive run names\n",
    "   wandb.init(name=\"resnet50-lr0.001-bs64\")\n",
    "\n",
    "2. ✅ Log hyperparameters in config\n",
    "   wandb.config.update({\"lr\": 0.001, \"batch_size\": 64})\n",
    "\n",
    "3. ✅ Use namespaces for metrics (with slashes)\n",
    "   wandb.log({\"train/loss\": loss, \"val/loss\": val_loss})\n",
    "\n",
    "4. ✅ Log at appropriate frequency\n",
    "   - Every epoch: wandb.log({\"epoch\": epoch, ...})\n",
    "   - Every N batches: if batch_idx % N == 0: wandb.log(...)\n",
    "\n",
    "5. ✅ Use wandb.watch() for PyTorch models\n",
    "   wandb.watch(model, log=\"all\", log_freq=100)\n",
    "\n",
    "6. ✅ Finish runs properly\n",
    "   wandb.finish()  # Always call this!\n",
    "\n",
    "7. ✅ Use tags for organization\n",
    "   wandb.init(tags=[\"experiment\", \"baseline\"])\n",
    "\n",
    "8. ✅ Log images/videos for debugging\n",
    "   wandb.log({\"predictions\": wandb.Image(image)})\n",
    "\n",
    "9. ✅ Use wandb.alert() for important events\n",
    "   wandb.alert(title=\"Training Complete\", text=\"Model converged!\")\n",
    "\n",
    "10. ✅ Compare runs using UI filters\n",
    "    - Filter by tags, config values, or metrics\n",
    "    - Use parallel coordinates plot for hyperparameter analysis\n",
    "\n",
    "COMMON COMMANDS:\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "Terminal commands:\n",
    "  wandb login                    # Login to wandb\n",
    "  wandb offline                  # Run in offline mode\n",
    "  wandb sync <run_dir>          # Sync offline runs\n",
    "  wandb status                  # Check login status\n",
    "\n",
    "Python API:\n",
    "  wandb.init()                  # Initialize run\n",
    "  wandb.log()                   # Log metrics\n",
    "  wandb.config                  # Access hyperparameters\n",
    "  wandb.watch()                 # Track model gradients\n",
    "  wandb.finish()                # End run\n",
    "  wandb.save()                  # Save files\n",
    "  wandb.log_artifact()         # Log artifacts\n",
    "\n",
    "VIEWING IN UI:\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "1. Go to https://wandb.ai\n",
    "2. Select your project (e.g., \"de-LLM\")\n",
    "3. Click on a run to see:\n",
    "   - Metrics: Time series plots\n",
    "   - System: CPU/GPU usage\n",
    "   - Logs: Console output\n",
    "   - Files: Saved checkpoints\n",
    "   - Media: Images/videos\n",
    "   - Tables: Data tables\n",
    "\n",
    "4. Compare runs:\n",
    "   - Select multiple runs\n",
    "   - Use filters (tags, config, metrics)\n",
    "   - View parallel coordinates plot\n",
    "\n",
    "5. Create reports:\n",
    "   - Share findings with team\n",
    "   - Document experiments\n",
    "   - Track progress over time\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d120b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Example 9: Quick Reference - Minimal Working Example\n",
    "# ============================================================================\n",
    "\n",
    "# Minimal example - copy this template for your projects\n",
    "wandb.init(\n",
    "    project=\"de-LLM\",\n",
    "    name=\"minimal-example\",\n",
    "    config={\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"batch_size\": 32\n",
    "    }\n",
    ")\n",
    "\n",
    "# Log metrics\n",
    "for i in range(10):\n",
    "    wandb.log({\n",
    "        \"loss\": 1.0 / (i + 1),\n",
    "        \"accuracy\": i * 0.1\n",
    "    })\n",
    "\n",
    "# Get run URL (open in browser to see results)\n",
    "print(f\"View your run at: {wandb.run.url}\")\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "print(\"\\n✅ Check the URL above to see your metrics in the wandb UI!\")\n",
    "print(\"   - Metrics tab: See loss and accuracy plots\")\n",
    "print(\"   - System tab: See resource usage\")\n",
    "print(\"   - Config tab: See hyperparameters\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
