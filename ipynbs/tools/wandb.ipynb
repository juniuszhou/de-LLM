{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfe9620f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wandb version: 0.23.1\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Weights & Biases (wandb) Usage Guide\n",
    "# ============================================================================\n",
    "# \n",
    "# wandb is a tool for experiment tracking, visualization, and collaboration\n",
    "# \n",
    "# Installation: pip install wandb\n",
    "# Login: wandb login (or set WANDB_API_KEY environment variable)\n",
    "# \n",
    "# Key Features:\n",
    "# - Track metrics, hyperparameters, and system metrics\n",
    "# - Visualize training curves in real-time\n",
    "# - Log images, tables, and other artifacts\n",
    "# - Compare multiple runs\n",
    "# - Share results with team\n",
    "# ============================================================================\n",
    "\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"wandb version:\", wandb.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e75d72a",
   "metadata": {},
   "source": [
    "# 1. Basic Setup and Initialization\n",
    "\n",
    "## Steps to get started:\n",
    "1. **Install**: `pip install wandb`\n",
    "2. **Login**: Run `wandb login` in terminal (or set `WANDB_API_KEY` env var)\n",
    "3. **Initialize**: Call `wandb.init()` with your project name\n",
    "4. **Access UI**: Visit https://wandb.ai to view your runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f93de42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjunius-zhou\u001b[0m (\u001b[33mjunius-zhou-junius\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user/github/junius/de-LLM/ipynbs/tools/wandb/run-20260109_200902-rs1phc58</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/junius-zhou-junius/de-LLM/runs/rs1phc58' target=\"_blank\">basic-example</a></strong> to <a href='https://wandb.ai/junius-zhou-junius/de-LLM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/junius-zhou-junius/de-LLM' target=\"_blank\">https://wandb.ai/junius-zhou-junius/de-LLM</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/junius-zhou-junius/de-LLM/runs/rs1phc58' target=\"_blank\">https://wandb.ai/junius-zhou-junius/de-LLM/runs/rs1phc58</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID: rs1phc58\n",
      "Run URL: https://wandb.ai/junius-zhou-junius/de-LLM/runs/rs1phc58\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁</td></tr><tr><td>loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.85</td></tr><tr><td>loss</td><td>0.5</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">basic-example</strong> at: <a href='https://wandb.ai/junius-zhou-junius/de-LLM/runs/rs1phc58' target=\"_blank\">https://wandb.ai/junius-zhou-junius/de-LLM/runs/rs1phc58</a><br> View project at: <a href='https://wandb.ai/junius-zhou-junius/de-LLM' target=\"_blank\">https://wandb.ai/junius-zhou-junius/de-LLM</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20260109_200902-rs1phc58/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Example 1: Basic Initialization\n",
    "# ============================================================================\n",
    "\n",
    "# Initialize a run\n",
    "# This creates a new run in your project and returns a run object\n",
    "run = wandb.init(\n",
    "    project=\"de-LLM\",                    # Project name (creates/uses existing project)\n",
    "    name=\"basic-example\",                # Run name (optional, auto-generated if not provided)\n",
    "    notes=\"Learning wandb basics\",       # Notes about this run\n",
    "    tags=[\"tutorial\", \"basic\"],          # Tags for filtering runs\n",
    "    config={                              # Hyperparameters/config\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"batch_size\": 32,\n",
    "        \"epochs\": 10,\n",
    "        \"model\": \"simple-nn\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Run ID: {run.id}\")\n",
    "print(f\"Run URL: {run.url}\")  # Click this URL to view in browser!\n",
    "\n",
    "# Log a simple metric\n",
    "wandb.log({\"loss\": 0.5, \"accuracy\": 0.85})\n",
    "\n",
    "# Finish the run (saves all data)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ab7414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user/github/junius/de-LLM/ipynbs/tools/wandb/run-20260109_201029-nd0hwx2o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/junius-zhou-junius/de-LLM/runs/nd0hwx2o' target=\"_blank\">training-example</a></strong> to <a href='https://wandb.ai/junius-zhou-junius/de-LLM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/junius-zhou-junius/de-LLM' target=\"_blank\">https://wandb.ai/junius-zhou-junius/de-LLM</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/junius-zhou-junius/de-LLM/runs/nd0hwx2o' target=\"_blank\">https://wandb.ai/junius-zhou-junius/de-LLM/runs/nd0hwx2o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss=0.903, Val Acc=0.440\n",
      "Epoch 1: Train Loss=0.577, Val Acc=0.539\n",
      "Epoch 2: Train Loss=0.395, Val Acc=0.708\n",
      "Epoch 3: Train Loss=0.228, Val Acc=0.675\n",
      "Epoch 4: Train Loss=0.156, Val Acc=0.905\n",
      "Epoch 5: Train Loss=0.320, Val Acc=0.896\n",
      "Epoch 6: Train Loss=0.154, Val Acc=1.019\n",
      "Epoch 7: Train Loss=0.130, Val Acc=1.157\n",
      "Epoch 8: Train Loss=0.237, Val Acc=1.190\n",
      "Epoch 9: Train Loss=0.107, Val Acc=1.301\n",
      "Epoch 10: Train Loss=0.012, Val Acc=1.427\n",
      "Epoch 11: Train Loss=0.051, Val Acc=1.545\n",
      "Epoch 12: Train Loss=0.127, Val Acc=1.628\n",
      "Epoch 13: Train Loss=0.141, Val Acc=1.795\n",
      "Epoch 14: Train Loss=0.110, Val Acc=1.902\n",
      "Epoch 15: Train Loss=-0.045, Val Acc=1.972\n",
      "Epoch 16: Train Loss=0.126, Val Acc=2.069\n",
      "Epoch 17: Train Loss=0.020, Val Acc=2.157\n",
      "Epoch 18: Train Loss=0.108, Val Acc=2.171\n",
      "Epoch 19: Train Loss=0.149, Val Acc=2.306\n",
      "Epoch 20: Train Loss=0.032, Val Acc=2.472\n",
      "Epoch 21: Train Loss=0.128, Val Acc=2.594\n",
      "Epoch 22: Train Loss=-0.013, Val Acc=2.708\n",
      "Epoch 23: Train Loss=-0.085, Val Acc=2.666\n",
      "Epoch 24: Train Loss=-0.035, Val Acc=2.811\n",
      "Epoch 25: Train Loss=-0.106, Val Acc=2.935\n",
      "Epoch 26: Train Loss=0.102, Val Acc=3.138\n",
      "Epoch 27: Train Loss=-0.031, Val Acc=3.105\n",
      "Epoch 28: Train Loss=0.148, Val Acc=3.271\n",
      "Epoch 29: Train Loss=0.030, Val Acc=3.313\n",
      "Epoch 30: Train Loss=0.030, Val Acc=3.470\n",
      "Epoch 31: Train Loss=-0.097, Val Acc=3.555\n",
      "Epoch 32: Train Loss=0.121, Val Acc=3.675\n",
      "Epoch 33: Train Loss=0.139, Val Acc=3.766\n",
      "Epoch 34: Train Loss=0.099, Val Acc=3.748\n",
      "Epoch 35: Train Loss=0.041, Val Acc=3.881\n",
      "Epoch 36: Train Loss=-0.037, Val Acc=4.137\n",
      "Epoch 37: Train Loss=0.095, Val Acc=4.246\n",
      "Epoch 38: Train Loss=0.084, Val Acc=4.197\n",
      "Epoch 39: Train Loss=0.043, Val Acc=4.395\n",
      "Epoch 40: Train Loss=0.100, Val Acc=4.444\n",
      "Epoch 41: Train Loss=0.018, Val Acc=4.450\n",
      "Epoch 42: Train Loss=-0.113, Val Acc=4.680\n",
      "Epoch 43: Train Loss=0.032, Val Acc=4.865\n",
      "Epoch 44: Train Loss=-0.045, Val Acc=4.830\n",
      "Epoch 45: Train Loss=0.064, Val Acc=4.979\n",
      "Epoch 46: Train Loss=0.049, Val Acc=5.057\n",
      "Epoch 47: Train Loss=-0.031, Val Acc=5.028\n",
      "Epoch 48: Train Loss=-0.113, Val Acc=5.270\n",
      "Epoch 49: Train Loss=0.068, Val Acc=5.368\n",
      "Epoch 50: Train Loss=0.074, Val Acc=5.484\n",
      "Epoch 51: Train Loss=0.061, Val Acc=5.595\n",
      "Epoch 52: Train Loss=0.334, Val Acc=5.659\n",
      "Epoch 53: Train Loss=-0.032, Val Acc=5.810\n",
      "Epoch 54: Train Loss=0.059, Val Acc=5.832\n",
      "Epoch 55: Train Loss=0.117, Val Acc=5.988\n",
      "Epoch 56: Train Loss=0.035, Val Acc=5.995\n",
      "Epoch 57: Train Loss=-0.061, Val Acc=6.038\n",
      "Epoch 58: Train Loss=0.015, Val Acc=6.231\n",
      "Epoch 59: Train Loss=0.135, Val Acc=6.263\n",
      "Epoch 60: Train Loss=0.025, Val Acc=6.480\n",
      "Epoch 61: Train Loss=-0.103, Val Acc=6.562\n",
      "Epoch 62: Train Loss=0.004, Val Acc=6.674\n",
      "Epoch 63: Train Loss=0.134, Val Acc=6.778\n",
      "Epoch 64: Train Loss=0.220, Val Acc=6.775\n",
      "Epoch 65: Train Loss=-0.148, Val Acc=6.951\n",
      "Epoch 66: Train Loss=-0.043, Val Acc=7.049\n",
      "Epoch 67: Train Loss=0.045, Val Acc=7.062\n",
      "Epoch 68: Train Loss=-0.002, Val Acc=7.315\n",
      "Epoch 69: Train Loss=-0.040, Val Acc=7.419\n",
      "Epoch 70: Train Loss=0.074, Val Acc=7.416\n",
      "Epoch 71: Train Loss=-0.038, Val Acc=7.561\n",
      "Epoch 72: Train Loss=-0.094, Val Acc=7.726\n",
      "Epoch 73: Train Loss=-0.033, Val Acc=7.732\n",
      "Epoch 74: Train Loss=-0.079, Val Acc=7.928\n",
      "Epoch 75: Train Loss=0.158, Val Acc=7.937\n",
      "Epoch 76: Train Loss=0.092, Val Acc=8.148\n",
      "Epoch 77: Train Loss=0.020, Val Acc=8.129\n",
      "Epoch 78: Train Loss=-0.111, Val Acc=8.266\n",
      "Epoch 79: Train Loss=0.079, Val Acc=8.250\n",
      "Epoch 80: Train Loss=0.045, Val Acc=8.470\n",
      "Epoch 81: Train Loss=0.097, Val Acc=8.501\n",
      "Epoch 82: Train Loss=0.064, Val Acc=8.724\n",
      "Epoch 83: Train Loss=0.149, Val Acc=8.671\n",
      "Epoch 84: Train Loss=-0.003, Val Acc=8.929\n",
      "Epoch 85: Train Loss=0.038, Val Acc=8.914\n",
      "Epoch 86: Train Loss=-0.026, Val Acc=9.030\n",
      "Epoch 87: Train Loss=0.090, Val Acc=9.082\n",
      "Epoch 88: Train Loss=-0.015, Val Acc=9.289\n",
      "Epoch 89: Train Loss=0.152, Val Acc=9.325\n",
      "Epoch 90: Train Loss=0.026, Val Acc=9.477\n",
      "Epoch 91: Train Loss=-0.022, Val Acc=9.595\n",
      "Epoch 92: Train Loss=0.031, Val Acc=9.594\n",
      "Epoch 93: Train Loss=-0.093, Val Acc=9.731\n",
      "Epoch 94: Train Loss=0.073, Val Acc=9.808\n",
      "Epoch 95: Train Loss=0.144, Val Acc=9.888\n",
      "Epoch 96: Train Loss=-0.242, Val Acc=10.043\n",
      "Epoch 97: Train Loss=-0.103, Val Acc=10.166\n",
      "Epoch 98: Train Loss=0.135, Val Acc=10.220\n",
      "Epoch 99: Train Loss=-0.039, Val Acc=10.294\n",
      "Epoch 100: Train Loss=0.134, Val Acc=10.497\n",
      "Epoch 101: Train Loss=-0.078, Val Acc=10.598\n",
      "Epoch 102: Train Loss=0.002, Val Acc=10.732\n",
      "Epoch 103: Train Loss=0.029, Val Acc=10.719\n",
      "Epoch 104: Train Loss=-0.064, Val Acc=10.847\n",
      "Epoch 105: Train Loss=-0.013, Val Acc=11.012\n",
      "Epoch 106: Train Loss=0.051, Val Acc=11.134\n",
      "Epoch 107: Train Loss=0.039, Val Acc=11.149\n",
      "Epoch 108: Train Loss=-0.011, Val Acc=11.295\n",
      "Epoch 109: Train Loss=0.135, Val Acc=11.278\n",
      "Epoch 110: Train Loss=-0.005, Val Acc=11.336\n",
      "Epoch 111: Train Loss=0.059, Val Acc=11.548\n",
      "Epoch 112: Train Loss=-0.103, Val Acc=11.716\n",
      "Epoch 113: Train Loss=-0.011, Val Acc=11.718\n",
      "Epoch 114: Train Loss=-0.140, Val Acc=11.770\n",
      "Epoch 115: Train Loss=-0.066, Val Acc=11.919\n",
      "Epoch 116: Train Loss=-0.032, Val Acc=12.069\n",
      "Epoch 117: Train Loss=0.051, Val Acc=12.200\n",
      "Epoch 118: Train Loss=0.019, Val Acc=12.247\n",
      "Epoch 119: Train Loss=0.040, Val Acc=12.410\n",
      "Epoch 120: Train Loss=0.079, Val Acc=12.472\n",
      "Epoch 121: Train Loss=0.011, Val Acc=12.437\n",
      "Epoch 122: Train Loss=0.060, Val Acc=12.657\n",
      "Epoch 123: Train Loss=0.260, Val Acc=12.764\n",
      "Epoch 124: Train Loss=-0.033, Val Acc=12.865\n",
      "Epoch 125: Train Loss=0.176, Val Acc=12.958\n",
      "Epoch 126: Train Loss=0.177, Val Acc=13.103\n",
      "Epoch 127: Train Loss=-0.059, Val Acc=13.187\n",
      "Epoch 128: Train Loss=0.016, Val Acc=13.285\n",
      "Epoch 129: Train Loss=0.146, Val Acc=13.476\n",
      "Epoch 130: Train Loss=-0.123, Val Acc=13.453\n",
      "Epoch 131: Train Loss=0.230, Val Acc=13.528\n",
      "Epoch 132: Train Loss=0.062, Val Acc=13.570\n",
      "Epoch 133: Train Loss=0.012, Val Acc=13.733\n",
      "Epoch 134: Train Loss=0.023, Val Acc=13.835\n",
      "Epoch 135: Train Loss=0.150, Val Acc=13.878\n",
      "Epoch 136: Train Loss=0.017, Val Acc=14.020\n",
      "Epoch 137: Train Loss=0.153, Val Acc=14.127\n",
      "Epoch 138: Train Loss=-0.133, Val Acc=14.328\n",
      "Epoch 139: Train Loss=-0.021, Val Acc=14.372\n",
      "Epoch 140: Train Loss=0.009, Val Acc=14.475\n",
      "Epoch 141: Train Loss=0.083, Val Acc=14.493\n",
      "Epoch 142: Train Loss=-0.171, Val Acc=14.639\n",
      "Epoch 143: Train Loss=-0.003, Val Acc=14.711\n",
      "Epoch 144: Train Loss=-0.053, Val Acc=14.807\n",
      "Epoch 145: Train Loss=-0.035, Val Acc=14.988\n",
      "Epoch 146: Train Loss=-0.075, Val Acc=15.078\n",
      "Epoch 147: Train Loss=-0.036, Val Acc=15.183\n",
      "Epoch 148: Train Loss=-0.018, Val Acc=15.289\n",
      "Epoch 149: Train Loss=-0.138, Val Acc=15.372\n",
      "Epoch 150: Train Loss=0.174, Val Acc=15.401\n",
      "Epoch 151: Train Loss=-0.101, Val Acc=15.482\n",
      "Epoch 152: Train Loss=0.147, Val Acc=15.646\n",
      "Epoch 153: Train Loss=0.027, Val Acc=15.769\n",
      "Epoch 154: Train Loss=0.037, Val Acc=15.786\n",
      "Epoch 155: Train Loss=-0.033, Val Acc=15.916\n",
      "Epoch 156: Train Loss=-0.169, Val Acc=16.113\n",
      "Epoch 157: Train Loss=-0.028, Val Acc=16.238\n",
      "Epoch 158: Train Loss=-0.274, Val Acc=16.277\n",
      "Epoch 159: Train Loss=-0.037, Val Acc=16.336\n",
      "Epoch 160: Train Loss=0.047, Val Acc=16.388\n",
      "Epoch 161: Train Loss=-0.039, Val Acc=16.410\n",
      "Epoch 162: Train Loss=-0.099, Val Acc=16.695\n",
      "Epoch 163: Train Loss=-0.089, Val Acc=16.742\n",
      "Epoch 164: Train Loss=-0.027, Val Acc=16.839\n",
      "Epoch 165: Train Loss=0.007, Val Acc=16.920\n",
      "Epoch 166: Train Loss=0.104, Val Acc=17.144\n",
      "Epoch 167: Train Loss=0.075, Val Acc=17.144\n",
      "Epoch 168: Train Loss=-0.062, Val Acc=17.313\n",
      "Epoch 169: Train Loss=0.006, Val Acc=17.306\n",
      "Epoch 170: Train Loss=0.115, Val Acc=17.460\n",
      "Epoch 171: Train Loss=0.073, Val Acc=17.558\n",
      "Epoch 172: Train Loss=0.160, Val Acc=17.702\n",
      "Epoch 173: Train Loss=0.109, Val Acc=17.764\n",
      "Epoch 174: Train Loss=0.045, Val Acc=17.788\n",
      "Epoch 175: Train Loss=0.082, Val Acc=17.948\n",
      "Epoch 176: Train Loss=0.144, Val Acc=18.087\n",
      "Epoch 177: Train Loss=-0.032, Val Acc=18.089\n",
      "Epoch 178: Train Loss=-0.065, Val Acc=18.291\n",
      "Epoch 179: Train Loss=0.044, Val Acc=18.336\n",
      "Epoch 180: Train Loss=-0.053, Val Acc=18.412\n",
      "Epoch 181: Train Loss=-0.036, Val Acc=18.598\n",
      "Epoch 182: Train Loss=-0.008, Val Acc=18.740\n",
      "Epoch 183: Train Loss=-0.056, Val Acc=18.821\n",
      "Epoch 184: Train Loss=-0.101, Val Acc=18.819\n",
      "Epoch 185: Train Loss=0.027, Val Acc=18.944\n",
      "Epoch 186: Train Loss=0.069, Val Acc=19.122\n",
      "Epoch 187: Train Loss=-0.067, Val Acc=19.148\n",
      "Epoch 188: Train Loss=0.038, Val Acc=19.222\n",
      "Epoch 189: Train Loss=0.049, Val Acc=19.457\n",
      "Epoch 190: Train Loss=0.037, Val Acc=19.386\n",
      "Epoch 191: Train Loss=0.086, Val Acc=19.618\n",
      "Epoch 192: Train Loss=0.106, Val Acc=19.656\n",
      "Epoch 193: Train Loss=-0.135, Val Acc=19.738\n",
      "Epoch 194: Train Loss=0.029, Val Acc=19.855\n",
      "Epoch 195: Train Loss=-0.032, Val Acc=19.897\n",
      "Epoch 196: Train Loss=-0.145, Val Acc=20.130\n",
      "Epoch 197: Train Loss=0.009, Val Acc=20.233\n",
      "Epoch 198: Train Loss=-0.007, Val Acc=20.204\n",
      "Epoch 199: Train Loss=-0.232, Val Acc=20.299\n",
      "Epoch 200: Train Loss=0.090, Val Acc=20.472\n",
      "Epoch 201: Train Loss=-0.054, Val Acc=20.520\n",
      "Epoch 202: Train Loss=-0.104, Val Acc=20.728\n",
      "Epoch 203: Train Loss=-0.029, Val Acc=20.750\n",
      "Epoch 204: Train Loss=-0.123, Val Acc=20.866\n",
      "Epoch 205: Train Loss=-0.076, Val Acc=20.961\n",
      "Epoch 206: Train Loss=-0.046, Val Acc=21.033\n",
      "Epoch 207: Train Loss=0.020, Val Acc=21.140\n",
      "Epoch 208: Train Loss=-0.012, Val Acc=21.232\n",
      "Epoch 209: Train Loss=-0.012, Val Acc=21.416\n",
      "Epoch 210: Train Loss=-0.115, Val Acc=21.504\n",
      "Epoch 211: Train Loss=0.030, Val Acc=21.695\n",
      "Epoch 212: Train Loss=0.009, Val Acc=21.657\n",
      "Epoch 213: Train Loss=0.071, Val Acc=21.739\n",
      "Epoch 214: Train Loss=-0.163, Val Acc=21.769\n",
      "Epoch 215: Train Loss=0.038, Val Acc=21.958\n",
      "Epoch 216: Train Loss=-0.016, Val Acc=22.006\n",
      "Epoch 217: Train Loss=0.046, Val Acc=22.119\n",
      "Epoch 218: Train Loss=-0.146, Val Acc=22.194\n",
      "Epoch 219: Train Loss=0.096, Val Acc=22.379\n",
      "Epoch 220: Train Loss=0.184, Val Acc=22.440\n",
      "Epoch 221: Train Loss=-0.019, Val Acc=22.533\n",
      "Epoch 222: Train Loss=-0.074, Val Acc=22.645\n",
      "Epoch 223: Train Loss=-0.038, Val Acc=22.790\n",
      "Epoch 224: Train Loss=-0.017, Val Acc=22.818\n",
      "Epoch 225: Train Loss=-0.050, Val Acc=22.969\n",
      "Epoch 226: Train Loss=-0.204, Val Acc=23.020\n",
      "Epoch 227: Train Loss=-0.111, Val Acc=23.080\n",
      "Epoch 228: Train Loss=0.154, Val Acc=23.195\n",
      "Epoch 229: Train Loss=-0.054, Val Acc=23.344\n",
      "Epoch 230: Train Loss=-0.086, Val Acc=23.395\n",
      "Epoch 231: Train Loss=0.111, Val Acc=23.617\n",
      "Epoch 232: Train Loss=-0.005, Val Acc=23.638\n",
      "Epoch 233: Train Loss=0.005, Val Acc=23.691\n",
      "Epoch 234: Train Loss=0.013, Val Acc=23.735\n",
      "Epoch 235: Train Loss=-0.023, Val Acc=23.828\n",
      "Epoch 236: Train Loss=0.005, Val Acc=23.966\n",
      "Epoch 237: Train Loss=0.113, Val Acc=24.194\n",
      "Epoch 238: Train Loss=0.019, Val Acc=24.250\n",
      "Epoch 239: Train Loss=-0.151, Val Acc=24.315\n",
      "Epoch 240: Train Loss=-0.145, Val Acc=24.479\n",
      "Epoch 241: Train Loss=0.026, Val Acc=24.509\n",
      "Epoch 242: Train Loss=0.078, Val Acc=24.655\n",
      "Epoch 243: Train Loss=0.077, Val Acc=24.772\n",
      "Epoch 244: Train Loss=0.032, Val Acc=24.839\n",
      "Epoch 245: Train Loss=-0.007, Val Acc=24.914\n",
      "Epoch 246: Train Loss=-0.054, Val Acc=25.019\n",
      "Epoch 247: Train Loss=-0.107, Val Acc=25.109\n",
      "Epoch 248: Train Loss=0.045, Val Acc=25.283\n",
      "Epoch 249: Train Loss=0.046, Val Acc=25.279\n",
      "Epoch 250: Train Loss=0.130, Val Acc=25.478\n",
      "Epoch 251: Train Loss=0.018, Val Acc=25.527\n",
      "Epoch 252: Train Loss=0.079, Val Acc=25.658\n",
      "Epoch 253: Train Loss=0.008, Val Acc=25.767\n",
      "Epoch 254: Train Loss=-0.050, Val Acc=25.750\n",
      "Epoch 255: Train Loss=0.085, Val Acc=25.962\n",
      "Epoch 256: Train Loss=-0.030, Val Acc=26.087\n",
      "Epoch 257: Train Loss=0.110, Val Acc=26.179\n",
      "Epoch 258: Train Loss=-0.060, Val Acc=26.229\n",
      "Epoch 259: Train Loss=-0.176, Val Acc=26.364\n",
      "Epoch 260: Train Loss=-0.098, Val Acc=26.422\n",
      "Epoch 261: Train Loss=0.200, Val Acc=26.557\n",
      "Epoch 262: Train Loss=-0.094, Val Acc=26.647\n",
      "Epoch 263: Train Loss=0.133, Val Acc=26.763\n",
      "Epoch 264: Train Loss=0.070, Val Acc=26.893\n",
      "Epoch 265: Train Loss=-0.081, Val Acc=26.982\n",
      "Epoch 266: Train Loss=0.053, Val Acc=27.068\n",
      "Epoch 267: Train Loss=-0.017, Val Acc=27.071\n",
      "Epoch 268: Train Loss=-0.025, Val Acc=27.188\n",
      "Epoch 269: Train Loss=-0.015, Val Acc=27.385\n",
      "Epoch 270: Train Loss=0.191, Val Acc=27.396\n",
      "Epoch 271: Train Loss=-0.104, Val Acc=27.547\n",
      "Epoch 272: Train Loss=0.077, Val Acc=27.701\n",
      "Epoch 273: Train Loss=0.134, Val Acc=27.795\n",
      "Epoch 274: Train Loss=-0.021, Val Acc=27.828\n",
      "Epoch 275: Train Loss=-0.103, Val Acc=28.000\n",
      "Epoch 276: Train Loss=0.030, Val Acc=28.071\n",
      "Epoch 277: Train Loss=0.126, Val Acc=28.129\n",
      "Epoch 278: Train Loss=-0.052, Val Acc=28.182\n",
      "Epoch 279: Train Loss=0.125, Val Acc=28.291\n",
      "Epoch 280: Train Loss=-0.241, Val Acc=28.504\n",
      "Epoch 281: Train Loss=-0.013, Val Acc=28.495\n",
      "Epoch 282: Train Loss=0.033, Val Acc=28.786\n",
      "Epoch 283: Train Loss=-0.054, Val Acc=28.727\n",
      "Epoch 284: Train Loss=-0.174, Val Acc=28.841\n",
      "Epoch 285: Train Loss=-0.088, Val Acc=28.904\n",
      "Epoch 286: Train Loss=0.138, Val Acc=29.050\n",
      "Epoch 287: Train Loss=0.041, Val Acc=29.139\n",
      "Epoch 288: Train Loss=0.061, Val Acc=29.198\n",
      "Epoch 289: Train Loss=-0.092, Val Acc=29.397\n",
      "Epoch 290: Train Loss=0.067, Val Acc=29.437\n",
      "Epoch 291: Train Loss=0.183, Val Acc=29.513\n",
      "Epoch 292: Train Loss=0.047, Val Acc=29.584\n",
      "Epoch 293: Train Loss=0.158, Val Acc=29.779\n",
      "Epoch 294: Train Loss=-0.168, Val Acc=29.833\n",
      "Epoch 295: Train Loss=0.142, Val Acc=29.911\n",
      "Epoch 296: Train Loss=0.080, Val Acc=30.030\n",
      "Epoch 297: Train Loss=-0.156, Val Acc=30.191\n",
      "Epoch 298: Train Loss=0.197, Val Acc=30.214\n",
      "Epoch 299: Train Loss=-0.085, Val Acc=30.398\n",
      "Epoch 300: Train Loss=-0.071, Val Acc=30.510\n",
      "Epoch 301: Train Loss=0.086, Val Acc=30.444\n",
      "Epoch 302: Train Loss=0.071, Val Acc=30.743\n",
      "Epoch 303: Train Loss=0.024, Val Acc=30.654\n",
      "Epoch 304: Train Loss=0.121, Val Acc=30.827\n",
      "Epoch 305: Train Loss=-0.145, Val Acc=30.930\n",
      "Epoch 306: Train Loss=0.109, Val Acc=31.179\n",
      "Epoch 307: Train Loss=-0.093, Val Acc=31.122\n",
      "Epoch 308: Train Loss=0.051, Val Acc=31.269\n",
      "Epoch 309: Train Loss=-0.093, Val Acc=31.332\n",
      "Epoch 310: Train Loss=0.187, Val Acc=31.468\n",
      "Epoch 311: Train Loss=0.013, Val Acc=31.513\n",
      "Epoch 312: Train Loss=0.295, Val Acc=31.585\n",
      "Epoch 313: Train Loss=-0.015, Val Acc=31.802\n",
      "Epoch 314: Train Loss=-0.142, Val Acc=31.879\n",
      "Epoch 315: Train Loss=-0.047, Val Acc=31.937\n",
      "Epoch 316: Train Loss=0.051, Val Acc=31.986\n",
      "Epoch 317: Train Loss=-0.067, Val Acc=32.127\n",
      "Epoch 318: Train Loss=-0.083, Val Acc=32.283\n",
      "Epoch 319: Train Loss=0.042, Val Acc=32.360\n",
      "Epoch 320: Train Loss=-0.180, Val Acc=32.459\n",
      "Epoch 321: Train Loss=0.214, Val Acc=32.553\n",
      "Epoch 322: Train Loss=-0.176, Val Acc=32.664\n",
      "Epoch 323: Train Loss=-0.190, Val Acc=32.779\n",
      "Epoch 324: Train Loss=0.016, Val Acc=32.857\n",
      "Epoch 325: Train Loss=-0.027, Val Acc=32.990\n",
      "Epoch 326: Train Loss=-0.116, Val Acc=33.032\n",
      "Epoch 327: Train Loss=0.110, Val Acc=33.117\n",
      "Epoch 328: Train Loss=0.049, Val Acc=33.170\n",
      "Epoch 329: Train Loss=0.039, Val Acc=33.266\n",
      "Epoch 330: Train Loss=-0.010, Val Acc=33.415\n",
      "Epoch 331: Train Loss=0.188, Val Acc=33.556\n",
      "Epoch 332: Train Loss=0.099, Val Acc=33.617\n",
      "Epoch 333: Train Loss=0.109, Val Acc=33.732\n",
      "Epoch 334: Train Loss=-0.020, Val Acc=33.926\n",
      "Epoch 335: Train Loss=-0.017, Val Acc=33.916\n",
      "Epoch 336: Train Loss=0.074, Val Acc=34.090\n",
      "Epoch 337: Train Loss=-0.077, Val Acc=34.094\n",
      "Epoch 338: Train Loss=0.191, Val Acc=34.316\n",
      "Epoch 339: Train Loss=0.233, Val Acc=34.281\n",
      "Epoch 340: Train Loss=-0.216, Val Acc=34.440\n",
      "Epoch 341: Train Loss=-0.067, Val Acc=34.547\n",
      "Epoch 342: Train Loss=-0.010, Val Acc=34.707\n",
      "Epoch 343: Train Loss=0.012, Val Acc=34.762\n",
      "Epoch 344: Train Loss=-0.024, Val Acc=34.947\n",
      "Epoch 345: Train Loss=-0.179, Val Acc=34.976\n",
      "Epoch 346: Train Loss=0.120, Val Acc=35.147\n",
      "Epoch 347: Train Loss=0.097, Val Acc=35.145\n",
      "Epoch 348: Train Loss=-0.010, Val Acc=35.332\n",
      "Epoch 349: Train Loss=-0.054, Val Acc=35.287\n",
      "Epoch 350: Train Loss=0.064, Val Acc=35.414\n",
      "Epoch 351: Train Loss=0.128, Val Acc=35.561\n",
      "Epoch 352: Train Loss=-0.177, Val Acc=35.691\n",
      "Epoch 353: Train Loss=-0.003, Val Acc=35.835\n",
      "Epoch 354: Train Loss=-0.070, Val Acc=35.812\n",
      "Epoch 355: Train Loss=0.116, Val Acc=36.060\n",
      "Epoch 356: Train Loss=0.137, Val Acc=35.979\n",
      "Epoch 357: Train Loss=0.014, Val Acc=36.136\n",
      "Epoch 358: Train Loss=0.092, Val Acc=36.218\n",
      "Epoch 359: Train Loss=0.016, Val Acc=36.384\n",
      "Epoch 360: Train Loss=0.000, Val Acc=36.408\n",
      "Epoch 361: Train Loss=0.041, Val Acc=36.520\n",
      "Epoch 362: Train Loss=0.067, Val Acc=36.597\n",
      "Epoch 363: Train Loss=-0.057, Val Acc=36.829\n",
      "Epoch 364: Train Loss=0.021, Val Acc=36.902\n",
      "Epoch 365: Train Loss=-0.104, Val Acc=36.912\n",
      "Epoch 366: Train Loss=-0.016, Val Acc=37.012\n",
      "Epoch 367: Train Loss=-0.009, Val Acc=37.125\n",
      "Epoch 368: Train Loss=0.048, Val Acc=37.233\n",
      "Epoch 369: Train Loss=-0.172, Val Acc=37.380\n",
      "Epoch 370: Train Loss=0.136, Val Acc=37.453\n",
      "Epoch 371: Train Loss=-0.106, Val Acc=37.510\n",
      "Epoch 372: Train Loss=-0.151, Val Acc=37.685\n",
      "Epoch 373: Train Loss=0.030, Val Acc=37.751\n",
      "Epoch 374: Train Loss=-0.042, Val Acc=37.821\n",
      "Epoch 375: Train Loss=0.135, Val Acc=37.950\n",
      "Epoch 376: Train Loss=0.088, Val Acc=38.041\n",
      "Epoch 377: Train Loss=0.010, Val Acc=38.100\n",
      "Epoch 378: Train Loss=-0.016, Val Acc=38.192\n",
      "Epoch 379: Train Loss=0.002, Val Acc=38.343\n",
      "Epoch 380: Train Loss=-0.008, Val Acc=38.494\n",
      "Epoch 381: Train Loss=-0.098, Val Acc=38.540\n",
      "Epoch 382: Train Loss=-0.164, Val Acc=38.727\n",
      "Epoch 383: Train Loss=-0.242, Val Acc=38.832\n",
      "Epoch 384: Train Loss=0.070, Val Acc=38.782\n",
      "Epoch 385: Train Loss=0.001, Val Acc=38.993\n",
      "Epoch 386: Train Loss=-0.071, Val Acc=39.104\n",
      "Epoch 387: Train Loss=-0.140, Val Acc=39.089\n",
      "Epoch 388: Train Loss=0.045, Val Acc=39.334\n",
      "Epoch 389: Train Loss=-0.037, Val Acc=39.299\n",
      "Epoch 390: Train Loss=-0.093, Val Acc=39.450\n",
      "Epoch 391: Train Loss=-0.008, Val Acc=39.513\n",
      "Epoch 392: Train Loss=-0.104, Val Acc=39.655\n",
      "Epoch 393: Train Loss=-0.013, Val Acc=39.719\n",
      "Epoch 394: Train Loss=-0.075, Val Acc=39.769\n",
      "Epoch 395: Train Loss=-0.094, Val Acc=40.010\n",
      "Epoch 396: Train Loss=0.020, Val Acc=40.001\n",
      "Epoch 397: Train Loss=0.004, Val Acc=40.120\n",
      "Epoch 398: Train Loss=0.021, Val Acc=40.250\n",
      "Epoch 399: Train Loss=0.016, Val Acc=40.364\n",
      "Epoch 400: Train Loss=0.062, Val Acc=40.462\n",
      "Epoch 401: Train Loss=0.104, Val Acc=40.518\n",
      "Epoch 402: Train Loss=0.062, Val Acc=40.696\n",
      "Epoch 403: Train Loss=-0.091, Val Acc=40.768\n",
      "Epoch 404: Train Loss=-0.135, Val Acc=40.828\n",
      "Epoch 405: Train Loss=0.135, Val Acc=40.927\n",
      "Epoch 406: Train Loss=-0.131, Val Acc=41.046\n",
      "Epoch 407: Train Loss=-0.159, Val Acc=41.119\n",
      "Epoch 408: Train Loss=0.028, Val Acc=41.305\n",
      "Epoch 409: Train Loss=-0.021, Val Acc=41.415\n",
      "Epoch 410: Train Loss=0.122, Val Acc=41.452\n",
      "Epoch 411: Train Loss=0.215, Val Acc=41.526\n",
      "Epoch 412: Train Loss=-0.077, Val Acc=41.620\n",
      "Epoch 413: Train Loss=0.002, Val Acc=41.738\n",
      "Epoch 414: Train Loss=-0.055, Val Acc=41.860\n",
      "Epoch 415: Train Loss=0.090, Val Acc=41.966\n",
      "Epoch 416: Train Loss=0.086, Val Acc=42.025\n",
      "Epoch 417: Train Loss=0.123, Val Acc=42.202\n",
      "Epoch 418: Train Loss=0.058, Val Acc=42.245\n",
      "Epoch 419: Train Loss=0.086, Val Acc=42.371\n",
      "Epoch 420: Train Loss=0.169, Val Acc=42.470\n",
      "Epoch 421: Train Loss=-0.280, Val Acc=42.582\n",
      "Epoch 422: Train Loss=0.028, Val Acc=42.618\n",
      "Epoch 423: Train Loss=0.150, Val Acc=42.766\n",
      "Epoch 424: Train Loss=0.094, Val Acc=42.862\n",
      "Epoch 425: Train Loss=-0.142, Val Acc=43.032\n",
      "Epoch 426: Train Loss=0.024, Val Acc=43.026\n",
      "Epoch 427: Train Loss=0.001, Val Acc=43.238\n",
      "Epoch 428: Train Loss=0.149, Val Acc=43.147\n",
      "Epoch 429: Train Loss=0.011, Val Acc=43.369\n",
      "Epoch 430: Train Loss=0.264, Val Acc=43.465\n",
      "Epoch 431: Train Loss=-0.156, Val Acc=43.533\n",
      "Epoch 432: Train Loss=0.267, Val Acc=43.542\n",
      "Epoch 433: Train Loss=0.040, Val Acc=43.819\n",
      "Epoch 434: Train Loss=0.050, Val Acc=43.856\n",
      "Epoch 435: Train Loss=0.163, Val Acc=43.992\n",
      "Epoch 436: Train Loss=0.284, Val Acc=43.994\n",
      "Epoch 437: Train Loss=-0.194, Val Acc=44.084\n",
      "Epoch 438: Train Loss=-0.043, Val Acc=44.200\n",
      "Epoch 439: Train Loss=-0.100, Val Acc=44.362\n",
      "Epoch 440: Train Loss=0.099, Val Acc=44.418\n",
      "Epoch 441: Train Loss=-0.040, Val Acc=44.548\n",
      "Epoch 442: Train Loss=0.105, Val Acc=44.544\n",
      "Epoch 443: Train Loss=0.052, Val Acc=44.752\n",
      "Epoch 444: Train Loss=-0.139, Val Acc=44.741\n",
      "Epoch 445: Train Loss=0.153, Val Acc=44.965\n",
      "Epoch 446: Train Loss=0.068, Val Acc=45.114\n",
      "Epoch 447: Train Loss=0.094, Val Acc=45.197\n",
      "Epoch 448: Train Loss=-0.070, Val Acc=45.226\n",
      "Epoch 449: Train Loss=0.109, Val Acc=45.452\n",
      "Epoch 450: Train Loss=-0.097, Val Acc=45.469\n",
      "Epoch 451: Train Loss=0.112, Val Acc=45.551\n",
      "Epoch 452: Train Loss=-0.177, Val Acc=45.736\n",
      "Epoch 453: Train Loss=0.001, Val Acc=45.672\n",
      "Epoch 454: Train Loss=0.169, Val Acc=45.871\n",
      "Epoch 455: Train Loss=-0.212, Val Acc=45.855\n",
      "Epoch 456: Train Loss=0.081, Val Acc=46.086\n",
      "Epoch 457: Train Loss=-0.109, Val Acc=46.187\n",
      "Epoch 458: Train Loss=0.130, Val Acc=46.305\n",
      "Epoch 459: Train Loss=0.110, Val Acc=46.312\n",
      "Epoch 460: Train Loss=0.079, Val Acc=46.437\n",
      "Epoch 461: Train Loss=-0.033, Val Acc=46.620\n",
      "Epoch 462: Train Loss=-0.078, Val Acc=46.672\n",
      "Epoch 463: Train Loss=-0.079, Val Acc=46.798\n",
      "Epoch 464: Train Loss=-0.037, Val Acc=46.936\n",
      "Epoch 465: Train Loss=0.082, Val Acc=46.975\n",
      "Epoch 466: Train Loss=-0.062, Val Acc=47.181\n",
      "Epoch 467: Train Loss=-0.030, Val Acc=47.074\n",
      "Epoch 468: Train Loss=0.182, Val Acc=47.205\n",
      "Epoch 469: Train Loss=0.106, Val Acc=47.287\n",
      "Epoch 470: Train Loss=0.122, Val Acc=47.461\n",
      "Epoch 471: Train Loss=0.098, Val Acc=47.593\n",
      "Epoch 472: Train Loss=0.003, Val Acc=47.687\n",
      "Epoch 473: Train Loss=-0.045, Val Acc=47.749\n",
      "Epoch 474: Train Loss=-0.060, Val Acc=47.882\n",
      "Epoch 475: Train Loss=-0.310, Val Acc=47.934\n",
      "Epoch 476: Train Loss=-0.044, Val Acc=48.028\n",
      "Epoch 477: Train Loss=-0.046, Val Acc=48.151\n",
      "Epoch 478: Train Loss=-0.009, Val Acc=48.207\n",
      "Epoch 479: Train Loss=0.122, Val Acc=48.375\n",
      "Epoch 480: Train Loss=-0.010, Val Acc=48.354\n",
      "Epoch 481: Train Loss=0.100, Val Acc=48.619\n",
      "Epoch 482: Train Loss=0.134, Val Acc=48.646\n",
      "Epoch 483: Train Loss=0.004, Val Acc=48.737\n",
      "Epoch 484: Train Loss=0.066, Val Acc=48.933\n",
      "Epoch 485: Train Loss=0.085, Val Acc=48.907\n",
      "Epoch 486: Train Loss=0.004, Val Acc=49.066\n",
      "Epoch 487: Train Loss=0.017, Val Acc=49.083\n",
      "Epoch 488: Train Loss=0.123, Val Acc=49.218\n",
      "Epoch 489: Train Loss=0.046, Val Acc=49.313\n",
      "Epoch 490: Train Loss=-0.056, Val Acc=49.434\n",
      "Epoch 491: Train Loss=-0.032, Val Acc=49.498\n",
      "Epoch 492: Train Loss=-0.048, Val Acc=49.694\n",
      "Epoch 493: Train Loss=0.004, Val Acc=49.727\n",
      "Epoch 494: Train Loss=0.074, Val Acc=49.928\n",
      "Epoch 495: Train Loss=-0.088, Val Acc=49.978\n",
      "Epoch 496: Train Loss=-0.042, Val Acc=50.112\n",
      "Epoch 497: Train Loss=-0.034, Val Acc=50.173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 498: Train Loss=-0.153, Val Acc=50.312\n",
      "Epoch 499: Train Loss=-0.033, Val Acc=50.414\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇██</td></tr><tr><td>learning_rate</td><td>█▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/accuracy</td><td>▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇█</td></tr><tr><td>train/loss</td><td>█▅▃▄▅▇▄▃▁▃▆▃▃▄▃▃▄▂▆▄▄▄▃▆▁▄▃▃▄▃▆▃▂▄▄▇▃▂▅▂</td></tr><tr><td>val/accuracy</td><td>▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇██</td></tr><tr><td>val/loss</td><td>█▄▆▆▆▃▂▇▂▂▄▃▂▇▃▅▁▄▃▄▇▆▅▅▂▄▅▄▅▅▂▆▇▆▇▃▆▆▄▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>499</td></tr><tr><td>learning_rate</td><td>0.0</td></tr><tr><td>train/accuracy</td><td>50.46394</td></tr><tr><td>train/loss</td><td>-0.03322</td></tr><tr><td>val/accuracy</td><td>50.41394</td></tr><tr><td>val/loss</td><td>0.06678</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">training-example</strong> at: <a href='https://wandb.ai/junius-zhou-junius/de-LLM/runs/nd0hwx2o' target=\"_blank\">https://wandb.ai/junius-zhou-junius/de-LLM/runs/nd0hwx2o</a><br> View project at: <a href='https://wandb.ai/junius-zhou-junius/de-LLM' target=\"_blank\">https://wandb.ai/junius-zhou-junius/de-LLM</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20260109_201029-nd0hwx2o/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Example 2: Training Loop with Metrics Logging\n",
    "# ============================================================================\n",
    "\n",
    "# Simulate a training loop\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "wandb.init(\n",
    "    project=\"de-LLM\",\n",
    "    name=\"training-example\",\n",
    "    config={\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"batch_size\": 64,\n",
    "        \"epochs\": 5,\n",
    "        \"optimizer\": \"Adam\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Simulate training\n",
    "for epoch in range(500):\n",
    "    # Simulate training metrics\n",
    "    train_loss = 1.0 / (epoch + 1) + np.random.normal(0, 0.1)\n",
    "    train_acc = 0.5 + epoch * 0.1 + np.random.normal(0, 0.05)\n",
    "    \n",
    "    # Simulate validation metrics\n",
    "    val_loss = train_loss + 0.1\n",
    "    val_acc = train_acc - 0.05\n",
    "\n",
    "    sleep(0.1)\n",
    "    \n",
    "    # Log metrics (creates time series plots)\n",
    "    # group data according to the keys with \"/\"\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch,\n",
    "        \"train/loss\": train_loss,\n",
    "        \"train/accuracy\": train_acc,\n",
    "        \"val/loss\": val_loss,\n",
    "        \"val/accuracy\": val_acc,\n",
    "        \"learning_rate\": 0.001 * (0.9 ** epoch)  # Learning rate schedule\n",
    "    })\n",
    "    \n",
    "    print(f\"Epoch {epoch}: Train Loss={train_loss:.3f}, Val Acc={val_acc:.3f}\")\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145e3a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Example 3: Logging Images and Plots\n",
    "# ============================================================================\n",
    "\n",
    "wandb.init(\n",
    "    project=\"de-LLM\",\n",
    "    name=\"image-logging-example\",\n",
    "    config={\"plot_type\": \"matplotlib\"}\n",
    ")\n",
    "\n",
    "# Create a simple plot\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "x = np.linspace(0, 10, 100)\n",
    "y = np.sin(x)\n",
    "ax.plot(x, y, label=\"sin(x)\")\n",
    "ax.set_xlabel(\"X\")\n",
    "ax.set_ylabel(\"Y\")\n",
    "ax.set_title(\"Sample Plot\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "# Log the plot\n",
    "wandb.log({\"plot\": wandb.Image(fig)})\n",
    "\n",
    "# Log multiple plots\n",
    "for i in range(3):\n",
    "    fig, ax = plt.subplots()\n",
    "    x = np.linspace(0, 10, 100)\n",
    "    y = np.sin(x + i)\n",
    "    ax.plot(x, y, label=f\"sin(x + {i})\")\n",
    "    ax.legend()\n",
    "    wandb.log({f\"plot_{i}\": wandb.Image(fig)})\n",
    "    plt.close(fig)\n",
    "\n",
    "# Log numpy array as image\n",
    "random_image = np.random.rand(64, 64, 3)\n",
    "wandb.log({\"random_image\": wandb.Image(random_image)})\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd92553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Example 4: Logging Tables (for data analysis)\n",
    "# ============================================================================\n",
    "\n",
    "wandb.init(\n",
    "    project=\"de-LLM\",\n",
    "    name=\"table-logging-example\"\n",
    ")\n",
    "\n",
    "# Create a table with predictions\n",
    "columns = [\"image_id\", \"prediction\", \"ground_truth\", \"confidence\"]\n",
    "data = [\n",
    "    [f\"img_{i}\", f\"class_{np.random.randint(0, 10)}\", f\"class_{np.random.randint(0, 10)}\", np.random.rand()]\n",
    "    for i in range(10)\n",
    "]\n",
    "\n",
    "table = wandb.Table(columns=columns, data=data)\n",
    "wandb.log({\"predictions\": table})\n",
    "\n",
    "# Log confusion matrix as table\n",
    "confusion_matrix = np.random.randint(0, 100, (5, 5))\n",
    "cm_table = wandb.Table(\n",
    "    columns=[f\"class_{i}\" for i in range(5)],\n",
    "    data=confusion_matrix.tolist()\n",
    ")\n",
    "wandb.log({\"confusion_matrix\": cm_table})\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083927ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Example 5: Complete Training Example (PyTorch)\n",
    "# ============================================================================\n",
    "\n",
    "# Simple model\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize wandb with hyperparameters\n",
    "wandb.init(\n",
    "    project=\"de-LLM\",\n",
    "    name=\"pytorch-training-example\",\n",
    "    config={\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"batch_size\": 32,\n",
    "        \"epochs\": 3,\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"model\": \"SimpleMLP\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create model and log architecture\n",
    "model = SimpleModel()\n",
    "wandb.watch(model, log=\"all\", log_freq=10)  # Track gradients and parameters\n",
    "\n",
    "# Create dummy data\n",
    "dummy_input = torch.randn(32, 784)\n",
    "dummy_target = torch.randint(0, 10, (32,))\n",
    "\n",
    "# Training setup\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=wandb.config.learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(wandb.config.epochs):\n",
    "    model.train()\n",
    "    \n",
    "    # Simulate batch training\n",
    "    for batch_idx in range(10):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(dummy_input)\n",
    "        loss = criterion(output, dummy_target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Log metrics every batch\n",
    "        if batch_idx % 5 == 0:\n",
    "            accuracy = (output.argmax(dim=1) == dummy_target).float().mean()\n",
    "            wandb.log({\n",
    "                \"batch_loss\": loss.item(),\n",
    "                \"batch_accuracy\": accuracy.item(),\n",
    "                \"epoch\": epoch,\n",
    "                \"batch\": batch_idx\n",
    "            })\n",
    "    \n",
    "    # Log epoch-level metrics\n",
    "    wandb.log({\n",
    "        \"epoch_loss\": loss.item(),\n",
    "        \"epoch\": epoch\n",
    "    })\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1eee19",
   "metadata": {},
   "source": [
    "# 2. How to View Data in wandb UI\n",
    "\n",
    "## Accessing the UI:\n",
    "\n",
    "1. **Web Interface**: \n",
    "   - Visit https://wandb.ai\n",
    "   - Login with your account\n",
    "   - Select your project (e.g., \"de-LLM\")\n",
    "\n",
    "2. **From Code**:\n",
    "   - Use `run.url` to get direct link to your run\n",
    "   - Or use `wandb.run.url` after initialization\n",
    "\n",
    "## What You Can See in the UI:\n",
    "\n",
    "### Dashboard View:\n",
    "- **Runs Table**: List of all runs with metrics, configs, and status\n",
    "- **Parallel Coordinates**: Visualize hyperparameter relationships\n",
    "- **Scatter Plots**: Compare runs across different metrics\n",
    "\n",
    "### Run Details:\n",
    "- **Metrics Tab**: Time series plots of all logged metrics\n",
    "- **System Tab**: CPU, GPU, memory usage over time\n",
    "- **Logs Tab**: Console output from your training\n",
    "- **Files Tab**: Saved model checkpoints, configs, etc.\n",
    "- **Media Tab**: Images, videos, audio you logged\n",
    "- **Tables Tab**: Data tables you logged\n",
    "\n",
    "### Key Features:\n",
    "- **Compare Runs**: Select multiple runs to compare side-by-side\n",
    "- **Filter Runs**: By tags, config values, or metrics\n",
    "- **Group Runs**: Organize runs by hyperparameters\n",
    "- **Sweep**: Hyperparameter optimization (see Example 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68b340e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Example 6: Hyperparameter Sweep (Automated Search)\n",
    "# ============================================================================\n",
    "\n",
    "# Define sweep configuration\n",
    "sweep_config = {\n",
    "    \"method\": \"grid\",  # or \"random\", \"bayes\"\n",
    "    \"metric\": {\n",
    "        \"name\": \"val_accuracy\",\n",
    "        \"goal\": \"maximize\"\n",
    "    },\n",
    "    \"parameters\": {\n",
    "        \"learning_rate\": {\n",
    "            \"values\": [0.001, 0.01, 0.1]\n",
    "        },\n",
    "        \"batch_size\": {\n",
    "            \"values\": [32, 64, 128]\n",
    "        },\n",
    "        \"epochs\": {\n",
    "            \"value\": 5\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize sweep (uncomment to create sweep)\n",
    "# sweep_id = wandb.sweep(sweep_config, project=\"de-LLM\")\n",
    "\n",
    "# Define training function for sweep\n",
    "def train_sweep():\n",
    "    # Initialize wandb for this sweep run\n",
    "    wandb.init(project=\"de-LLM\")\n",
    "    \n",
    "    # Access hyperparameters from config\n",
    "    lr = wandb.config.learning_rate\n",
    "    batch_size = wandb.config.batch_size\n",
    "    \n",
    "    # Simulate training\n",
    "    for epoch in range(wandb.config.epochs):\n",
    "        train_loss = 1.0 / (epoch + 1) + np.random.normal(0, 0.1)\n",
    "        val_acc = 0.5 + epoch * 0.1 + np.random.normal(0, 0.05)\n",
    "        \n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"val_accuracy\": val_acc\n",
    "        })\n",
    "    \n",
    "    wandb.finish()\n",
    "\n",
    "# Run sweep (uncomment to execute)\n",
    "# wandb.agent(sweep_id, train_sweep, count=10)\n",
    "\n",
    "print(\"Sweep configuration created. Uncomment code to run sweep.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051cceaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Example 7: Logging Model Artifacts\n",
    "# ============================================================================\n",
    "\n",
    "wandb.init(\n",
    "    project=\"de-LLM\",\n",
    "    name=\"artifact-example\"\n",
    ")\n",
    "\n",
    "# Create a simple model\n",
    "model = SimpleModel()\n",
    "\n",
    "# Save model checkpoint\n",
    "torch.save(model.state_dict(), \"model_checkpoint.pth\")\n",
    "\n",
    "# Log as artifact\n",
    "artifact = wandb.Artifact(\"model\", type=\"model\")\n",
    "artifact.add_file(\"model_checkpoint.pth\")\n",
    "wandb.log_artifact(artifact)\n",
    "\n",
    "# Log dataset\n",
    "dataset_artifact = wandb.Artifact(\"dataset\", type=\"dataset\")\n",
    "# Add files to artifact\n",
    "# dataset_artifact.add_dir(\"data/\")\n",
    "wandb.log_artifact(dataset_artifact)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e131dcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Example 8: Best Practices and Tips\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\"\"\n",
    "BEST PRACTICES:\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "1. ✅ Use descriptive run names\n",
    "   wandb.init(name=\"resnet50-lr0.001-bs64\")\n",
    "\n",
    "2. ✅ Log hyperparameters in config\n",
    "   wandb.config.update({\"lr\": 0.001, \"batch_size\": 64})\n",
    "\n",
    "3. ✅ Use namespaces for metrics (with slashes)\n",
    "   wandb.log({\"train/loss\": loss, \"val/loss\": val_loss})\n",
    "\n",
    "4. ✅ Log at appropriate frequency\n",
    "   - Every epoch: wandb.log({\"epoch\": epoch, ...})\n",
    "   - Every N batches: if batch_idx % N == 0: wandb.log(...)\n",
    "\n",
    "5. ✅ Use wandb.watch() for PyTorch models\n",
    "   wandb.watch(model, log=\"all\", log_freq=100)\n",
    "\n",
    "6. ✅ Finish runs properly\n",
    "   wandb.finish()  # Always call this!\n",
    "\n",
    "7. ✅ Use tags for organization\n",
    "   wandb.init(tags=[\"experiment\", \"baseline\"])\n",
    "\n",
    "8. ✅ Log images/videos for debugging\n",
    "   wandb.log({\"predictions\": wandb.Image(image)})\n",
    "\n",
    "9. ✅ Use wandb.alert() for important events\n",
    "   wandb.alert(title=\"Training Complete\", text=\"Model converged!\")\n",
    "\n",
    "10. ✅ Compare runs using UI filters\n",
    "    - Filter by tags, config values, or metrics\n",
    "    - Use parallel coordinates plot for hyperparameter analysis\n",
    "\n",
    "COMMON COMMANDS:\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "Terminal commands:\n",
    "  wandb login                    # Login to wandb\n",
    "  wandb offline                  # Run in offline mode\n",
    "  wandb sync <run_dir>          # Sync offline runs\n",
    "  wandb status                  # Check login status\n",
    "\n",
    "Python API:\n",
    "  wandb.init()                  # Initialize run\n",
    "  wandb.log()                   # Log metrics\n",
    "  wandb.config                  # Access hyperparameters\n",
    "  wandb.watch()                 # Track model gradients\n",
    "  wandb.finish()                # End run\n",
    "  wandb.save()                  # Save files\n",
    "  wandb.log_artifact()         # Log artifacts\n",
    "\n",
    "VIEWING IN UI:\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "1. Go to https://wandb.ai\n",
    "2. Select your project (e.g., \"de-LLM\")\n",
    "3. Click on a run to see:\n",
    "   - Metrics: Time series plots\n",
    "   - System: CPU/GPU usage\n",
    "   - Logs: Console output\n",
    "   - Files: Saved checkpoints\n",
    "   - Media: Images/videos\n",
    "   - Tables: Data tables\n",
    "\n",
    "4. Compare runs:\n",
    "   - Select multiple runs\n",
    "   - Use filters (tags, config, metrics)\n",
    "   - View parallel coordinates plot\n",
    "\n",
    "5. Create reports:\n",
    "   - Share findings with team\n",
    "   - Document experiments\n",
    "   - Track progress over time\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d120b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Example 9: Quick Reference - Minimal Working Example\n",
    "# ============================================================================\n",
    "\n",
    "# Minimal example - copy this template for your projects\n",
    "wandb.init(\n",
    "    project=\"de-LLM\",\n",
    "    name=\"minimal-example\",\n",
    "    config={\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"batch_size\": 32\n",
    "    }\n",
    ")\n",
    "\n",
    "# Log metrics\n",
    "for i in range(10):\n",
    "    wandb.log({\n",
    "        \"loss\": 1.0 / (i + 1),\n",
    "        \"accuracy\": i * 0.1\n",
    "    })\n",
    "\n",
    "# Get run URL (open in browser to see results)\n",
    "print(f\"View your run at: {wandb.run.url}\")\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "print(\"\\n✅ Check the URL above to see your metrics in the wandb UI!\")\n",
    "print(\"   - Metrics tab: See loss and accuracy plots\")\n",
    "print(\"   - System tab: See resource usage\")\n",
    "print(\"   - Config tab: See hyperparameters\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
