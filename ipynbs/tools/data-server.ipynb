{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "8173567d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "\n",
        "# ─────────────── Normal full download ───────────────\n",
        "# ds = load_dataset(\"rotten_tomatoes\")                     # full dataset\n",
        "# # or\n",
        "# ds = load_dataset(\"rotten_tomatoes\", split=\"train\")       # only one split\n",
        "\n",
        "# print(ds)\n",
        "# for i in range(10):\n",
        "#     print(ds[i])           # first example\n",
        "# print(ds.column_names)\n",
        "\n",
        "a = np.random.randint(0, 100)\n",
        "print(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "92427b12",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['features', 'rows', 'num_rows_total', 'num_rows_per_page', 'partial'])\n",
            "10\n",
            "{'row_idx': 150, 'row': {'text': 'This Weekend Steve Martin Also Apologized For A Racist Tweet\\n\\n\\nThis weekend, the world was out to get Justine Sacco fired for her ignorant tweet and failed to notice what Steve Martin was up to.\\n\\nMartin was having some fun on his Twitter page, discussing the merits of grammar.\\xa0 Fans of his would ask him questions about wording and he\\'d fire back a funny comment until one particular comment struck people the wrong way.\\xa0 @BethanyWedel wrote: \"Is this how you spell lasonia?\"\\xa0 And Martin wrote back...\\n\\n\\n\\n\\n\\nMoments later, unlike Sacco, Steve Martin had the wherewithal to delete the comment but it was too late.\\xa0 People started calling Martin out for his remark, which honestly I don\\'t even understand.\\xa0 Is he making a joke about how an African American would say lasagna?\\xa0 Or is he commenting about how they wouldn\\'t know how to spell it if it were written in a menu.\\xa0 I have no idea.\\n\\nAfter many of his fans started calling him out, he wrote:\\n\\n\\n\\n\\n\\nMany of you like to compare Justine Sacco to the scenario of \"What if Chris Rock or Sarah Silverman said that!\".\\xa0 Well here is a similar scenario presented to you from the same exact weekend starring Steve Martin, an extremely accomplished comedian.\\xa0 And yes, he too had to apologize.\\xa0 Unfortunately for Sacco, she\\'s not a comedian for hire, and her bosses had the right to remove her for her grossly inappropriate comment.\\xa0\\n\\nIn 2004 Gilbert Gottfried made comments about the Tsnumi victims while hired to be the recurring voice of the Aflac duck.\\xa0 He tweeted 12 jokes, one of which was \"Japan called me. They said \"maybe those jokes are a hit in the US, but over here, they\\'re all sinking.\"\\xa0 He was then fired from his job.\\xa0 That\\'s right.\\xa0 A comedian was fired...from his job.\\n\\n\\n\\n\\n\\nBut the question is was Steve Martin\\'s tweet harsh enough in your eyes to warrant an apology or should he just quit spending so much time on Twitter and write a sequel to The Jerk already. (I love this movie)\\n\\n- Todd Spence (twitter)\\n\\n\\nUm_ok User\\n\\nJust to see, I asked my black friend to repeat Steve Martin\\'s words and I play the part of the stupid person and everyone around us laughed. Then I switched it up and I said it to my black friend around a new group of people and suddenly I was told that was rude and insensitive. Sorry people but the day you all play stupid and start seeing that black neighborhoods tend to have lower IQ levels and the inhabitants have a lesser grasp of the english language then other areas. This isn\\'t me being racist, its simple fact. Go ahead and \"axe\" somebody. Ebonics came about through laziness just like the Southern Drawl has existed...but we just called it dumb rednecks. Steve made a factual comment that italians would likely know how to spell a dish they have been served all their lives and black families might not. Don\\'t like it? easy, every time you hear another black person speak with terrible english, correct them and tell them they perpetuate a bad stereotype and should learn to use proper english to set better examples for their community. Or, just live with the fact that so many black neighborhoods are just lower on the IQ scale and will never learn...just like so many other poor ethnicities have equally low IQs. Honestly, race makes no difference because we could be speaking about poor white, mexican, or any race for that matter and they all tend to se english as a second language with their primary being their own version of slang.Â\\xa0\\n\\nTrey-Evitt-16 User\\n\\nI think we\\'re overly sensitive. I am white-as my Saxon Sutton Hoo helmet avatar might be a clue-and from South Carolina. I have joked about pronunciation of certain words with black friends since....ever. \"Ask/Axe\" being the most common. Where I\\'m from, near the Geechee/Gullah regions, the \"Str\" consonant blend is pronounced \"scr\", so, \"Straight\" is pronouncedÂ\\xa0 \"Scraight\". I once said, \"Depending on your neighborhood, \\'indiscreet\\' is either an adjective or a preposition\"and friends both black and white cracked up. And...get this. We were in jail. Yes it was low-security, classified among minor misdemeanor offenders from traffic violations to small amounts of marijuana, but if you\\'ve been locked up, you know jail is no place for a whiteboy to be perceived as \"racist\", even among non-violent offenders.\\n\\nThe real quandary is, \"Is my intelligence being questioned, or are we joking around about colloquialisms and pronunciation\".Â\\xa0\\n\\n\"1bigfatcat\" \\'s point is well-taken, that, on a serious level, anti-white hate/discrimination goes challenged for the most part; it is rare for an act of black-on-white violence to be classified as a \"hate crime\". And it awakens my \"inner skinhead\" that neither Nelson Mandela\\'s memorial services, nor First Lady Obama\\'s previous trip to South Africa, afforded a dialog about white victims the Boer Genocide.Â\\xa0\\n\\nBlack comedians seem to get more leeway;Â\\xa0\\n\\n\"Gynecologist\" is correct Cris Rock would never feel the need to apologize for the same remark Steve Martin made.Â\\xa0\\n\\nAny idiot thinking Steve Martin is racist would have to read this tweet, Google his image and see him with \"a banjo on his knee\"Â\\xa0 His agnostic free-thought, his description of himself as \"born a poor black child\" in \"The Jerk\", and his beginnings as a Second-City/SNL comic in the subversive counterculture of the 60s and 70s should speak volumes. Racist atheists are few and far between.\\n\\n\\njoedoaks User\\n\\nOookay.... I\\'d only give that a 3.Â\\xa0 That\\'s for both comedy and racism.Â\\xa0 You everÂ\\xa0READ how horrifically some supposedly educated people can spell nowadays?\\n\\nTacoLoco User\\n\\nthat was probably the funniest thing steve martin has said in 20 years, i guess only black people can make race based jokes in america\\n\\nLMNT115 User\\n\\nI don\\'t get it....why is it a black comedian can stand on stage and tear into \\' Whitey \\' and everyone laughs...but a white comedian throws down something like what Steve Martin\\'s unacceptable ?\\n\\nXzelick User\\n\\nTodd Spence thinks that Japanese Tsunami happened in 2004, what an idiot. Â\\xa0Gilbert Gottfried was fired after the Tohoku earthquake/tsunami in 2011!\\n\\n\\nWould have been funny if Chris Rock said it.\\n\\nBigBlueMouse User\\n\\n\" he commenting about how they wouldn\\'t know how to spell it if it were written in a menu.Â\\xa0 I have no idea.\"\\n\\nThat\\'s not surprising, since you\\'ve used the word \"reocurring\", which doesn\\'t even exist.\\n\\n\\nIt\\'s an obvious joke, dummy. Lasonia sounds like a black woman\\'s name.\\n\\n1bigfatcat User\\n\\nI\\'m a white male and therefore it\\'s legal to hate on me and discriminate against me.\\n\\nEvery day, people treat me with prejudice because I am a white male.Â\\xa0Â\\xa0\\n\\nHow about this:Â\\xa0 Instead of taking offence, take \"tolerance!\"\\n\\nComedy is comedy.....unless you don\\'t like it, then it\\'s \"racist.\"\\n\\nEither black, islamic, Japanese, american, etc., comedians ALL tell jokes on each other, or NO one tells jokes on each other.\\n\\nBe fair, and tolerant!\\n\\nMatt-Kim-806 User\\n\\nit\\'s not a double standard you just think the cases are the same because you\\'re stupid\\n\\nNeedles_Malloy User\\n\\n@1bigfatcatÂ\\xa0Since you admitted you are white, you are hereby immediately (at birth) guilty of white privilege. Every word you have ever spoken, is literally, the most racist statement uttered since a Hitler speech. The only way to assuage your much-deserved guilt is to vote for Obama, twice, and then tell all of your other white-guilted Obamadrone friends that you have begun step 1 of your 123,698 step journey to make amends to the blacks.Â', 'url': 'http://www.break.com/article/justine-sacco-and-steve-martin-apologize-for-racist-tweets-2555226', 'id': '<urn:uuid:500b7430-e9d9-42d0-9e9b-25361c02a1d0>', 'language': 'en', 'language_score': 0.9673395752906799, 'fasttext_score': 0.4027075171470642}, 'truncated_cells': []}\n"
          ]
        }
      ],
      "source": [
        "# Python + requests (recommended)\n",
        "import requests\n",
        "from pprint import pprint\n",
        "\n",
        "params = {\n",
        "    # \"dataset\": \"cornell-movie-review-data/rotten_tomatoes\",\n",
        "    \"dataset\": \"mlfoundations/dclm-baseline-1.0-parquet\",\n",
        "    \"config\":  \"default\",\n",
        "    \"split\":   \"train\",\n",
        "    \"offset\":  150,\n",
        "    \"length\":  10\n",
        "}\n",
        "\n",
        "url = \"https://datasets-server.huggingface.co/rows\"\n",
        "response = requests.get(url, params=params)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    data = response.json()\n",
        "    print(data.keys())              \n",
        "    print(len(data[\"rows\"]))        \n",
        "    print(data[\"rows\"][0])       \n",
        "else:\n",
        "    print(response.status_code, response.text)\n",
        "\n",
        "# url = \"https://datasets-server.huggingface.co/size\"\n",
        "# params = {\n",
        "#     \"dataset\": \"mlfoundations/dclm-baseline-1.0-parquet\",\n",
        "# }\n",
        "# response = requests.get(url, params=params)\n",
        "\n",
        "# if response.status_code == 200:\n",
        "#     data = response.json()\n",
        "#     pprint(data)\n",
        "#     splits = data.get(\"size\").get(\"dataset\").get(\"num_rows\")\n",
        "#     pprint(splits)\n",
        "    # size = sum(split_info.get(\"num_rows\", 0) for split_info in splits.values())\n",
        "    # print(size )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a09d91f1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: 404 {\"error\":\"The dataset has been renamed. Please use the current dataset name.\"}\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "name = \"mlfoundations/dclm-baseline-1.0-parquet\"\n",
        "BASE = \"https://datasets-server.huggingface.co/rows\"\n",
        "\n",
        "# ─── 2.A Get first rows (preview) ────────────────────────────────\n",
        "def get_first_rows(dataset_name, config_name=\"default\", split=\"train\", rows=100):\n",
        "    url = f\"{BASE}\"\n",
        "    params = {\n",
        "        \"dataset\": dataset_name,\n",
        "        \"config\": config_name,\n",
        "        \"split\": split,\n",
        "        \"rows\": rows,\n",
        "    }\n",
        "    r = requests.get(url, params=params)\n",
        "    if r.status_code != 200:\n",
        "        print(\"Error:\", r.status_code, r.text)\n",
        "        return None\n",
        "    return r.json()\n",
        "\n",
        "# Example\n",
        "preview = get_first_rows(\"rotten_tomatoes\")\n",
        "if preview:\n",
        "    print(json.dumps(preview[\"rows\"][:2], indent=2))   # first 2 examples\n",
        "\n",
        "# ─── 2.B Get parquet URLs (good for large datasets / custom loading) ───\n",
        "def get_parquet_urls(dataset_name):\n",
        "    url = f\"{BASE}/parquet\"\n",
        "    params = {\"dataset\": dataset_name}\n",
        "    # r = requests.get(url, params=params)\n",
        "    r = requests.get(\"https://datasets-server.huggingface.co/rows?dataset=cornell-movie-review-data/rotten_tomatoes&config=default&split=train&offset=150&length=10\")\n",
        "    # r = requests.get(f\"https://datasets-server.huggingface.co/rows/?dataset=rotten_tomatoes&offset=0&length=100\")\n",
        "    if r.status_code == 200:\n",
        "        return r.json()\n",
        "    else:\n",
        "        print(\"Error:\", r.text)\n",
        "        return None\n",
        "\n",
        "# Example\n",
        "parquet_info = get_parquet_urls(name)\n",
        "# if parquet_info:\n",
        "#     print(\"Parquet files:\")\n",
        "#     for file in parquet_info.get(\"parquet\", []):\n",
        "#         print(f\"• {file['split']} → {file['url']} ({file['size_bytes']/1e6:.1f} MB)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e377eaee",
      "metadata": {},
      "source": [
        "# Methods to Fetch Data from HuggingFace Datasets-Server\n",
        "\n",
        "## Available Methods:\n",
        "\n",
        "1. **`/rows`** - Fetch specific rows by offset and length\n",
        "2. **`/first-rows`** - Get preview rows (first N rows)\n",
        "3. **`/parquet`** - Get parquet file URLs (download entire files)\n",
        "4. **`/size`** - Get dataset size information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "1271b74f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "Method 1: Fetch Rows by Offset/Length\n",
            "======================================================================\n",
            "Error 422: {\"error\":\"Parameter 'config' is required\"}\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Method 1: Fetch Rows by Offset and Length (Most Flexible)\n",
        "# ============================================================================\n",
        "\n",
        "def fetch_rows(dataset_name, offset=0, length=100, config=None, split=None):\n",
        "    \"\"\"\n",
        "    Fetch specific rows from HuggingFace datasets-server.\n",
        "    \n",
        "    Args:\n",
        "        dataset_name: Name of the dataset\n",
        "        offset: Starting row index (default: 0)\n",
        "        length: Number of rows to fetch (default: 100)\n",
        "        config: Optional config name\n",
        "        split: Optional split name (e.g., \"train\", \"test\")\n",
        "    \n",
        "    Returns:\n",
        "        JSON response with rows data\n",
        "    \"\"\"\n",
        "    url = f\"{BASE}/rows\"\n",
        "    params = {\n",
        "        \"dataset\": dataset_name,\n",
        "        \"offset\": offset,\n",
        "        \"length\": length,\n",
        "    }\n",
        "    \n",
        "    if config:\n",
        "        params[\"config\"] = config\n",
        "    if split:\n",
        "        params[\"split\"] = split\n",
        "    \n",
        "    r = requests.get(url, params=params)\n",
        "    if r.status_code == 200:\n",
        "        return r.json()\n",
        "    else:\n",
        "        print(f\"Error {r.status_code}: {r.text}\")\n",
        "        return None\n",
        "\n",
        "# Example: Fetch first 10 rows\n",
        "print(\"=\" * 70)\n",
        "print(\"Method 1: Fetch Rows by Offset/Length\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "dataset_name = \"rotten_tomatoes\"\n",
        "data = fetch_rows(dataset_name, offset=0, length=10, split=\"train\")\n",
        "\n",
        "if data:\n",
        "    print(f\"Dataset: {dataset_name}\")\n",
        "    print(f\"Total rows in response: {len(data.get('rows', []))}\")\n",
        "    print(f\"\\nFirst row:\")\n",
        "    if data.get('rows'):\n",
        "        first_row = data['rows'][0]\n",
        "        print(f\"  Row data: {first_row.get('row', {})}\")\n",
        "        print(f\"  Row index: {first_row.get('row_idx', 'N/A')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29ebbc27",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Method 2: Fetch Data in Batches (For Large Datasets)\n",
        "# ============================================================================\n",
        "\n",
        "def fetch_dataset_batch(dataset_name, start_idx=0, batch_size=100, split=\"train\"):\n",
        "    \"\"\"\n",
        "    Fetch a batch of data from the dataset.\n",
        "    \n",
        "    Args:\n",
        "        dataset_name: Name of the dataset\n",
        "        start_idx: Starting index\n",
        "        batch_size: Number of rows to fetch\n",
        "        split: Dataset split name\n",
        "    \n",
        "    Returns:\n",
        "        List of row dictionaries\n",
        "    \"\"\"\n",
        "    data = fetch_rows(dataset_name, offset=start_idx, length=batch_size, split=split)\n",
        "    if data:\n",
        "        return [row.get('row', {}) for row in data.get('rows', [])]\n",
        "    return []\n",
        "\n",
        "def fetch_all_data(dataset_name, total_rows=None, batch_size=100, split=\"train\"):\n",
        "    \"\"\"\n",
        "    Fetch all data from dataset in batches.\n",
        "    \n",
        "    Args:\n",
        "        dataset_name: Name of the dataset\n",
        "        total_rows: Total number of rows (if None, will fetch until empty)\n",
        "        batch_size: Number of rows per batch\n",
        "        split: Dataset split name\n",
        "    \n",
        "    Returns:\n",
        "        List of all row dictionaries\n",
        "    \"\"\"\n",
        "    all_data = []\n",
        "    offset = 0\n",
        "    \n",
        "    while True:\n",
        "        batch = fetch_dataset_batch(dataset_name, offset, batch_size, split)\n",
        "        if not batch:\n",
        "            break\n",
        "        \n",
        "        all_data.extend(batch)\n",
        "        offset += len(batch)\n",
        "        \n",
        "        print(f\"Fetched {len(all_data)} rows so far...\")\n",
        "        \n",
        "        if total_rows and len(all_data) >= total_rows:\n",
        "            all_data = all_data[:total_rows]\n",
        "            break\n",
        "    \n",
        "    return all_data\n",
        "\n",
        "# Example: Fetch data in batches\n",
        "print(\"=\" * 70)\n",
        "print(\"Method 2: Fetch Data in Batches\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Fetch first 50 rows in batches of 20\n",
        "batches = []\n",
        "for i in range(0, 50, 20):\n",
        "    batch = fetch_dataset_batch(\"rotten_tomatoes\", start_idx=i, batch_size=20, split=\"train\")\n",
        "    if batch:\n",
        "        batches.extend(batch)\n",
        "        print(f\"Fetched batch starting at index {i}: {len(batch)} rows\")\n",
        "\n",
        "print(f\"\\nTotal rows fetched: {len(batches)}\")\n",
        "if batches:\n",
        "    print(f\"Sample row keys: {list(batches[0].keys())}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5aa7f3d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Method 3: Get Dataset Size First, Then Fetch\n",
        "# ============================================================================\n",
        "\n",
        "def get_dataset_size(dataset_name, config=None, split=None):\n",
        "    \"\"\"\n",
        "    Get the size (number of rows) of a dataset.\n",
        "    \n",
        "    Args:\n",
        "        dataset_name: Name of the dataset\n",
        "        config: Optional config name\n",
        "        split: Optional split name\n",
        "    \n",
        "    Returns:\n",
        "        Number of rows, or None if error\n",
        "    \"\"\"\n",
        "    url = f\"{BASE}/size\"\n",
        "    params = {\"dataset\": dataset_name}\n",
        "    \n",
        "    if config:\n",
        "        params[\"config\"] = config\n",
        "    if split:\n",
        "        params[\"split\"] = split\n",
        "    \n",
        "    r = requests.get(url, params=params)\n",
        "    if r.status_code == 200:\n",
        "        data = r.json()\n",
        "        if split:\n",
        "            return data.get(\"splits\", {}).get(split, {}).get(\"num_rows\", 0)\n",
        "        else:\n",
        "            # Return total across all splits\n",
        "            splits = data.get(\"splits\", {})\n",
        "            return sum(split_info.get(\"num_rows\", 0) for split_info in splits.values())\n",
        "    else:\n",
        "        print(f\"Error {r.status_code}: {r.text}\")\n",
        "        return None\n",
        "\n",
        "# Example: Get size and fetch data\n",
        "print(\"=\" * 70)\n",
        "print(\"Method 3: Get Size Then Fetch\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "dataset_name = \"rotten_tomatoes\"\n",
        "size = get_dataset_size(dataset_name, split=\"train\")\n",
        "print(f\"Dataset: {dataset_name}\")\n",
        "print(f\"Train split size: {size} rows\")\n",
        "\n",
        "if size:\n",
        "    # Fetch first 10 rows\n",
        "    data = fetch_rows(dataset_name, offset=0, length=10, split=\"train\")\n",
        "    if data:\n",
        "        print(f\"\\nFetched {len(data.get('rows', []))} rows\")\n",
        "        print(f\"Total available: {size} rows\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa5df74b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Method 4: Download Parquet Files (For Large Datasets)\n",
        "# ============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "from io import BytesIO\n",
        "\n",
        "def download_parquet_data(dataset_name, split=\"train\", max_rows=None):\n",
        "    \"\"\"\n",
        "    Download and load data from parquet files.\n",
        "    \n",
        "    Args:\n",
        "        dataset_name: Name of the dataset\n",
        "        split: Dataset split name\n",
        "        max_rows: Maximum number of rows to load (None for all)\n",
        "    \n",
        "    Returns:\n",
        "        pandas DataFrame with the data\n",
        "    \"\"\"\n",
        "    # Get parquet URLs\n",
        "    parquet_info = get_parquet_urls(dataset_name)\n",
        "    \n",
        "    if not parquet_info:\n",
        "        return None\n",
        "    \n",
        "    # Find the parquet file for the requested split\n",
        "    parquet_files = parquet_info.get(\"parquet\", [])\n",
        "    target_file = None\n",
        "    \n",
        "    for file_info in parquet_files:\n",
        "        if file_info.get(\"split\") == split:\n",
        "            target_file = file_info\n",
        "            break\n",
        "    \n",
        "    if not target_file:\n",
        "        print(f\"Split '{split}' not found in parquet files\")\n",
        "        return None\n",
        "    \n",
        "    parquet_url = target_file[\"url\"]\n",
        "    print(f\"Downloading parquet file: {parquet_url}\")\n",
        "    print(f\"Size: {target_file['size_bytes'] / 1e6:.1f} MB\")\n",
        "    \n",
        "    # Download and read parquet\n",
        "    response = requests.get(parquet_url)\n",
        "    if response.status_code == 200:\n",
        "        parquet_data = BytesIO(response.content)\n",
        "        df = pd.read_parquet(parquet_data)\n",
        "        \n",
        "        if max_rows:\n",
        "            df = df.head(max_rows)\n",
        "        \n",
        "        return df\n",
        "    else:\n",
        "        print(f\"Error downloading: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "# Example: Download parquet data\n",
        "print(\"=\" * 70)\n",
        "print(\"Method 4: Download Parquet Files\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Note: This downloads the entire file, use with caution for large datasets\n",
        "# For demo, we'll just show the URL structure\n",
        "parquet_info = get_parquet_urls(\"rotten_tomatoes\")\n",
        "if parquet_info:\n",
        "    print(\"Available parquet files:\")\n",
        "    for file_info in parquet_info.get(\"parquet\", []):\n",
        "        print(f\"  Split: {file_info.get('split')}\")\n",
        "        print(f\"  URL: {file_info.get('url')}\")\n",
        "        print(f\"  Size: {file_info.get('size_bytes', 0) / 1e6:.1f} MB\")\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a78a585",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# Method 5: Complete Example - Fetch and Process Data\n",
        "# ============================================================================\n",
        "\n",
        "def fetch_and_process_dataset(dataset_name, num_rows=100, split=\"train\"):\n",
        "    \"\"\"\n",
        "    Complete example: Fetch data and process it.\n",
        "    \n",
        "    Args:\n",
        "        dataset_name: Name of the dataset\n",
        "        num_rows: Number of rows to fetch\n",
        "        split: Dataset split name\n",
        "    \n",
        "    Returns:\n",
        "        List of processed row dictionaries\n",
        "    \"\"\"\n",
        "    print(f\"Fetching {num_rows} rows from {dataset_name} ({split} split)...\")\n",
        "    \n",
        "    # Fetch data in batches\n",
        "    all_rows = []\n",
        "    batch_size = 50\n",
        "    \n",
        "    for offset in range(0, num_rows, batch_size):\n",
        "        batch_length = min(batch_size, num_rows - offset)\n",
        "        data = fetch_rows(dataset_name, offset=offset, length=batch_length, split=split)\n",
        "        \n",
        "        if data:\n",
        "            rows = data.get(\"rows\", [])\n",
        "            for row_info in rows:\n",
        "                row_data = row_info.get(\"row\", {})\n",
        "                all_rows.append(row_data)\n",
        "        \n",
        "        print(f\"  Progress: {len(all_rows)}/{num_rows} rows\")\n",
        "    \n",
        "    return all_rows\n",
        "\n",
        "# Example: Complete workflow\n",
        "print(\"=\" * 70)\n",
        "print(\"Method 5: Complete Example\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "dataset_name = \"rotten_tomatoes\"\n",
        "rows = fetch_and_process_dataset(dataset_name, num_rows=20, split=\"train\")\n",
        "\n",
        "print(f\"\\nFetched {len(rows)} rows\")\n",
        "if rows:\n",
        "    print(f\"\\nSample row:\")\n",
        "    print(f\"  Keys: {list(rows[0].keys())}\")\n",
        "    print(f\"  First row data: {rows[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "6cd3eb70",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "Method 6: Streaming Iterator (Memory Efficient)\n",
            "======================================================================\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'fetch_rows' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m)\n\u001b[32m     43\u001b[39m count = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrotten_tomatoes\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcount\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[43m \u001b[49m\u001b[43m<\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m:\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mstream_dataset\u001b[39m\u001b[34m(dataset_name, split, batch_size)\u001b[39m\n\u001b[32m     17\u001b[39m offset = \u001b[32m0\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     data = \u001b[43mfetch_rows\u001b[49m(dataset_name, offset=offset, length=batch_size, split=split)\n\u001b[32m     22\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[32m     23\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "\u001b[31mNameError\u001b[39m: name 'fetch_rows' is not defined"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Method 6: Streaming Iterator (Memory Efficient)\n",
        "# ============================================================================\n",
        "\n",
        "def stream_dataset(dataset_name, split=\"train\", batch_size=100):\n",
        "    \"\"\"\n",
        "    Stream dataset rows one by one (memory efficient).\n",
        "    \n",
        "    Args:\n",
        "        dataset_name: Name of the dataset\n",
        "        split: Dataset split name\n",
        "        batch_size: Number of rows to fetch per request\n",
        "    \n",
        "    Yields:\n",
        "        Row dictionaries\n",
        "    \"\"\"\n",
        "    offset = 0\n",
        "    \n",
        "    while True:\n",
        "        data = fetch_rows(dataset_name, offset=offset, length=batch_size, split=split)\n",
        "        \n",
        "        if not data:\n",
        "            break\n",
        "        \n",
        "        rows = data.get(\"rows\", [])\n",
        "        if not rows:\n",
        "            break\n",
        "        \n",
        "        for row_info in rows:\n",
        "            yield row_info.get(\"row\", {})\n",
        "        \n",
        "        offset += len(rows)\n",
        "        \n",
        "        # Stop if we got fewer rows than requested (end of dataset)\n",
        "        if len(rows) < batch_size:\n",
        "            break\n",
        "\n",
        "# Example: Stream dataset\n",
        "print(\"=\" * 70)\n",
        "print(\"Method 6: Streaming Iterator (Memory Efficient)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "count = 0\n",
        "for row in stream_dataset(\"rotten_tomatoes\", split=\"train\", batch_size=10):\n",
        "    count += 1\n",
        "    if count <= 3:\n",
        "        print(f\"Row {count}: {list(row.keys())}\")\n",
        "    if count >= 10:\n",
        "        break\n",
        "\n",
        "print(f\"\\nStreamed {count} rows\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91e10fe8",
      "metadata": {},
      "source": [
        "## Summary: Methods to Fetch Data from HuggingFace Datasets-Server\n",
        "\n",
        "### API Endpoints:\n",
        "\n",
        "1. **`/rows`** - Fetch specific rows\n",
        "   - Parameters: `dataset`, `offset`, `length`, `config`, `split`\n",
        "   - Best for: Fetching specific ranges of data\n",
        "\n",
        "2. **`/first-rows`** - Get preview\n",
        "   - Parameters: `dataset`, `config`, `split`, `rows`\n",
        "   - Best for: Quick preview of dataset\n",
        "\n",
        "3. **`/parquet`** - Get parquet file URLs\n",
        "   - Parameters: `dataset`\n",
        "   - Best for: Downloading entire dataset files\n",
        "\n",
        "4. **`/size`** - Get dataset size\n",
        "   - Parameters: `dataset`, `config`, `split`\n",
        "   - Best for: Knowing dataset size before fetching\n",
        "\n",
        "### When to Use Each Method:\n",
        "\n",
        "- **`/rows`**: When you need specific rows or streaming\n",
        "- **`/first-rows`**: Quick preview or exploration\n",
        "- **`/parquet`**: Large datasets, full download\n",
        "- **`/size`**: Planning data fetching strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0f462ac",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[71, 12, 18, 33, 69, 53, 16,  6, 91, 29, 34, 72, 92, 49, 44, 43, 28, 42,\n",
            "          1, 21]])\n",
            "tensor([[[ 0.0990, -0.6426,  0.5603, -1.0565,  0.0440, -0.3703, -0.6957,\n",
            "           1.3226, -0.0709, -0.6829],\n",
            "         [-1.1853, -0.3042, -0.2293,  0.9693,  0.7995,  0.0747,  1.2152,\n",
            "          -1.9108,  0.3757, -1.0080],\n",
            "         [-0.2708,  2.3503, -0.4000, -0.4539, -2.7979, -0.2071, -0.1967,\n",
            "          -1.3771, -1.7468, -0.5628],\n",
            "         [ 1.0065,  1.0424,  0.3867, -0.5904,  1.0626, -1.1685, -0.2614,\n",
            "          -0.4813, -0.1306, -1.1721],\n",
            "         [ 0.9525, -1.3721, -1.7239,  1.1409,  0.7826, -0.3203, -1.3876,\n",
            "          -1.4209,  0.8466,  0.0927],\n",
            "         [ 1.0444, -0.9065,  1.2252, -0.7500, -1.1239, -0.2047, -0.9176,\n",
            "          -1.4298,  1.1588, -1.5845],\n",
            "         [ 0.5058,  1.3729, -0.6583,  1.5117,  1.1433, -1.0974, -0.3852,\n",
            "          -0.9062, -0.1113, -0.4177],\n",
            "         [-0.5341, -0.7787,  1.3349,  0.6949,  0.1358,  0.2917,  1.7196,\n",
            "           0.2920, -0.7599, -0.1376],\n",
            "         [ 0.4266,  2.0088,  0.7999,  0.3661,  1.5543, -0.5530, -1.3909,\n",
            "           2.1963,  0.1299, -0.6269],\n",
            "         [-1.2855,  0.4254,  1.5260, -1.1320, -1.5478, -1.2349,  0.4900,\n",
            "           0.3042, -0.2382,  1.3744],\n",
            "         [ 0.1208,  1.2443,  0.5536, -1.6776,  1.2954, -1.2434,  0.1924,\n",
            "          -0.9264, -0.5396,  0.0791],\n",
            "         [ 0.6115, -0.6697, -0.5475,  1.0857, -0.1569, -1.0537,  0.0468,\n",
            "           1.4789,  0.0192,  0.5932],\n",
            "         [ 1.5514,  0.3074, -0.1577,  1.0321,  0.9381,  0.0032,  0.4807,\n",
            "          -1.0782, -1.2447, -0.7306],\n",
            "         [ 0.2745, -0.1771, -0.9333, -1.1513, -1.9228, -0.5369,  0.1473,\n",
            "           1.7789,  2.2959,  0.6917],\n",
            "         [ 0.2174, -1.9245,  1.1384,  1.6499, -0.3926, -1.1900,  0.4878,\n",
            "          -0.1652,  0.0062,  1.0940],\n",
            "         [ 1.3834,  0.2142,  0.2850,  0.6928, -0.7039,  0.1668, -0.2054,\n",
            "           0.7146, -0.5307,  0.4812],\n",
            "         [ 0.4080, -0.7819,  1.5677, -0.0138, -1.2681, -0.4432, -0.4714,\n",
            "           0.2271, -0.5913,  0.6506],\n",
            "         [ 0.3532,  2.0074,  1.1087,  0.7603,  1.6938, -1.3144, -0.8197,\n",
            "           2.0929, -0.4725,  0.2886],\n",
            "         [-0.7696,  1.4893, -1.2728,  0.2426,  0.4025, -0.7557, -0.6699,\n",
            "          -0.2988, -1.3158, -1.5496],\n",
            "         [-0.2173,  0.0174, -1.5727, -1.3804,  0.5335, -0.6318, -0.6897,\n",
            "           1.4309, -0.1513, -1.4118]]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "import torch\n",
        "class SimpleModel(nn.Module):\n",
        "   def __init__(self, vocab_size: int, embedding_dim: int):\n",
        "      super().__init__()\n",
        "      self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "   def forward(self, x: Tensor):\n",
        "      return self.embedding(x)\n",
        "      \n",
        "\n",
        "model = SimpleModel(vocab_size=100, embedding_dim=10)\n",
        "\n",
        "x = torch.randint(0, 100, (1, 10, 10))\n",
        "print(x)\n",
        "\n",
        "print(model(x))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
