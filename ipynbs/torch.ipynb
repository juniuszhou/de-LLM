{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3d6de31",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'llm_autoeval'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1027855823.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mllm_autoeval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtable\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_final_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmake_table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mllm_autoeval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mupload_to_github_gist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'llm_autoeval'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "\n",
    "from llm_autoeval.table import make_final_table, make_table\n",
    "from llm_autoeval.upload import upload_to_github_gist\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "MODEL_ID = os.getenv(\"MODEL_ID\")\n",
    "BENCHMARK = os.getenv(\"BENCHMARK\")\n",
    "GITHUB_API_TOKEN = os.getenv(\"GITHUB_API_TOKEN\")\n",
    "\n",
    "\n",
    "def _make_autoeval_summary(directory: str, elapsed_time: float) -> str:\n",
    "    # Variables\n",
    "    tables = []\n",
    "    averages = []\n",
    "\n",
    "    # Tasks\n",
    "    if BENCHMARK == \"openllm\":\n",
    "        tasks = [\"ARC\", \"HellaSwag\", \"MMLU\", \"TruthfulQA\", \"Winogrande\", \"GSM8K\"]\n",
    "    elif BENCHMARK == \"nous\":\n",
    "        tasks = [\"AGIEval\", \"GPT4All\", \"TruthfulQA\", \"Bigbench\"]\n",
    "    elif BENCHMARK == \"eq-bench\":\n",
    "        tasks = [\"EQ-Bench\"]\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            f\"The benchmark {BENCHMARK} could not be found.\"\n",
    "        )\n",
    "\n",
    "    # Load results\n",
    "    for task in tasks:\n",
    "        file_path = f\"{directory}/{task.lower()}.json\"\n",
    "        if os.path.exists(file_path):\n",
    "            json_data = open(file_path, \"r\").read()\n",
    "            data = json.loads(json_data, strict=False)\n",
    "            table, average = make_table(data, task)\n",
    "        else:\n",
    "            table = \"\"\n",
    "            average = \"Error: File does not exist\"\n",
    "\n",
    "        tables.append(table)\n",
    "        averages.append(average)\n",
    "\n",
    "    # Generate tables\n",
    "    summary = \"\"\n",
    "    for index, task in enumerate(tasks):\n",
    "        summary += f\"### {task}\\n{tables[index]}\\nAverage: {averages[index]}%\\n\\n\"\n",
    "    result_dict = {k: v for k, v in zip(tasks, averages)}\n",
    "\n",
    "    # Calculate the final average, excluding strings\n",
    "    if all(isinstance(e, float) for e in averages):\n",
    "        final_average = round(sum(averages) / len(averages), 2)\n",
    "        summary += f\"Average score: {final_average}%\"\n",
    "        result_dict.update({\"Average\": final_average})\n",
    "    else:\n",
    "        summary += \"Average score: Not available due to errors\"\n",
    "\n",
    "    # Generate final table\n",
    "    final_table = make_final_table(result_dict, MODEL_ID)\n",
    "    summary = final_table + \"\\n\" + summary\n",
    "    return summary\n",
    "\n",
    "\n",
    "def _get_result_dict(directory: str) -> dict:\n",
    "    \"\"\"Walk down directories to get JSON path\"\"\"\n",
    "\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".json\"):\n",
    "                return json.load(open(os.path.join(root, file)))\n",
    "    raise FileNotFoundError(f\"No JSON file found in {directory}\")\n",
    "\n",
    "\n",
    "def _make_lighteval_summary(directory: str, elapsed_time: float) -> str:\n",
    "    from lighteval.evaluator import make_results_table\n",
    "\n",
    "    result_dict = _get_result_dict(directory)\n",
    "    final_table = make_results_table(result_dict)\n",
    "    summary = f\"## {MODEL_ID.split('/')[-1]} - {BENCHMARK.capitalize()}\\n\\n\"\n",
    "    summary += final_table\n",
    "    return summary\n",
    "\n",
    "\n",
    "def main(directory: str, elapsed_time: float) -> None:\n",
    "    # Tasks\n",
    "    if BENCHMARK == \"openllm\" or BENCHMARK == \"nous\" or BENCHMARK == \"eq-bench\":\n",
    "        summary = _make_autoeval_summary(directory, elapsed_time)\n",
    "    elif BENCHMARK == \"lighteval\":\n",
    "        summary = _make_lighteval_summary(directory, elapsed_time)\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            f\"BENCHMARK should be 'openllm' or 'nous' (current value = {BENCHMARK})\"\n",
    "        )\n",
    "\n",
    "    # Add elapsed time\n",
    "    convert = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n",
    "    summary += f\"\\n\\nElapsed time: {convert}\"\n",
    "\n",
    "    # Upload to GitHub Gist\n",
    "    upload_to_github_gist(\n",
    "        summary,\n",
    "        f\"{MODEL_ID.split('/')[-1]}-{BENCHMARK.capitalize()}.md\",\n",
    "        GITHUB_API_TOKEN,\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create the parser\n",
    "    parser = argparse.ArgumentParser(description=\"Summarize results and upload them.\")\n",
    "    parser.add_argument(\n",
    "        \"directory\", type=str, help=\"The path to the directory with the JSON results\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"elapsed_time\",\n",
    "        type=float,\n",
    "        help=\"Elapsed time since the start of the evaluation\",\n",
    "    )\n",
    "\n",
    "    # Parse the arguments\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Check if the directory exists\n",
    "    if not os.path.isdir(args.directory):\n",
    "        raise ValueError(f\"The directory {args.directory} does not exist.\")\n",
    "\n",
    "    # Call the main function with the directory argument\n",
    "    main(args.directory, args.elapsed_time)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
