{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d6de31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model parameters size is : 2\n",
      "Parameter containing:\n",
      "tensor([[ 0.0357, -0.0287,  0.1724, -0.2707, -0.0137, -0.0381,  0.0329, -0.1977,\n",
      "          0.0094,  0.0615]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1080], requires_grad=True)\n",
      "tensor([[-0.4749, -0.0625,  0.8765, -0.2465,  0.0393,  0.4007, -1.2988,  0.0104,\n",
      "         -0.8718, -1.3266],\n",
      "        [ 1.8593, -0.6186, -0.2988,  0.8235,  0.6518, -0.6880,  0.1031, -0.2644,\n",
      "         -0.3095, -0.2543],\n",
      "        [-0.2667, -1.1421, -0.3940, -0.5046,  0.2540,  2.2011, -0.0635,  1.2413,\n",
      "          0.5912, -0.5063],\n",
      "        [-0.1872, -1.1356, -2.1528,  2.2590,  1.3966,  0.7185,  2.3417,  0.7461,\n",
      "         -0.8237,  0.6908],\n",
      "        [ 1.6295,  0.7206,  2.1363, -2.6480,  2.2121,  0.5357,  1.7930, -0.1907,\n",
      "         -0.6173,  0.8818]])\n",
      "Original weight (first 3): tensor([ 0.0357, -0.0287,  0.1724])\n",
      "==================================================\n",
      "None\n",
      "None\n",
      "==================================================\n",
      "Before backpropagation\n",
      "Parameter containing:\n",
      "tensor([[ 0.0357, -0.0287,  0.1724, -0.2707, -0.0137, -0.0381,  0.0329, -0.1977,\n",
      "          0.0094,  0.0615]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1080], requires_grad=True)\n",
      "After backpropagation\n",
      "Parameter containing:\n",
      "tensor([[ 0.0357, -0.0287,  0.1724, -0.2707, -0.0137, -0.0381,  0.0329, -0.1977,\n",
      "          0.0094,  0.0615]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1080], requires_grad=True)\n",
      "After optimizer step\n",
      "Parameter containing:\n",
      "tensor([[ 0.0257, -0.0387,  0.1624, -0.2606, -0.0037, -0.0481,  0.0429, -0.1877,\n",
      "          0.0194,  0.0715]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.0979], requires_grad=True)\n",
      "==================================================\n",
      "None\n",
      "None\n",
      "==================================================\n",
      "Before backpropagation\n",
      "Parameter containing:\n",
      "tensor([[ 0.0257, -0.0387,  0.1624, -0.2606, -0.0037, -0.0481,  0.0429, -0.1877,\n",
      "          0.0194,  0.0715]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.0979], requires_grad=True)\n",
      "After backpropagation\n",
      "Parameter containing:\n",
      "tensor([[ 0.0257, -0.0387,  0.1624, -0.2606, -0.0037, -0.0481,  0.0429, -0.1877,\n",
      "          0.0194,  0.0715]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.0979], requires_grad=True)\n",
      "After optimizer step\n",
      "Parameter containing:\n",
      "tensor([[ 0.0157, -0.0487,  0.1524, -0.2506,  0.0063, -0.0581,  0.0529, -0.1777,\n",
      "          0.0294,  0.0815]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.0879], requires_grad=True)\n",
      "==================================================\n",
      "None\n",
      "None\n",
      "==================================================\n",
      "Before backpropagation\n",
      "Parameter containing:\n",
      "tensor([[ 0.0157, -0.0487,  0.1524, -0.2506,  0.0063, -0.0581,  0.0529, -0.1777,\n",
      "          0.0294,  0.0815]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.0879], requires_grad=True)\n",
      "After backpropagation\n",
      "Parameter containing:\n",
      "tensor([[ 0.0157, -0.0487,  0.1524, -0.2506,  0.0063, -0.0581,  0.0529, -0.1777,\n",
      "          0.0294,  0.0815]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.0879], requires_grad=True)\n",
      "After optimizer step\n",
      "Parameter containing:\n",
      "tensor([[ 0.0057, -0.0587,  0.1424, -0.2406,  0.0163, -0.0681,  0.0629, -0.1677,\n",
      "          0.0394,  0.0915]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.0779], requires_grad=True)\n",
      "\n",
      "Final Learned Values:\n",
      "Weight (should be ~2.0):  Parameter containing:\n",
      "tensor([[ 0.0057, -0.0587,  0.1424, -0.2406,  0.0163, -0.0681,  0.0629, -0.1677,\n",
      "          0.0394,  0.0915]], requires_grad=True)\n",
      "Bias   (should be ~1.0):  Parameter containing:\n",
      "tensor([0.0779], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 1. Setup a simple model and data\n",
    "model = nn.Linear(10, 1)\n",
    "# two parameters: weight and bias\n",
    "print(\"model parameters size is :\", len(list(model.parameters())))\n",
    "for para in model.parameters():\n",
    "    print(para)\n",
    "\n",
    "# there are 5 data sample\n",
    "input_data = torch.randn(5, 10)\n",
    "print(input_data)\n",
    "\n",
    "# the real output of 5 data sample\n",
    "target = torch.randn(5, 1)\n",
    "# loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 2. Initialize AdamW optimizer\n",
    "# weight_decay: The coefficient for L2 regularization (default: 0.01)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.01, weight_decay=0.01)\n",
    "\n",
    "print(f\"Original weight (first 3): {model.weight.data[0][:3]}\")\n",
    "\n",
    "\n",
    "# 4. Training Loop\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    # a. Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(input_data)\n",
    "\n",
    "    # b. Compute and print loss\n",
    "    loss = criterion(y_pred, target)\n",
    "\n",
    "    # c. Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    print(\"=\" * 50)\n",
    "    # grad for all parameters will be zero, then backpropagation will be applied again.\n",
    "    for para in model.parameters():\n",
    "        print(para.grad)\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    print(\"Before backpropagation\")\n",
    "    for para in model.parameters():\n",
    "        print(para)\n",
    "    \n",
    "    # update gradient\n",
    "    loss.backward()\n",
    "    print(\"After backpropagation\")\n",
    "    for para in model.parameters():\n",
    "        print(para)\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    print(\"After optimizer step\")\n",
    "    for para in model.parameters():\n",
    "        print(para)\n",
    "\n",
    "    # Print the loss every 5 epochs to see the update\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print(\"\\nFinal Learned Values:\")\n",
    "print(f\"Weight (should be ~2.0): \", model.weight)\n",
    "print(f\"Bias   (should be ~1.0): \", model.bias)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
