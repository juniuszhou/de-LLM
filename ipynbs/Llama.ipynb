{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05660b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from utils.login_huggingface import login_huggingface\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "login_huggingface()\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "# all parameters number\n",
    "print(model.num_parameters())\n",
    "\n",
    "message = \"Hello, how are you?\"\n",
    "tokens = tokenizer.encode(message, return_tensors=\"pt\")\n",
    "print(tokens.shape)\n",
    "outputs = model(tokens, output_hidden_states=True)\n",
    "\n",
    "for idx, h in enumerate(outputs.hidden_states):\n",
    "    print(idx, h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1d3275",
   "metadata": {},
   "outputs": [],
   "source": [
    "Step 1: Environment Setup\n",
    "Install dependencies in a virtual environment (e.g., via venv or Conda). Use bitsandbytes for 4-bit quantization to fit the model in memory.\n",
    "Bashpip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118  # Adjust for your CUDA\n",
    "pip install -U transformers datasets accelerate peft trl bitsandbytes wandb  # Core libs\n",
    "pip install xformers  # For efficient attention\n",
    "# Optional for faster training on Colab:\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "Log in to Hugging Face for gated models:\n",
    "Pythonfrom huggingface_hub import login\n",
    "login()  # Paste your token\n",
    "Step 2: Data Preparation\n",
    "Load and format your small dataset into chat/instruction pairs. Use a chat template for Llama 3's format.\n",
    "Pythonfrom datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "# Load small dataset (e.g., 1,000 samples)\n",
    "dataset = load_dataset(\"ruslanmv/ai-medical-chatbot\", split=\"all\")  # Or your dataset\n",
    "dataset = dataset.shuffle(seed=42).select(range(1000))  # Subsample\n",
    "dataset = dataset.train_test_split(test_size=0.2)  # 800 train, 200 eval\n",
    "\n",
    "# Load tokenizer early for formatting\n",
    "from transformers import AutoTokenizer\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"  # Or Llama-3-8B\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Fix padding\n",
    "\n",
    "# Format as chat (user-assistant pairs)\n",
    "def format_chat(example):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": example[\"Patient\"]},  # Adapt to your fields\n",
    "        {\"role\": \"assistant\", \"content\": example[\"Doctor\"]}\n",
    "    ]\n",
    "    example[\"text\"] = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(format_chat, num_proc=4)\n",
    "Tip: For tiny data (<500 samples), add diversity by shuffling and splitting 80/20. Ensure sequences ≤512 tokens to avoid OOM.\n",
    "Step 3: Load Model with Quantization (QLoRA Setup)\n",
    "Load the base model in 4-bit for memory savings (~4–6 GB VRAM usage).\n",
    "Pythonfrom transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load quantized model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",  # Auto-distribute if multi-GPU\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"flash_attention_2\" if torch.cuda.is_available() else \"eager\",  # Faster attention\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)  # Enable gradient checkpoints\n",
    "\n",
    "# Setup chat format\n",
    "from trl import setup_chat_format\n",
    "model, tokenizer = setup_chat_format(model, tokenizer)\n",
    "Tip: On small data, quantization prevents overfitting by adding noise; use nf4 for better accuracy.\n",
    "Step 4: Configure LoRA\n",
    "Apply LoRA to target key modules (e.g., attention layers), training ~1–2% of params.\n",
    "Pythonpeft_config = LoraConfig(\n",
    "    r=16,  # Rank: Higher for more capacity, but 8–32 for small data\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()  # Verify: ~2M trainable params\n",
    "Tip: For very small data, lower r=8 to reduce params and overfitting.\n",
    "Step 5: Training\n",
    "Use TRL's SFTTrainer for supervised fine-tuning. Set small batch sizes and epochs.\n",
    "Pythonfrom trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./llama-3-finetuned\",\n",
    "    num_train_epochs=1,  # 1–2 for small data\n",
    "    per_device_train_batch_size=2,  # Adjust based on VRAM\n",
    "    gradient_accumulation_steps=4,  # Effective batch=8\n",
    "    learning_rate=2e-4,\n",
    "    warmup_steps=10,\n",
    "    logging_steps=10,\n",
    "    eval_steps=50,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True,  # Or bf16 for Ampere+ GPUs\n",
    "    optim=\"paged_adamw_8bit\",  # Memory-efficient optimizer\n",
    "    report_to=\"wandb\",  # Optional logging\n",
    "    dataloader_num_workers=4,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,\n",
    "    peft_config=peft_config,\n",
    "    args=args,\n",
    "    packing=False,  # No packing for small data\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "Tip: Train for 1 epoch; use cosine scheduler (lr_scheduler_type=\"cosine\") for smooth convergence. Early stopping if val loss plateaus.\n",
    "Step 6: Evaluation\n",
    "Test on held-out data; compute metrics like perplexity or accuracy.\n",
    "Pythonfrom datasets import load_metric\n",
    "import numpy as np\n",
    "\n",
    "# Simple perplexity eval (adapt for your task)\n",
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)  # e.g., {'eval_loss': 0.45}\n",
    "\n",
    "# For classification/QA: Use ROUGE/BLEU\n",
    "rouge = load_metric(\"rouge\")\n",
    "# Generate predictions and score...\n",
    "Tip: On small data, cross-validate or use k-fold to estimate performance reliably.\n",
    "Step 7: Save, Merge, and Local Inference\n",
    "Save the LoRA adapter (small ~10–50 MB file).\n",
    "Pythontrainer.model.save_pretrained(\"./llama-3-lora-adapter\")\n",
    "trainer.model.push_to_hub(\"your-username/llama-3-small-finetune\")  # Optional\n",
    "Merge adapter with base for full model:\n",
    "Pythonfrom peft import PeftModel\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n",
    "merged_model = PeftModel.from_pretrained(base_model, \"./llama-3-lora-adapter\")\n",
    "merged_model = merged_model.merge_and_unload()\n",
    "merged_model.save_pretrained(\"./llama-3-merged\")\n",
    "For local inference (e.g., on CPU/laptop), quantize to GGUF:\n",
    "\n",
    "Clone llama.cpp: git clone https://github.com/ggerganov/llama.cpp\n",
    "Convert: python convert-hf-to-gguf.py ./llama-3-merged --outfile llama-3-small.gguf --outtype f16\n",
    "Quantize: ./llama-quantize llama-3-small.gguf llama-3-small-q4.gguf Q4_K_M (~4 GB file)\n",
    "Run: ./llama-cli -m llama-3-small-q4.gguf -p \"Your prompt here\" --chat-template chatml (or use Ollama/Jan.ai for GUI).\n",
    "\n",
    "Example Inference:\n",
    "Pythonfrom transformers import pipeline\n",
    "pipe = pipeline(\"text-generation\", model=\"./llama-3-merged\", tokenizer=tokenizer)\n",
    "output = pipe(\"User: How to treat a headache? Assistant:\", max_new_tokens=100)\n",
    "print(output[0][\"generated_text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
