{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af19113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output shape: torch.Size([2, 24, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, embed_dim: int, n_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "        self.d_model = embed_dim\n",
    "        self.n_heads = n_heads\n",
    "        # splitted embed_dim for each head\n",
    "        self.d_k = embed_dim // n_heads\n",
    "        # avoid overfitting\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.q = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v = nn.Linear(embed_dim, embed_dim)    \n",
    "\n",
    "        self.o = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        self.scale = math.sqrt(self.d_k)\n",
    "\n",
    "    def forward(self, query, key, value, attn_mask=None, key_padding_mask=None):\n",
    "        # q, k, v shape: (batch, seq_len, d_model)\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # q, k, v shape: (batch, seq_len, d_model)\n",
    "        q = self.q(query)\n",
    "        k = self.k(key)\n",
    "        v = self.v(value)\n",
    "\n",
    "        # sequence length could be computed automatically. so it is not a hyperparameter for Attention.\n",
    "\n",
    "        # -1 should be the embed_dim\n",
    "        # q, k, v shape: (batch, n_heads, seq_len, d_k)\n",
    "        q = q.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        k = k.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        v = v.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # combine query and key info\n",
    "        # scores shape: (batch, n_heads, seq_len, seq_len)\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / self.scale\n",
    "\n",
    "        # mask is important to avoid using padding token\n",
    "        # for encoder 如果不 mask，padding token 会干扰注意力分数，导致模型学到噪声\n",
    "        # for decoder 如果不 mask，当前位置会“偷看”后面的真实词 → 信息泄露 → 模型学不到真正预测未来的能力\n",
    "\n",
    "        # scores shape: (batch, n_heads, seq_len, seq_len) but the value is -1e9 when mask is 0, or first seq_len < second seq_len\n",
    "        # Apply masks\n",
    "        if attn_mask is not None:\n",
    "            # attn_mask: usually (seq_q, seq_k) or broadcastable\n",
    "            scores = scores + attn_mask   # usually -inf in future positions\n",
    "        if key_padding_mask is not None:\n",
    "            # key_padding_mask: (B, seq_k) → expand to (B, H, seq_q, seq_k)\n",
    "            scores = scores.masked_fill(\n",
    "                key_padding_mask.unsqueeze(1).unsqueeze(2) == 0,\n",
    "                float('-inf')\n",
    "            )\n",
    "\n",
    "        # normalize the last dimension as embed_dim, after softmax, the value is between 0 and 1. and -1e9 will be almost 0\n",
    "        scores = F.softmax(scores, dim=-1)\n",
    "        # avoid overfitting\n",
    "        scores = self.dropout(scores)\n",
    "\n",
    "        # scores shape: (batch, n_heads, seq_len, seq_len)\n",
    "        # v shape: (batch, n_heads, seq_len, d_k)\n",
    "        context = torch.matmul(scores, v) # (batch, heads, seq, d_k) or scores @ v\n",
    "        # transpose to (batch, seq, heads, d_k), then combine heads and d_k\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)  # Fixed: self.embed_dim -> self.d_model\n",
    "        # output shape: (batch, seq_len, embed_dim)\n",
    "        output = self.o(context)  # Fixed: self.W_o -> self.o\n",
    "        return output\n",
    "        \n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"\"\"Position-wise Feed-Forward Network (two linear layers + activation)\"\"\"\n",
    "    def __init__(self, embed_dim: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        # 把 d_model 维的向量 “扩展” 到更高维空间（通常 4 倍）\n",
    "        self.linear1 = nn.Linear(embed_dim, d_ff)\n",
    "        # 把扩展后的高维特征 “压缩” 回原来的 d_model 维度\n",
    "        # 作用：提炼精华、输出更有价值的特征\n",
    "        self.linear2 = nn.Linear(d_ff, embed_dim)\n",
    "        # 作用：防止过拟合\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # 作用：增加非线性\n",
    "        self.activation = nn.SELU()   # original paper used ReLU\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"One Encoder Layer = Self-Attention + Feed-Forward + residual + norm\"\"\"\n",
    "    def __init__(self, embed_dim: int, num_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = Attention(embed_dim, num_heads, dropout)\n",
    "        self.cross_attn = Attention(embed_dim, num_heads, dropout)\n",
    "\n",
    "        self.feed_forward = PositionwiseFeedForward(embed_dim, d_ff, dropout)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.norm3 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, encoder_output, self_attn_mask=None, self_key_padding_mask=None,\n",
    "                cross_attn_mask=None, cross_key_padding_mask=None):  # Fixed: mask should be passed as parameter, not stored\n",
    "        # x shape: (batch, seq_len, embed_dim)\n",
    "        # Self-Attention sub-layer\n",
    "        self_attn_output = self.self_attn(x, x, x, None, None)  # Fixed: pass mask parameter\n",
    "        \n",
    "        # x keep the original info, and the attention output provide the new knowledge just learned\n",
    "        x = self.norm1(x + self.dropout(self_attn_output))          # residual + norm (post-norm)\n",
    "\n",
    "        cross_attn_output = self.cross_attn(x, encoder_output, encoder_output, cross_attn_mask, cross_key_padding_mask)\n",
    "        x = self.norm2(x + self.dropout(cross_attn_output))\n",
    "        \n",
    "        # Feed-Forward sub-layer\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))            # residual + norm\n",
    "        \n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder is a stack of DecoderLayer\n",
    "\n",
    "    key_padding_mask\n",
    "    如果你用 Hugging Face → 直接用 tokenizer 返回的 attention_mask（1=保留，0=padding），这是 90% 项目的最常见方式\n",
    "    不加 padding_mask 会导致模型“关注”无意义的 padding，严重影响性能。\n",
    "    padding的默认idx是0，所以用 mask == 0 就可以。它会广播的\n",
    "    (batch_size, seq_len)\n",
    "\n",
    "    attn_mask\n",
    "    它是一个上三角矩阵，用于限制注意力只关注当前位置之前的词。\n",
    "    在 Decoder 中，它用于限制 Decoder 只“看到”它之前的位置，避免“偷看”未来的信息。\n",
    "    在 Encoder 中，它用于限制 Encoder 只“看到”它之前的位置，避免“偷看”未来的信息。\n",
    "\n",
    "    encoder_output\n",
    "    是来自Encoder的输出，用于Cross-Attention。 它是Encoder最后的输出，不是其中的每一层。\n",
    "    它和Decoder的形状是一样的，都是 (batch_size, seq_len, embed_dim)\n",
    "\n",
    "    src_seq = 64\n",
    "    tgt_seq = 50\n",
    "    所有序列都要 padding 到 batch 中最长的长度\n",
    "    basically, 4096 is enough for most cases. \n",
    "\n",
    "    encoder input & decoder input\n",
    "    它们往往是不同的，根据不同的训练任务。比如翻译，二个完全不同，而且要右移一个token。\n",
    "    对于GPT Llama没有encoder，只有decoder。\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size: int, embed_dim: int, n_heads: int, d_ff: int, dropout: float = 0.1, n_layers: int = 6):  # Fixed: removed mask from __init__\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_layers = n_layers\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        # 所以 d_ff 通常 远大于 d_model，形成“先扩展、再压缩”的瓶颈结构（bottleneck），这能显著增加模型的非线性表达能力。\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.layers = nn.ModuleList([DecoderLayer(embed_dim, n_heads, d_ff, dropout) for _ in range(n_layers)])\n",
    "\n",
    "    # x: [batch_size, seq_len, embed_dim]\n",
    "    def forward(self, x, encoder_output, mask=None):  # Fixed: add mask parameter\n",
    "        \"\"\"\n",
    "        x: (batch, seq_len, d_model)  — usually after embedding + pos encoding\n",
    "        mask: (batch, seq_len, seq_len) or (batch, 1, seq_len, seq_len)  — optional padding mask\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, encoder_output)  # Fixed: pass mask to each layer\n",
    "        return self.norm(x)\n",
    "        \n",
    "\n",
    "# Test parameters\n",
    "vocab_size = 1000\n",
    "d_model = 512\n",
    "n_heads = 8\n",
    "d_ff = 2048\n",
    "n_layers = 6\n",
    "batch_size = 2\n",
    "src_seq_len = 16\n",
    "tgt_seq_len = 24\n",
    "\n",
    "# Create improved encoder\n",
    "decoder = Decoder(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=d_model,\n",
    "    n_heads=n_heads,\n",
    "    d_ff=d_ff,\n",
    "    n_layers=n_layers,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "decoder_input = torch.randn(batch_size, tgt_seq_len, d_model)\n",
    "encoder_output = torch.randn(batch_size, src_seq_len, d_model)\n",
    "\n",
    "# Create dummy input (token indices)\n",
    "# input_tokens = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "# print(f\"Input tokens shape: {input_tokens.shape}\")\n",
    "\n",
    "# Create padding mask (simulate padding at end)\n",
    "padding_mask = None\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    output = decoder(decoder_input, encoder_output)\n",
    "    print(f\"\\nOutput shape: {output.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
