{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddb3c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, embed_dim: int, n_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "        self.d_model = embed_dim\n",
    "        self.n_heads = n_heads\n",
    "        # splitted embed_dim for each head\n",
    "        self.d_k = embed_dim // n_heads\n",
    "        # avoid overfitting\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.q = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v = nn.Linear(embed_dim, embed_dim)    \n",
    "\n",
    "        self.o = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        self.scale = math.sqrt(self.d_k)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        # q, k, v shape: (batch, seq_len, d_model)\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # q, k, v shape: (batch, seq_len, d_model)\n",
    "        q = self.q(query)\n",
    "        k = self.k(key)\n",
    "        v = self.v(value)\n",
    "\n",
    "        # -1 should be the embed_dim\n",
    "        # q, k, v shape: (batch, n_heads, seq_len, d_k)\n",
    "        q = q.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        k = k.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        v = v.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # combine query and key info\n",
    "        # scores shape: (batch, n_heads, seq_len, seq_len)\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / self.scale\n",
    "\n",
    "        # mask is important to avoid using padding token\n",
    "        # for encoder 如果不 mask，padding token 会干扰注意力分数，导致模型学到噪声\n",
    "        # for decoder 如果不 mask，当前位置会“偷看”后面的真实词 → 信息泄露 → 模型学不到真正预测未来的能力\n",
    "\n",
    "        # scores shape: (batch, n_heads, seq_len, seq_len) but the value is -1e9 when mask is 0, or first seq_len < second seq_len\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # normalize the last dimension as embed_dim, after softmax, the value is between 0 and 1. and -1e9 will be almost 0\n",
    "        scores = F.softmax(scores, dim=-1)\n",
    "        # avoid overfitting\n",
    "        scores = self.dropout(scores)\n",
    "\n",
    "        # scores shape: (batch, n_heads, seq_len, seq_len)\n",
    "        # v shape: (batch, n_heads, seq_len, d_k)\n",
    "        context = torch.matmul(scores, v) # (batch, heads, seq, d_k) or scores @ v\n",
    "        # transpose to (batch, seq, heads, d_k), then combine heads and d_k\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)  # Fixed: self.embed_dim -> self.d_model\n",
    "        # output shape: (batch, seq_len, embed_dim)\n",
    "        output = self.o(context)  # Fixed: self.W_o -> self.o\n",
    "        return output\n",
    "        \n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"\"\"Position-wise Feed-Forward Network (two linear layers + activation)\"\"\"\n",
    "    def __init__(self, embed_dim: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        # 把 d_model 维的向量 “扩展” 到更高维空间（通常 4 倍）\n",
    "        self.linear1 = nn.Linear(embed_dim, d_ff)\n",
    "        # 把扩展后的高维特征 “压缩” 回原来的 d_model 维度\n",
    "        # 作用：提炼精华、输出更有价值的特征\n",
    "        self.linear2 = nn.Linear(d_ff, embed_dim)\n",
    "        # 作用：防止过拟合\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # 作用：增加非线性\n",
    "        self.activation = nn.SELU()   # original paper used ReLU\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"One Encoder Layer = Self-Attention + Feed-Forward + residual + norm\"\"\"\n",
    "    def __init__(self, embed_dim: int, num_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = Attention(embed_dim, num_heads, dropout)\n",
    "        self.feed_forward = PositionwiseFeedForward(embed_dim, d_ff, dropout)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):  # Fixed: mask should be passed as parameter, not stored\n",
    "        # x shape: (batch, seq_len, embed_dim)\n",
    "        # Self-Attention sub-layer\n",
    "        attn_output = self.self_attn(x, x, x, mask)  # Fixed: pass mask parameter\n",
    "        \n",
    "        # x keep the original info, and the attention output provide the new knowledge just learned\n",
    "        x = self.norm1(x + self.dropout(attn_output))          # residual + norm (post-norm)\n",
    "        \n",
    "        # Feed-Forward sub-layer\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))            # residual + norm\n",
    "        \n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embed_dim: int, n_heads: int, d_ff: int, dropout: float = 0.1, n_layers: int = 6):  # Fixed: removed mask from __init__\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_layers = n_layers\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        # 所以 d_ff 通常 远大于 d_model，形成“先扩展、再压缩”的瓶颈结构（bottleneck），这能显著增加模型的非线性表达能力。\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.layers = nn.ModuleList([EncoderLayer(embed_dim, n_heads, d_ff, dropout) for _ in range(n_layers)])\n",
    "\n",
    "    # x: [batch_size, seq_len, embed_dim]\n",
    "    def forward(self, x, mask=None):  # Fixed: add mask parameter\n",
    "        \"\"\"\n",
    "        x: (batch, seq_len, d_model)  — usually after embedding + pos encoding\n",
    "        mask: (batch, seq_len, seq_len) or (batch, 1, seq_len, seq_len)  — optional padding mask\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)  # Fixed: pass mask to each layer\n",
    "        return self.norm(x)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dba4e3d",
   "metadata": {},
   "source": [
    "# Encoder Implementation Review & Improvements\n",
    "\n",
    "## Issues Found:\n",
    "\n",
    "1. **Bug in Attention class**: Uses `self.embed_dim` but it's actually `self.d_model`\n",
    "2. **Mask handling**: Mask is passed to EncoderLayer but not properly forwarded to Attention\n",
    "3. **Missing components**: No input embedding or positional encoding layers\n",
    "4. **Inconsistent naming**: Mix of `embed_dim` and `d_model`\n",
    "5. **Mask parameter**: Encoder accepts mask but doesn't use it\n",
    "\n",
    "## Improvements:\n",
    "\n",
    "1. Fix the bug in Attention forward method\n",
    "2. Properly handle mask propagation through layers\n",
    "3. Add input embedding layer\n",
    "4. Add positional encoding\n",
    "5. Standardize naming conventions\n",
    "6. Add better documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be4fbbea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Key Improvements Summary\n",
      "======================================================================\n",
      "\n",
      "1. ✅ Fixed Bug: self.embed_dim -> self.d_model in Attention forward\n",
      "2. ✅ Fixed Bug: self.W_o -> self.o in Attention forward  \n",
      "3. ✅ Added Input Embedding layer\n",
      "4. ✅ Added Positional Encoding layer\n",
      "5. ✅ Proper mask handling (pass mask through layers)\n",
      "6. ✅ Pre-norm option (more stable than post-norm)\n",
      "7. ✅ Better documentation and type hints\n",
      "8. ✅ Helper method: create_padding_mask()\n",
      "9. ✅ Consistent naming (d_model throughout)\n",
      "10. ✅ Better activation (GELU instead of SELU)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# IMPROVED ENCODER IMPLEMENTATION\n",
    "# ============================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Self-Attention Mechanism\n",
    "    \n",
    "    Improvements:\n",
    "    - Fixed bug: self.embed_dim -> self.d_model\n",
    "    - Better variable naming consistency\n",
    "    - Improved documentation\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim: int, n_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert embed_dim % n_heads == 0, \"embed_dim must be divisible by n_heads\"\n",
    "        \n",
    "        self.d_model = embed_dim  # Standard naming: d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = embed_dim // n_heads  # Dimension per head\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Query, Key, Value projections\n",
    "        self.q = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.k = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.v = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        \n",
    "        # Output projection\n",
    "        self.o = nn.Linear(embed_dim, embed_dim, bias=False)  # Fixed: was self.W_o\n",
    "\n",
    "        self.scale = math.sqrt(self.d_k)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query: (batch, seq_len, d_model)\n",
    "            key: (batch, seq_len, d_model)\n",
    "            value: (batch, seq_len, d_model)\n",
    "            mask: (batch, 1, seq_len, seq_len) or (batch, seq_len, seq_len)\n",
    "        \n",
    "        Returns:\n",
    "            output: (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = query.size(0), query.size(1)\n",
    "\n",
    "        # Linear projections\n",
    "        q = self.q(query)  # (batch, seq_len, d_model)\n",
    "        k = self.k(key)\n",
    "        v = self.v(value)\n",
    "\n",
    "        # Reshape and transpose for multi-head attention\n",
    "        # (batch, seq_len, d_model) -> (batch, n_heads, seq_len, d_k)\n",
    "        q = q.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        k = k.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        v = v.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # Compute attention scores\n",
    "        # (batch, n_heads, seq_len, seq_len)\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / self.scale\n",
    "\n",
    "        # Apply mask (if provided)\n",
    "        if mask is not None:\n",
    "            # Ensure mask has correct shape for broadcasting\n",
    "            if mask.dim() == 3:\n",
    "                mask = mask.unsqueeze(1)  # (batch, 1, seq_len, seq_len)\n",
    "            scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "\n",
    "        # Softmax normalization\n",
    "        scores = F.softmax(scores, dim=-1)\n",
    "        scores = self.dropout(scores)\n",
    "\n",
    "        # Apply attention to values\n",
    "        # (batch, n_heads, seq_len, d_k)\n",
    "        context = torch.matmul(scores, v)\n",
    "        \n",
    "        # Concatenate heads: (batch, n_heads, seq_len, d_k) -> (batch, seq_len, d_model)\n",
    "        context = context.transpose(1, 2).contiguous().view(\n",
    "            batch_size, seq_len, self.d_model\n",
    "        )\n",
    "        \n",
    "        # Output projection\n",
    "        output = self.o(context)  # Fixed: was self.W_o\n",
    "        return output\n",
    "\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Position-wise Feed-Forward Network\n",
    "    \n",
    "    Improvements:\n",
    "    - Better activation function (GELU is more common than SELU)\n",
    "    - Clearer comments\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(embed_dim, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = nn.GELU()  # GELU is more common than SELU in modern transformers\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, seq_len, d_model)\n",
    "        Returns:\n",
    "            output: (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        return self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Single Encoder Layer\n",
    "    \n",
    "    Improvements:\n",
    "    - Proper mask handling (pass mask to attention)\n",
    "    - Pre-norm vs Post-norm option\n",
    "    - Better residual connection handling\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        embed_dim: int, \n",
    "        num_heads: int, \n",
    "        d_ff: int, \n",
    "        dropout: float = 0.1,\n",
    "        pre_norm: bool = True  # Pre-norm is more stable than post-norm\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.self_attn = Attention(embed_dim, num_heads, dropout)\n",
    "        self.feed_forward = PositionwiseFeedForward(embed_dim, d_ff, dropout)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.pre_norm = pre_norm\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, seq_len, d_model)\n",
    "            mask: (batch, seq_len, seq_len) or None\n",
    "        Returns:\n",
    "            x: (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        if self.pre_norm:\n",
    "            # Pre-norm: normalize before attention/FFN\n",
    "            # Self-Attention sub-layer\n",
    "            attn_output = self.self_attn(\n",
    "                self.norm1(x), self.norm1(x), self.norm1(x), mask\n",
    "            )\n",
    "            x = x + self.dropout1(attn_output)\n",
    "            \n",
    "            # Feed-Forward sub-layer\n",
    "            ff_output = self.feed_forward(self.norm2(x))\n",
    "            x = x + self.dropout2(ff_output)\n",
    "        else:\n",
    "            # Post-norm: normalize after attention/FFN (original Transformer)\n",
    "            attn_output = self.self_attn(x, x, x, mask)\n",
    "            x = self.norm1(x + self.dropout1(attn_output))\n",
    "            \n",
    "            ff_output = self.feed_forward(x)\n",
    "            x = self.norm2(x + self.dropout2(ff_output))\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Sinusoidal Positional Encoding\n",
    "    \n",
    "    Improvement: Added positional encoding class\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, max_len: int = 5000, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                            (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, seq_len, d_model)\n",
    "        Returns:\n",
    "            x: (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer Encoder\n",
    "    \n",
    "    Improvements:\n",
    "    - Added input embedding layer\n",
    "    - Added positional encoding\n",
    "    - Proper mask handling\n",
    "    - Better parameter organization\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_size: int, \n",
    "        embed_dim: int, \n",
    "        n_heads: int, \n",
    "        d_ff: int, \n",
    "        n_layers: int = 6,\n",
    "        max_seq_len: int = 5000,\n",
    "        dropout: float = 0.1,\n",
    "        pre_norm: bool = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # Input embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoding = PositionalEncoding(embed_dim, max_seq_len, dropout)\n",
    "        \n",
    "        # Encoder layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(embed_dim, n_heads, d_ff, dropout, pre_norm) \n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        # Final layer norm\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch, seq_len) - token indices\n",
    "            mask: (batch, seq_len, seq_len) or (batch, 1, seq_len, seq_len) - padding mask\n",
    "        Returns:\n",
    "            output: (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # Input embedding\n",
    "        x = self.embedding(x) * math.sqrt(self.embed_dim)  # Scale embeddings\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        # Pass through encoder layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        \n",
    "        # Final normalization\n",
    "        return self.norm(x)\n",
    "    \n",
    "    def create_padding_mask(self, x, pad_idx=0):\n",
    "        \"\"\"\n",
    "        Create padding mask from input tokens\n",
    "        \n",
    "        Args:\n",
    "            x: (batch, seq_len) - token indices\n",
    "            pad_idx: padding token index (default: 0)\n",
    "        Returns:\n",
    "            mask: (batch, 1, seq_len, seq_len) - 1 for valid positions, 0 for padding\n",
    "        \"\"\"\n",
    "        mask = (x != pad_idx).unsqueeze(1).unsqueeze(2)  # (batch, 1, 1, seq_len)\n",
    "        mask = mask.expand(x.size(0), 1, x.size(1), x.size(1))  # (batch, 1, seq_len, seq_len)\n",
    "        return mask\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# COMPARISON: Original vs Improved\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Key Improvements Summary\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "1. ✅ Fixed Bug: self.embed_dim -> self.d_model in Attention forward\n",
    "2. ✅ Fixed Bug: self.W_o -> self.o in Attention forward  \n",
    "3. ✅ Added Input Embedding layer\n",
    "4. ✅ Added Positional Encoding layer\n",
    "5. ✅ Proper mask handling (pass mask through layers)\n",
    "6. ✅ Pre-norm option (more stable than post-norm)\n",
    "7. ✅ Better documentation and type hints\n",
    "8. ✅ Helper method: create_padding_mask()\n",
    "9. ✅ Consistent naming (d_model throughout)\n",
    "10. ✅ Better activation (GELU instead of SELU)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0e264bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens shape: torch.Size([2, 10])\n",
      "Padding mask shape: torch.Size([2, 1, 10, 10])\n",
      "\n",
      "Output shape: torch.Size([2, 10, 512])\n",
      "Expected: (2, 10, 512)\n",
      "✅ Shape matches: True\n",
      "\n",
      "Output without mask shape: torch.Size([2, 10, 512])\n",
      "✅ Works without mask: True\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Test the Improved Encoder\n",
    "# ============================================================================\n",
    "\n",
    "# Test parameters\n",
    "vocab_size = 1000\n",
    "d_model = 512\n",
    "n_heads = 8\n",
    "d_ff = 2048\n",
    "n_layers = 6\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "\n",
    "# Create improved encoder\n",
    "encoder = Encoder(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=d_model,\n",
    "    n_heads=n_heads,\n",
    "    d_ff=d_ff,\n",
    "    n_layers=n_layers,\n",
    "    dropout=0.1,\n",
    "    pre_norm=True\n",
    ")\n",
    "\n",
    "# Create dummy input (token indices)\n",
    "input_tokens = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "print(f\"Input tokens shape: {input_tokens.shape}\")\n",
    "\n",
    "# Create padding mask (simulate padding at end)\n",
    "padding_mask = encoder.create_padding_mask(input_tokens)\n",
    "print(f\"Padding mask shape: {padding_mask.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    output = encoder(input_tokens, mask=padding_mask)\n",
    "    print(f\"\\nOutput shape: {output.shape}\")\n",
    "    print(f\"Expected: ({batch_size}, {seq_len}, {d_model})\")\n",
    "    print(f\"✅ Shape matches: {output.shape == (batch_size, seq_len, d_model)}\")\n",
    "\n",
    "# Test without mask\n",
    "output_no_mask = encoder(input_tokens)\n",
    "print(f\"\\nOutput without mask shape: {output_no_mask.shape}\")\n",
    "print(f\"✅ Works without mask: {output_no_mask.shape == (batch_size, seq_len, d_model)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c300054c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Detailed Review Comments\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DETAILED REVIEW COMMENTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\"\"\n",
    "ORIGINAL CODE ISSUES:\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "1. ❌ CRITICAL BUG in Attention.forward():\n",
    "   - Line uses: self.embed_dim but should be self.d_model\n",
    "   - Line uses: self.W_o but should be self.o\n",
    "   - This will cause AttributeError at runtime!\n",
    "\n",
    "2. ❌ Mask not properly handled:\n",
    "   - EncoderLayer accepts mask parameter but doesn't pass it to Attention\n",
    "   - Encoder accepts mask but never uses it\n",
    "   - Mask shape handling could be improved\n",
    "\n",
    "3. ❌ Missing components:\n",
    "   - No input embedding layer (expects pre-embedded input)\n",
    "   - No positional encoding (critical for transformers!)\n",
    "   - No way to create padding masks\n",
    "\n",
    "4. ⚠️  Inconsistent naming:\n",
    "   - Mix of embed_dim and d_model\n",
    "   - Standard convention is d_model\n",
    "\n",
    "5. ⚠️  Post-norm vs Pre-norm:\n",
    "   - Original uses post-norm (less stable)\n",
    "   - Pre-norm is more stable and commonly used now\n",
    "\n",
    "6. ⚠️  Activation function:\n",
    "   - Uses SELU (less common)\n",
    "   - GELU is more standard in modern transformers\n",
    "\n",
    "IMPROVEMENTS MADE:\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "✅ Fixed all bugs\n",
    "✅ Added input embedding\n",
    "✅ Added positional encoding  \n",
    "✅ Proper mask handling\n",
    "✅ Pre-norm option (with post-norm fallback)\n",
    "✅ Better documentation\n",
    "✅ Helper methods for mask creation\n",
    "✅ Consistent naming conventions\n",
    "✅ More standard activation (GELU)\n",
    "\n",
    "USAGE EXAMPLE:\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "encoder = Encoder(\n",
    "    vocab_size=10000,\n",
    "    embed_dim=512,\n",
    "    n_heads=8,\n",
    "    d_ff=2048,\n",
    "    n_layers=6,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# Input: token indices (not embeddings!)\n",
    "tokens = torch.randint(0, 10000, (batch_size, seq_len))\n",
    "\n",
    "# Create mask\n",
    "mask = encoder.create_padding_mask(tokens)\n",
    "\n",
    "# Forward pass\n",
    "output = encoder(tokens, mask=mask)\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
