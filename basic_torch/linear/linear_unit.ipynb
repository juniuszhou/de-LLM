{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1caf0b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e54bbfda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1])\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "\"LayerNormKernelImpl\" not implemented for 'Long'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m weight = nn.Parameter(torch.ones(\u001b[32m1\u001b[39m)).to(device)\n\u001b[32m     13\u001b[39m bias = nn.Parameter(torch.zeros(\u001b[32m1\u001b[39m)).to(device)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m y = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-5\u001b[39;49m\u001b[43m)\u001b[49m.to_device().to(device)\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Convert to numpy for plotting\u001b[39;00m\n\u001b[32m     20\u001b[39m x_np = x.numpy()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/junius/de-LLM/.venv/lib/python3.12/site-packages/torch/nn/functional.py:2901\u001b[39m, in \u001b[36mlayer_norm\u001b[39m\u001b[34m(input, normalized_shape, weight, bias, eps)\u001b[39m\n\u001b[32m   2891\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[32m   2892\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m   2893\u001b[39m         layer_norm,\n\u001b[32m   2894\u001b[39m         (\u001b[38;5;28minput\u001b[39m, weight, bias),\n\u001b[32m   (...)\u001b[39m\u001b[32m   2899\u001b[39m         eps=eps,\n\u001b[32m   2900\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2901\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2902\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackends\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcudnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43menabled\u001b[49m\n\u001b[32m   2903\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mNotImplementedError\u001b[39m: \"LayerNormKernelImpl\" not implemented for 'Long'"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Visualize ReLU or GELU Curve\n",
    "# ============================================================================\n",
    "\n",
    "# Create a range of input values from -5 to 5\n",
    "import torch.nn.functional as F\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "x = torch.random(-5, 5, (1000,1)).to(device)\n",
    "print(x.shape)\n",
    "\n",
    "weight = nn.Parameter(torch.ones(1)).to(device)\n",
    "bias = nn.Parameter(torch.zeros(1)).to(device)\n",
    "\n",
    "y = F.layer_norm(x, weight.shape, \n",
    "weight,\n",
    "bias, eps=1e-5).to_device().to(device)\n",
    "\n",
    "# Convert to numpy for plotting\n",
    "x_np = x.numpy()\n",
    "y_np = y.detach().numpy()\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_np, y_np, 'b-', linewidth=2, label='ReLU: f(x) = max(0, x)')\n",
    "plt.axhline(y=0, color='k', linestyle='--', linewidth=0.5, alpha=0.5)\n",
    "plt.axvline(x=0, color='k', linestyle='--', linewidth=0.5, alpha=0.5)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlabel('Input (x)', fontsize=12)\n",
    "plt.ylabel('Output f(x)', fontsize=12)\n",
    "plt.title('ReLU Activation Function', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.xlim(-5, 5)\n",
    "plt.ylim(-0.5, 5)\n",
    "\n",
    "# Add annotations\n",
    "plt.annotate('Linear region\\n(slope = 1)', \n",
    "             xy=(3, 3), xytext=(3.5, 2),\n",
    "             arrowprops=dict(arrowstyle='->', color='green', lw=1.5),\n",
    "             fontsize=10, color='green', fontweight='bold')\n",
    "\n",
    "plt.annotate('Zero region\\n(slope = 0)', \n",
    "             xy=(-3, 0), xytext=(-4, 1.5),\n",
    "             arrowprops=dict(arrowstyle='->', color='red', lw=1.5),\n",
    "             fontsize=10, color='red', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nReLU Characteristics:\")\n",
    "print(\"- For x > 0: Linear with slope = 1\")\n",
    "print(\"- For x <= 0: Output = 0 (dead neurons)\")\n",
    "print(\"- Non-linear activation function\")\n",
    "print(\"- Helps with vanishing gradient problem\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d552016",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3feb6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Practical Example: Freezing Model Parameters\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Practical Example: Freezing Model Parameters\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create a simple model\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(5, 10)\n",
    "        self.layer2 = nn.Linear(10, 3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layer2(torch.relu(self.layer1(x)))\n",
    "\n",
    "model = SimpleModel()\n",
    "\n",
    "print(\"Before freezing:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"  {name}: requires_grad = {param.requires_grad}\")\n",
    "\n",
    "# Freeze layer1 parameters\n",
    "print(\"\\nFreezing layer1...\")\n",
    "for param in model.layer1.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "print(\"\\nAfter freezing layer1:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"  {name}: requires_grad = {param.requires_grad}\")\n",
    "\n",
    "# Test forward and backward\n",
    "x = torch.randn(2, 5, requires_grad=True)\n",
    "output = model(x)\n",
    "loss = output.sum()\n",
    "\n",
    "print(f\"\\nForward pass successful: output shape = {output.shape}\")\n",
    "\n",
    "# Backward pass - only layer2 will have gradients\n",
    "loss.backward()\n",
    "\n",
    "print(\"\\nGradients after backward:\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        print(f\"  {name}: gradient exists (shape: {param.grad.shape})\")\n",
    "    else:\n",
    "        print(f\"  {name}: no gradient (frozen)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80074b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f172572",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9770c106",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d240ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d2551d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c22eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
