{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf51d8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content=\"What's the capital of France?\", additional_kwargs={}, response_metadata={}, id='d89018a4-64e8-4fb7-ba57-7c2d7043aec6'), AIMessage(content='The capital of France is Paris.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 32, 'total_tokens': 40, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'llama3.2:3b', 'system_fingerprint': 'fp_ollama', 'id': 'chatcmpl-165', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--4b8f165a-77ea-4d4b-9dbc-6d7e1f5cfcb9-0', usage_metadata={'input_tokens': 32, 'output_tokens': 8, 'total_tokens': 40, 'input_token_details': {}, 'output_token_details': {}})]}\n",
      "messages [HumanMessage(content=\"What's the capital of France?\", additional_kwargs={}, response_metadata={}, id='d89018a4-64e8-4fb7-ba57-7c2d7043aec6'), AIMessage(content='The capital of France is Paris.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 32, 'total_tokens': 40, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'llama3.2:3b', 'system_fingerprint': 'fp_ollama', 'id': 'chatcmpl-165', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--4b8f165a-77ea-4d4b-9dbc-6d7e1f5cfcb9-0', usage_metadata={'input_tokens': 32, 'output_tokens': 8, 'total_tokens': 40, 'input_token_details': {}, 'output_token_details': {}})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "import json\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pprint import pprint\n",
    "from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n",
    "\n",
    "# define two models\n",
    "basic_model = ChatOpenAI(\n",
    "    model=\"llama3.2:3b\",\n",
    "    temperature=0,\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    api_key=\"ollama\"\n",
    ")\n",
    "\n",
    "advanced_model = ChatOpenAI(\n",
    "    model=\"llama3.2:3b\",\n",
    "    temperature=0.5,\n",
    "    base_url=\"http://localhost:11434/v1\",\n",
    "    api_key=\"ollama\"\n",
    ")\n",
    "\n",
    "@wrap_model_call\n",
    "def dynamic_model_selection(request: ModelRequest, handler) -> ModelResponse:\n",
    "    \"\"\"Choose model based on conversation complexity.\"\"\"\n",
    "    message_count = len(request.state[\"messages\"])\n",
    "\n",
    "    if message_count > 10:\n",
    "        # Use an advanced model for longer conversations\n",
    "        model = advanced_model\n",
    "    else:\n",
    "        model = basic_model\n",
    "\n",
    "    return handler(request.override(model=model))\n",
    "\n",
    "# set model selector in middleware\n",
    "agent = create_agent(\n",
    "    model=basic_model,  # Default model\n",
    "    tools=[],\n",
    "    middleware=[dynamic_model_selection]\n",
    ")\n",
    "\n",
    "result = agent.invoke({\"input\": \"What's the capital of France?\", \"messages\": [{\"role\": \"user\", \"content\": \"What's the capital of France?\"}]})\n",
    "print(result)\n",
    "\n",
    "result_dict = dict(result)\n",
    "for key, value in result_dict.items():\n",
    "    print(key, value)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
